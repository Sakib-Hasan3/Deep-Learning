<<<<<<< HEAD
### **Sigmoid Activation Function**

---

## üîπ 1. Definition

**Sigmoid activation function** ‡¶è‡¶ï‡¶ü‡¶ø non-linear function ‡¶Ø‡¶æ neural network-‡¶è neuron-‡¶è‡¶∞ output ‡¶ï‡ßá **0 ‡¶è‡¶¨‡¶Ç 1 ‡¶è‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá** ‡¶∏‡ßÄ‡¶Æ‡¶æ‡¶¨‡¶¶‡ßç‡¶ß ‡¶ï‡¶∞‡ßá‡•§

‡¶ó‡¶æ‡¶£‡¶ø‡¶§‡¶ø‡¶ï‡¶≠‡¶æ‡¶¨‡ßá,

[
\sigma(x) = \frac{1}{1 + e^{-x}}
]

---

## üîπ 2. Output Range

[
0 < \sigma(x) < 1
]

üëâ ‡¶§‡¶æ‡¶á sigmoid ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑‡¶≠‡¶æ‡¶¨‡ßá **probability output** (‡¶Ø‡ßá‡¶Æ‡¶® binary classification) ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡•§

---

## üîπ 3. Graph Behavior (‡¶¨‡ßã‡¶ù‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø)

* (x \rightarrow +\infty \Rightarrow \sigma(x) \approx 1)
* (x \rightarrow -\infty \Rightarrow \sigma(x) \approx 0)
* (x = 0 \Rightarrow \sigma(0) = 0.5)

üëâ S-shaped curve (smooth & continuous)

---

## üîπ 4. Derivative (Backpropagation-‡¶è ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡ßÉ‡¶§)

[
\frac{d}{dx}[\sigma(x)] = \sigma(x),(1 - \sigma(x))
]

Maximum value:

[
\max(\sigma'(x)) = 0.25
]

---

## üîπ 5. Why Sigmoid is Used?

‚úî Non-linear mapping
‚úî Smooth gradient
‚úî Output can be interpreted as probability
‚úî Suitable for **binary classification output layer**

---

## üîπ 6. Sigmoid in ANN Learning (Intuition)

* Input weighted sum ‚Üí sigmoid
* Output ‡¶¨‡¶≤‡ßá ‡¶¶‡ßá‡ßü **‚Äú‡¶π‡ßç‡¶Ø‡¶æ‡¶Å ‡¶®‡¶æ ‡¶π‡¶ì‡ßü‡¶æ‡¶∞ ‡¶∏‡¶Æ‡ßç‡¶≠‡¶æ‡¶¨‡¶®‡¶æ‚Äù**
* Backpropagation-‡¶è derivative ‡¶¶‡¶ø‡ßü‡ßá weight update ‡¶π‡ßü

üëâ Example:
[
y = 0.87 \Rightarrow 87% \text{ probability}
]

---

## üîπ 7. Advantages

‚úî Simple and smooth
‚úî Differentiable everywhere
‚úî Probabilistic output
‚úî Historically important

---

## üîπ 8. Disadvantages

‚ùå **Vanishing Gradient Problem**
‚ùå Not zero-centered
‚ùå Slow convergence
‚ùå Saturation at extreme values

---

## üîπ 9. Where Sigmoid is Used Today?

‚úÖ Output layer (Binary classification)
‚ùå Hidden layers (deep networks)

---

## üîπ 10. Small Numerical Example

‡¶ß‡¶∞‡¶ø,
[
x = 2
]

[
\sigma(2) = \frac{1}{1+e^{-2}} \approx 0.88
]

Derivative:
[
\sigma'(2) = 0.88(1-0.88) = 0.1056
]

---

## üîë Exam-Ready One Line

> **Sigmoid is a smooth, non-linear activation function with output range (0,1), commonly used in the output layer for binary classification but prone to vanishing gradient in deep networks.**

---

## üß† One-Line Memory Trick

> **Sigmoid = Probability output + Vanishing gradient risk**


=======
### **Sigmoid Activation Function**

---

## üîπ 1. Definition

**Sigmoid activation function** ‡¶è‡¶ï‡¶ü‡¶ø non-linear function ‡¶Ø‡¶æ neural network-‡¶è neuron-‡¶è‡¶∞ output ‡¶ï‡ßá **0 ‡¶è‡¶¨‡¶Ç 1 ‡¶è‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá** ‡¶∏‡ßÄ‡¶Æ‡¶æ‡¶¨‡¶¶‡ßç‡¶ß ‡¶ï‡¶∞‡ßá‡•§

‡¶ó‡¶æ‡¶£‡¶ø‡¶§‡¶ø‡¶ï‡¶≠‡¶æ‡¶¨‡ßá,

[
\sigma(x) = \frac{1}{1 + e^{-x}}
]

---

## üîπ 2. Output Range

[
0 < \sigma(x) < 1
]

üëâ ‡¶§‡¶æ‡¶á sigmoid ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑‡¶≠‡¶æ‡¶¨‡ßá **probability output** (‡¶Ø‡ßá‡¶Æ‡¶® binary classification) ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡•§

---

## üîπ 3. Graph Behavior (‡¶¨‡ßã‡¶ù‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø)

* (x \rightarrow +\infty \Rightarrow \sigma(x) \approx 1)
* (x \rightarrow -\infty \Rightarrow \sigma(x) \approx 0)
* (x = 0 \Rightarrow \sigma(0) = 0.5)

üëâ S-shaped curve (smooth & continuous)

---

## üîπ 4. Derivative (Backpropagation-‡¶è ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡ßÉ‡¶§)

[
\frac{d}{dx}[\sigma(x)] = \sigma(x),(1 - \sigma(x))
]

Maximum value:

[
\max(\sigma'(x)) = 0.25
]

---

## üîπ 5. Why Sigmoid is Used?

‚úî Non-linear mapping
‚úî Smooth gradient
‚úî Output can be interpreted as probability
‚úî Suitable for **binary classification output layer**

---

## üîπ 6. Sigmoid in ANN Learning (Intuition)

* Input weighted sum ‚Üí sigmoid
* Output ‡¶¨‡¶≤‡ßá ‡¶¶‡ßá‡ßü **‚Äú‡¶π‡ßç‡¶Ø‡¶æ‡¶Å ‡¶®‡¶æ ‡¶π‡¶ì‡ßü‡¶æ‡¶∞ ‡¶∏‡¶Æ‡ßç‡¶≠‡¶æ‡¶¨‡¶®‡¶æ‚Äù**
* Backpropagation-‡¶è derivative ‡¶¶‡¶ø‡ßü‡ßá weight update ‡¶π‡ßü

üëâ Example:
[
y = 0.87 \Rightarrow 87% \text{ probability}
]

---

## üîπ 7. Advantages

‚úî Simple and smooth
‚úî Differentiable everywhere
‚úî Probabilistic output
‚úî Historically important

---

## üîπ 8. Disadvantages

‚ùå **Vanishing Gradient Problem**
‚ùå Not zero-centered
‚ùå Slow convergence
‚ùå Saturation at extreme values

---

## üîπ 9. Where Sigmoid is Used Today?

‚úÖ Output layer (Binary classification)
‚ùå Hidden layers (deep networks)

---

## üîπ 10. Small Numerical Example

‡¶ß‡¶∞‡¶ø,
[
x = 2
]

[
\sigma(2) = \frac{1}{1+e^{-2}} \approx 0.88
]

Derivative:
[
\sigma'(2) = 0.88(1-0.88) = 0.1056
]

---

## üîë Exam-Ready One Line

> **Sigmoid is a smooth, non-linear activation function with output range (0,1), commonly used in the output layer for binary classification but prone to vanishing gradient in deep networks.**

---

## üß† One-Line Memory Trick

> **Sigmoid = Probability output + Vanishing gradient risk**


>>>>>>> f45ebbad1686e699afe9932c4175eeff501d254b
