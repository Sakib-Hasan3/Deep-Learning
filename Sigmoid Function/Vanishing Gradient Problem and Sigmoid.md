---

## ЁЯФ╣ 1. Vanishing Gradient Problem ржХрзА?

**Vanishing Gradient Problem** рждржЦржи ржШржЯрзЗ ржпржЦржи
ЁЯСЙ **Backpropagation ржЪрж▓рж╛ржХрж╛рж▓рзАржи gradient ржЦрзБржм ржЫрзЛржЯ рж╣рждрзЗ рж╣рждрзЗ ржкрзНрж░рж╛рзЯ 0 рж╣рзЯрзЗ ржпрж╛рзЯ**ред

ржлрж▓рж╛ржлрж▓:

* Early (input-side) layers рж╢рзЗржЦрзЗ ржирж╛
* Weights ржЖржкржбрзЗржЯ рж╣рзЯ ржирж╛
* Network training ржЦрзБржм ржзрзАрж░ ржмрж╛ ржмржирзНржз рж╣рзЯрзЗ ржпрж╛рзЯ

---

## ЁЯФ╣ 2. Sigmoid Activation Function (рж╕ржВржХрзНрж╖рзЗржкрзЗ)

Sigmoid function:

[
\sigma(x) = \frac{1}{1+e^{-x}}
]

Derivative:

[
\sigma'(x) = \sigma(x),(1-\sigma(x))
]

Output range:
[
0 < \sigma(x) < 1
]

---

## ЁЯФ╣ 3. Sigmoid ржХрзЗржи Vanishing Gradient рж╕рзГрж╖рзНржЯрж┐ ржХрж░рзЗ?

### ЁЯФ╕ (a) Derivative ржЦрзБржм ржЫрзЛржЯ

Sigmoid derivative-ржПрж░ **maximum value**:

[
\max(\sigma'(x)) = 0.25
]

ЁЯСЙ ржорж╛ржирзЗ, ржпрзЗржХрзЛржирзЛ ржХрзНрж╖рзЗрждрзНрж░рзЗржЗ
[
\sigma'(x) \le 0.25
]

---

### ЁЯФ╕ (b) Backpropagation-ржП ржмрж╛рж░ржмрж╛рж░ ржЧрзБржг рж╣рзЯ (Chain Rule)

ржзрж░рж┐ 5-layer network:

[
\text{Gradient} = 0.25 \times 0.25 \times 0.25 \times 0.25 \times 0.25
]

[
= 0.00098 \approx 0
]

ЁЯСЙ Gradient ржкрзНрж░рж╛рзЯ **рж╢рзВржирзНржп рж╣рзЯрзЗ ржЧрзЗрж▓**

---

## ЁЯФ╣ 4. Saturation Problem (Sigmoid-ржПрж░ ржмрзЬ рж╕ржорж╕рзНржпрж╛)

ржпржЦржи input ржЦрзБржм ржмрзЬ ржмрж╛ ржЦрзБржм ржЫрзЛржЯ рж╣рзЯ:

* (x \gg 0 \Rightarrow \sigma(x) \approx 1)
* (x \ll 0 \Rightarrow \sigma(x) \approx 0)

рждржЦржи,

[
\sigma'(x) \approx 0
]

ЁЯСЙ Gradient тЙИ 0
ЁЯСЙ Weight update рж╣рзЯ ржирж╛
ЁЯСЙ **Neuron effectively тАЬdeadтАЭ**

---

## ЁЯФ╣ 5. Sigmoid + Vanishing Gradient (Step-by-Step Intuition)

1. Deep network
2. Sigmoid activation everywhere
3. Derivative < 1
4. Chain rule-ржП ржмрж╛рж░ржмрж╛рж░ multiplication
5. Gradient тЖТ 0
6. Initial layers learn nothing

ЁЯСЙ ржПржХрзЗржЗ ржмрж▓рзЗ **Vanishing Gradient Problem**

---

## ЁЯФ╣ 6. Mathematical Summary

Backpropagation equation:

[
\frac{\partial E}{\partial w}
=============================

\frac{\partial E}{\partial y}
\cdot
\frac{\partial y}{\partial z}
\cdot
\frac{\partial z}{\partial w}
]

Sigmoid рж╣рж▓рзЗ:

[
\frac{\partial y}{\partial z} = \sigma(z)(1-\sigma(z)) < 0.25
]

Repeated multiplication тЗТ gradient тЖТ 0

---

## ЁЯФ╣ 7. Why Sigmoid is avoided in Hidden Layers?

тЭМ Not zero-centered
тЭМ Causes vanishing gradient
тЭМ Slow convergence
тЭМ Poor performance in deep networks

ЁЯСЙ рждрж╛ржЗ ржЖржЬржХрж╛рж▓ **hidden layer-ржП sigmoid ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛ рж╣рзЯ ржирж╛**

---

## ЁЯФ╣ 8. How to Solve Vanishing Gradient?

тЬФ ReLU / Leaky ReLU ржмрзНржпржмрж╣рж╛рж░
тЬФ Tanh (sigmoid-ржПрж░ ржЪрзЗрзЯрзЗ ржнрж╛рж▓рзЛ)
тЬФ Proper weight initialization (Xavier, He)
тЬФ Batch Normalization
тЬФ Residual connections (ResNet)

---

## ЁЯФС Exam-Ready One Line

> **Vanishing Gradient Problem occurs when gradients become extremely small during backpropagation, and sigmoid activation aggravates this due to its small derivative and saturation behavior.**

---

## ЁЯза One-Line Memory Trick

> **Sigmoid + Deep Network = Vanishing Gradient**


