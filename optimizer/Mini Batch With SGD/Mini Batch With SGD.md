## **Mini-Batch SGD (Stochastic Gradient Descent) â€” Basic Theory**

![Image](https://miro.medium.com/1%2AI2BkYWA_OJzHWFo6kc0fmQ.jpeg)

![Image](https://miro.medium.com/1%2A9A0u4eeU_75bPgEuEwnwVQ.png)

![Image](https://www.researchgate.net/publication/261499993/figure/fig3/AS%3A613849782837261%401523364601324/flowchart-of-GPU-based-mini-batch-learning-algorithm.png)

---

### ðŸ”¹ 1) Mini-Batch SGD à¦•à§€?

**Mini-Batch Stochastic Gradient Descent** à¦¹à¦²à§‹ Gradient Descent-à¦à¦° à¦à¦®à¦¨ à¦à¦•à¦Ÿà¦¿ à¦ªà¦¦à§à¦§à¦¤à¦¿ à¦¯à§‡à¦–à¦¾à¦¨à§‡
ðŸ‘‰ **à¦ªà§à¦°à§‹ dataset à¦à¦•à¦¸à¦¾à¦¥à§‡ à¦¨à§Ÿ**,
ðŸ‘‰ à¦†à¦¬à¦¾à¦° **à¦à¦•à¦Ÿà¦¾ sample à¦¦à¦¿à§Ÿà§‡à¦“ à¦¨à§Ÿ**,
ðŸ‘‰ à¦¬à¦°à¦‚ **à¦›à§‹à¦Ÿ à¦›à§‹à¦Ÿ batch (subset)** à¦¨à¦¿à§Ÿà§‡ parameter update à¦•à¦°à¦¾ à¦¹à§Ÿà¥¤

**One-line definition:**

> *Mini-Batch SGD updates model parameters using the average gradient computed from a small batch of training samples.*

---

### ðŸ”¹ 2) à¦•à§‡à¦¨ Mini-Batch à¦¦à¦°à¦•à¦¾à¦°?

Gradient Descent-à¦à¦° à¦¤à¦¿à¦¨à¦Ÿà¦¾ à¦§à¦°à¦¨ à¦¤à§à¦²à¦¨à¦¾ à¦•à¦°à¦²à§‡ à¦¬à¦¿à¦·à§Ÿà¦Ÿà¦¾ à¦ªà¦°à¦¿à¦·à§à¦•à¦¾à¦° à¦¹à§Ÿ:

| Method             | à¦•à§€à¦­à¦¾à¦¬à§‡ à¦•à¦¾à¦œ à¦•à¦°à§‡ | à¦¸à¦®à¦¸à§à¦¯à¦¾               |
| ------------------ | -------------- | -------------------- |
| **Batch GD**       | à¦¸à¦¬ data à¦à¦•à¦¸à¦¾à¦¥à§‡ | à¦–à§à¦¬ à¦§à§€à¦°, à¦¬à§‡à¦¶à¦¿ memory |
| **SGD**            | à§§à¦Ÿà¦¾ sample     | à¦–à§à¦¬ noisy, unstable  |
| **Mini-Batch SGD** | 32â€“256 sample  | âœ… Speed + stability  |

ðŸ‘‰ Mini-Batch SGD **à¦¦à§à¦‡ à¦¦à§à¦¨à¦¿à§Ÿà¦¾à¦° à¦¸à§‡à¦°à¦¾ à¦¸à¦®à¦¾à¦§à¦¾à¦¨**à¥¤

---

### ðŸ”¹ 3) Mini-Batch SGD à¦•à§€à¦­à¦¾à¦¬à§‡ à¦•à¦¾à¦œ à¦•à¦°à§‡? (Concept Flow)

1. à¦ªà§à¦°à§‹ training dataset à¦¨à¦¾à¦“
2. Dataset à¦•à§‡ **à¦›à§‹à¦Ÿ batch** à¦ à¦­à¦¾à¦— à¦•à¦°à§‹ (à¦¯à§‡à¦®à¦¨ 32, 64)
3. à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾ batch-à¦à¦° à¦œà¦¨à§à¦¯:

   * Forward pass
   * Batch loss (average)
   * Gradient calculation
   * Weight update
4. à¦¸à¦¬ batch à¦¶à§‡à¦· à¦¹à¦²à§‡ = **1 epoch**

---

### ðŸ”¹ 4) Mathematical Idea (à¦¸à¦¹à¦œà¦­à¦¾à¦¬à§‡)

à¦§à¦°à¦¿,

* Batch size = (m)
* à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾ sample-à¦à¦° loss = (\ell_i)

**Batch cost:**
[
J_{\text{batch}}=\frac{1}{m}\sum_{i=1}^{m}\ell_i
]

**Gradient:**
[
\nabla J_{\text{batch}}=\frac{1}{m}\sum_{i=1}^{m}\nabla \ell_i
]

**Update rule:**
[
\theta \leftarrow \theta - \eta \nabla J_{\text{batch}}
]

ðŸ‘‰ à¦…à¦°à§à¦¥à¦¾à§Ž, **batch-à¦à¦° à¦—à§œ gradient** à¦¦à¦¿à§Ÿà§‡ updateà¥¤

---

### ðŸ”¹ 5) Important Terms (Exam Favourite)

* **Batch size:** à¦à¦•à¦¬à¦¾à¦°à§‡ à¦•à§Ÿà¦Ÿà¦¾ sample
* **Iteration:** à¦à¦• batch â†’ à¦à¦• update
* **Epoch:** à¦ªà§à¦°à§‹ dataset à¦à¦•à¦¬à¦¾à¦° complete

---

### ðŸ”¹ 6) Mini-Batch SGD à¦•à§‡à¦¨ Deep Learning-à¦ Default?

âœ” GPU-friendly (matrix operations)
âœ” SGD-à¦à¦° à¦šà§‡à§Ÿà§‡ stable
âœ” Batch GD-à¦à¦° à¦šà§‡à§Ÿà§‡ fast
âœ” Large dataset-à¦ scalable

---

### ðŸ§  Memory Trick

> **SGD = 1 sample**
> **Mini-Batch = few samples**
> **Batch = all samples**

---

### âœï¸ Exam-Ready One Line

> **Mini-Batch SGD updates model parameters using the average gradient of a small subset of data, providing a balance between fast convergence and stable learning.**

