<<<<<<< HEAD
## **SGD vs Mini-Batch vs Batch Gradient Descent â€” Full Comparison (Easy + Exam-Ready)**

![Image](https://miro.medium.com/1%2AFXHp55rpDM0tkaD5oz3Dvg.png)

![Image](https://www.researchgate.net/publication/369740077/figure/fig2/AS%3A11431281136661792%401680549836475/Comparison-of-the-optimization-paths-for-gradient-descent-training-GD-and-the-solutions.ppm)

![Image](https://miro.medium.com/1%2AbKSddSmLDaYszWllvQ3Z6A.png)

---

## ğŸ”¹ 1ï¸âƒ£ Basic Idea (à¦à¦• à¦²à¦¾à¦‡à¦¨à§‡)

* **Batch GD** â†’ à¦¸à¦¬ data à¦à¦•à¦¸à¦¾à¦¥à§‡ â†’ à¦à¦•à¦¬à¦¾à¦° update
* **SGD** â†’ à§§à¦Ÿà¦¾ sample â†’ à¦¸à¦¾à¦¥à§‡ à¦¸à¦¾à¦¥à§‡ update
* **Mini-Batch SGD** â†’ à¦•à§Ÿà§‡à¦•à¦Ÿà¦¾ sample â†’ average à¦•à¦°à§‡ update

---

## ğŸ”¹ 2ï¸âƒ£ How Weight Update Happens

à¦§à¦°à¦¿ total dataset size = (N)


### ğŸ”¸ Batch Gradient Descent
$$
	heta \leftarrow \theta - \eta \frac{1}{N} \sum_{i=1}^{N} \nabla \ell_i
$$


### ğŸ”¸ Stochastic Gradient Descent (SGD)
$$
	heta \leftarrow \theta - \eta \nabla \ell_i
$$
(à¦à¦•à¦Ÿà¦¾ sample)


### ğŸ”¸ Mini-Batch SGD
$$
	heta \leftarrow \theta - \eta \frac{1}{m} \sum_{i=1}^{m} \nabla \ell_i
$$
($m$ = batch size, à¦¯à§‡à¦®à¦¨ 32/64)

---

## ğŸ”¹ 3ï¸âƒ£ Core Comparison Table (Most Important)

| Feature            | Batch GD      | SGD         | Mini-Batch SGD    |
| ------------------ | ------------- | ----------- | ----------------- |
| Samples per update | All (N)       | 1           | Few (m)           |
| Update speed       | âŒ Slow        | âœ… Very fast | âœ… Fast            |
| Stability          | âœ… Very stable | âŒ Noisy     | âœ… Balanced        |
| Memory usage       | âŒ High        | âœ… Very low  | âœ… Moderate        |
| Convergence        | Smooth        | Zig-zag     | Smooth-ish        |
| GPU efficiency     | âŒ Poor        | âŒ Poor      | âœ… Excellent       |
| Practical use      | Rare          | Rare        | â­ **Most common** |

---

## ğŸ”¹ 4ï¸âƒ£ Intuition (à¦¸à¦¹à¦œà¦­à¦¾à¦¬à§‡ à¦¬à§‹à¦à¦¾)

### ğŸ¢ Batch GD

* à¦ªà§à¦°à§‹ à¦ªà¦¾à¦¹à¦¾à§œà§‡à¦° à¦®à§à¦¯à¦¾à¦ª à¦¦à§‡à¦–à§‡ à¦¨à¦¾à¦®à¦¾
* à¦§à§€à¦° à¦•à¦¿à¦¨à§à¦¤à§ à¦¨à¦¿à¦¶à§à¦šà¦¿à¦¤

### ğŸ‡ SGD

* à¦šà§‹à¦– à¦¬à¦¨à§à¦§ à¦•à¦°à§‡ à¦¦à§Œà§œà¦¾à¦¨à§‹
* à¦¦à§à¦°à§à¦¤ à¦•à¦¿à¦¨à§à¦¤à§ à¦à¦¦à¦¿à¦•-à¦“à¦¦à¦¿à¦•

### ğŸ Mini-Batch

* à¦›à§‹à¦Ÿ à¦®à§à¦¯à¦¾à¦ª à¦¦à§‡à¦–à§‡ à¦¦à§Œà§œà¦¾à¦¨à§‹
* à¦¦à§à¦°à§à¦¤ + à¦¨à¦¿à§Ÿà¦¨à§à¦¤à§à¦°à¦¿à¦¤

---

## ğŸ”¹ 5ï¸âƒ£ Noise vs Convergence

* **Batch GD:**

  * Smooth path
  * Exact minimum à¦–à§à¦à¦œà§‡ à¦ªà¦¾à§Ÿ
* **SGD:**

  * High noise
  * Minimum-à¦à¦° à¦†à¦¶à§‡à¦ªà¦¾à¦¶à§‡ à¦˜à§‹à¦°à§‡
* **Mini-Batch:**

  * Moderate noise
  * Stable minimum-à¦à¦° à¦•à¦¾à¦›à§‡ à¦ªà§Œà¦à¦›à¦¾à§Ÿ

---

## ğŸ”¹ 6ï¸âƒ£ Epoch, Iteration Difference

à¦§à¦°à¦¿:

* Dataset = 1000 samples
* Batch size = 100

| Method     | Iterations per epoch |
| ---------- | -------------------- |
| Batch GD   | 1                    |
| SGD        | 1000                 |
| Mini-Batch | 10                   |

---

## ğŸ”¹ 7ï¸âƒ£ When to Use Which?

### âœ… Use **Batch GD** when:

* Dataset à¦–à§à¦¬ à¦›à§‹à¦Ÿ
* Memory issue à¦¨à§‡à¦‡
* Mathematical clarity à¦¦à¦°à¦•à¦¾à¦°

### âœ… Use **SGD** when:

* Streaming / online data
* Extremely large dataset
* Very fast rough learning à¦¦à¦°à¦•à¦¾à¦°

### âœ… Use **Mini-Batch SGD** when:

* Deep learning
* GPU/TPU à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦°
* Real-world large dataset

ğŸ‘‰ **Industry default = Mini-Batch SGD**

---

## ğŸ§  Memory Trick

> **Batch = Slow & Stable**
> **SGD = Fast & Noisy**
> **Mini-Batch = Fast + Stable (Best)**

---

## âœï¸ Exam-Ready One-Line Answer

> **Batch gradient descent uses all data per update, SGD uses one sample per update causing noisy convergence, while mini-batch gradient descent balances speed and stability by updating with a small subset of data.**

---

=======
## **SGD vs Mini-Batch vs Batch Gradient Descent â€” Full Comparison (Easy + Exam-Ready)**

![Image](https://miro.medium.com/1%2AFXHp55rpDM0tkaD5oz3Dvg.png)

![Image](https://www.researchgate.net/publication/369740077/figure/fig2/AS%3A11431281136661792%401680549836475/Comparison-of-the-optimization-paths-for-gradient-descent-training-GD-and-the-solutions.ppm)

![Image](https://miro.medium.com/1%2AbKSddSmLDaYszWllvQ3Z6A.png)

---

## ğŸ”¹ 1ï¸âƒ£ Basic Idea (à¦à¦• à¦²à¦¾à¦‡à¦¨à§‡)

* **Batch GD** â†’ à¦¸à¦¬ data à¦à¦•à¦¸à¦¾à¦¥à§‡ â†’ à¦à¦•à¦¬à¦¾à¦° update
* **SGD** â†’ à§§à¦Ÿà¦¾ sample â†’ à¦¸à¦¾à¦¥à§‡ à¦¸à¦¾à¦¥à§‡ update
* **Mini-Batch SGD** â†’ à¦•à§Ÿà§‡à¦•à¦Ÿà¦¾ sample â†’ average à¦•à¦°à§‡ update

---

## ğŸ”¹ 2ï¸âƒ£ How Weight Update Happens

à¦§à¦°à¦¿ total dataset size = (N)


### ğŸ”¸ Batch Gradient Descent
$$
	heta \leftarrow \theta - \eta \frac{1}{N} \sum_{i=1}^{N} \nabla \ell_i
$$


### ğŸ”¸ Stochastic Gradient Descent (SGD)
$$
	heta \leftarrow \theta - \eta \nabla \ell_i
$$
(à¦à¦•à¦Ÿà¦¾ sample)


### ğŸ”¸ Mini-Batch SGD
$$
	heta \leftarrow \theta - \eta \frac{1}{m} \sum_{i=1}^{m} \nabla \ell_i
$$
($m$ = batch size, à¦¯à§‡à¦®à¦¨ 32/64)

---

## ğŸ”¹ 3ï¸âƒ£ Core Comparison Table (Most Important)

| Feature            | Batch GD      | SGD         | Mini-Batch SGD    |
| ------------------ | ------------- | ----------- | ----------------- |
| Samples per update | All (N)       | 1           | Few (m)           |
| Update speed       | âŒ Slow        | âœ… Very fast | âœ… Fast            |
| Stability          | âœ… Very stable | âŒ Noisy     | âœ… Balanced        |
| Memory usage       | âŒ High        | âœ… Very low  | âœ… Moderate        |
| Convergence        | Smooth        | Zig-zag     | Smooth-ish        |
| GPU efficiency     | âŒ Poor        | âŒ Poor      | âœ… Excellent       |
| Practical use      | Rare          | Rare        | â­ **Most common** |

---

## ğŸ”¹ 4ï¸âƒ£ Intuition (à¦¸à¦¹à¦œà¦­à¦¾à¦¬à§‡ à¦¬à§‹à¦à¦¾)

### ğŸ¢ Batch GD

* à¦ªà§à¦°à§‹ à¦ªà¦¾à¦¹à¦¾à§œà§‡à¦° à¦®à§à¦¯à¦¾à¦ª à¦¦à§‡à¦–à§‡ à¦¨à¦¾à¦®à¦¾
* à¦§à§€à¦° à¦•à¦¿à¦¨à§à¦¤à§ à¦¨à¦¿à¦¶à§à¦šà¦¿à¦¤

### ğŸ‡ SGD

* à¦šà§‹à¦– à¦¬à¦¨à§à¦§ à¦•à¦°à§‡ à¦¦à§Œà§œà¦¾à¦¨à§‹
* à¦¦à§à¦°à§à¦¤ à¦•à¦¿à¦¨à§à¦¤à§ à¦à¦¦à¦¿à¦•-à¦“à¦¦à¦¿à¦•

### ğŸ Mini-Batch

* à¦›à§‹à¦Ÿ à¦®à§à¦¯à¦¾à¦ª à¦¦à§‡à¦–à§‡ à¦¦à§Œà§œà¦¾à¦¨à§‹
* à¦¦à§à¦°à§à¦¤ + à¦¨à¦¿à§Ÿà¦¨à§à¦¤à§à¦°à¦¿à¦¤

---

## ğŸ”¹ 5ï¸âƒ£ Noise vs Convergence

* **Batch GD:**

  * Smooth path
  * Exact minimum à¦–à§à¦à¦œà§‡ à¦ªà¦¾à§Ÿ
* **SGD:**

  * High noise
  * Minimum-à¦à¦° à¦†à¦¶à§‡à¦ªà¦¾à¦¶à§‡ à¦˜à§‹à¦°à§‡
* **Mini-Batch:**

  * Moderate noise
  * Stable minimum-à¦à¦° à¦•à¦¾à¦›à§‡ à¦ªà§Œà¦à¦›à¦¾à§Ÿ

---

## ğŸ”¹ 6ï¸âƒ£ Epoch, Iteration Difference

à¦§à¦°à¦¿:

* Dataset = 1000 samples
* Batch size = 100

| Method     | Iterations per epoch |
| ---------- | -------------------- |
| Batch GD   | 1                    |
| SGD        | 1000                 |
| Mini-Batch | 10                   |

---

## ğŸ”¹ 7ï¸âƒ£ When to Use Which?

### âœ… Use **Batch GD** when:

* Dataset à¦–à§à¦¬ à¦›à§‹à¦Ÿ
* Memory issue à¦¨à§‡à¦‡
* Mathematical clarity à¦¦à¦°à¦•à¦¾à¦°

### âœ… Use **SGD** when:

* Streaming / online data
* Extremely large dataset
* Very fast rough learning à¦¦à¦°à¦•à¦¾à¦°

### âœ… Use **Mini-Batch SGD** when:

* Deep learning
* GPU/TPU à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦°
* Real-world large dataset

ğŸ‘‰ **Industry default = Mini-Batch SGD**

---

## ğŸ§  Memory Trick

> **Batch = Slow & Stable**
> **SGD = Fast & Noisy**
> **Mini-Batch = Fast + Stable (Best)**

---

## âœï¸ Exam-Ready One-Line Answer

> **Batch gradient descent uses all data per update, SGD uses one sample per update causing noisy convergence, while mini-batch gradient descent balances speed and stability by updating with a small subset of data.**

---

>>>>>>> f45ebbad1686e699afe9932c4175eeff501d254b
