<<<<<<< HEAD
## **SGD (Stochastic Gradient Descent) â€” Numerical Example (Step-by-Step)**

![Image](https://miro.medium.com/1%2Af9Wbr-pYMZ2AEzS2yp8EtQ.jpeg)

![Image](https://miro.medium.com/v2/resize%3Afit%3A1400/1%2AhJSLxZMjYVzgF5A_MoqeVQ.jpeg)

![Image](https://media2.dev.to/dynamic/image/width%3D800%2Cheight%3D%2Cfit%3Dscale-down%2Cgravity%3Dauto%2Cformat%3Dauto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fp484n1db128m716iomcv.jpg)

à¦†à¦®à¦°à¦¾ à¦à¦•à¦Ÿà¦¾ **simple linear regression** à¦¨à¦¿à§Ÿà§‡ SGD à¦¦à§‡à¦–à¦¾à¦¬à§‹à¥¤

---

## ğŸ”¹ 1ï¸âƒ£ Problem Setup

Model:
[
\hat{y} = wx + b
]

Loss (single sample):
[
\ell = \frac{1}{2}(y-\hat{y})^2
]

---

## ğŸ”¹ 2ï¸âƒ£ Given Values

* Initial weight: (w = 1)
* Initial bias: (b = 0)
* Learning rate: (\eta = 0.1)

Training samples (à¦à¦•à¦Ÿà¦¾ à¦à¦•à¦Ÿà¦¾ à¦•à¦°à§‡ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦¹à¦¬à§‡):

1. ((x_1=1,; y_1=2))
2. ((x_2=2,; y_2=3))

---

# âœ… SGD Step-1 (Sample 1 à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§‡)

### ğŸ”¸ Forward pass

[
\hat{y}_1 = (1)(1) + 0 = 1
]

### ğŸ”¸ Error

[
e_1 = y_1 - \hat{y}_1 = 2 - 1 = 1
]

### ğŸ”¸ Gradients

[
\frac{\partial \ell}{\partial w} = -e_1 x_1 = -1 \cdot 1 = -1
]
[
\frac{\partial \ell}{\partial b} = -e_1 = -1
]

### ğŸ”¸ Update (SGD)

[
w = 1 - 0.1(-1) = 1.1
]
[
b = 0 - 0.1(-1) = 0.1
]

---

# âœ… SGD Step-2 (Sample 2 à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§‡)

### ğŸ”¸ Forward pass

[
\hat{y}_2 = (1.1)(2) + 0.1 = 2.3
]

### ğŸ”¸ Error

[
e_2 = 3 - 2.3 = 0.7
]

### ğŸ”¸ Gradients

[
\frac{\partial \ell}{\partial w} = -e_2 x_2 = -0.7 \cdot 2 = -1.4
]
[
\frac{\partial \ell}{\partial b} = -e_2 = -0.7
]

### ğŸ”¸ Update

[
w = 1.1 - 0.1(-1.4) = 1.24
]
[
b = 0.1 - 0.1(-0.7) = 0.17
]

---

## ğŸ”¹ 3ï¸âƒ£ Parameters After One Epoch (SGD)

[
\boxed{w = 1.24,\quad b = 0.17}
]

ğŸ‘‰ à¦²à¦•à§à¦·à§à¦¯ à¦•à¦°à§‹:

* à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾ **data point à¦¦à¦¿à§Ÿà§‡à¦‡ à¦¸à¦¾à¦¥à§‡ à¦¸à¦¾à¦¥à§‡ update** à¦¹à§Ÿà§‡à¦›à§‡
* à¦à¦‡à¦Ÿà¦¾à¦‡ **Stochastic Gradient Descent**

---

## ğŸ”¹ 4ï¸âƒ£ Why This Is â€œStochasticâ€?

* à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾ sample à¦†à¦²à¦¾à¦¦à¦¾ direction à¦¦à§‡à¦–à¦¾à§Ÿ
* Update path **zig-zag** à¦•à¦°à§‡
* à¦•à¦¿à¦¨à§à¦¤à§ average direction à¦ à¦¿à¦• à¦¥à¦¾à¦•à§‡

---

## ğŸ§  Memory Trick

> **SGD = one sample â†’ one update**

---

## âœï¸ Exam-Ready One Line

> **In SGD, model parameters are updated after computing the loss gradient for each individual training example, resulting in fast but noisy convergence.**

=======
## **SGD (Stochastic Gradient Descent) â€” Numerical Example (Step-by-Step)**

![Image](https://miro.medium.com/1%2Af9Wbr-pYMZ2AEzS2yp8EtQ.jpeg)

![Image](https://miro.medium.com/v2/resize%3Afit%3A1400/1%2AhJSLxZMjYVzgF5A_MoqeVQ.jpeg)

![Image](https://media2.dev.to/dynamic/image/width%3D800%2Cheight%3D%2Cfit%3Dscale-down%2Cgravity%3Dauto%2Cformat%3Dauto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fp484n1db128m716iomcv.jpg)

à¦†à¦®à¦°à¦¾ à¦à¦•à¦Ÿà¦¾ **simple linear regression** à¦¨à¦¿à§Ÿà§‡ SGD à¦¦à§‡à¦–à¦¾à¦¬à§‹à¥¤

---

## ğŸ”¹ 1ï¸âƒ£ Problem Setup

Model:
[
\hat{y} = wx + b
]

Loss (single sample):
[
\ell = \frac{1}{2}(y-\hat{y})^2
]

---

## ğŸ”¹ 2ï¸âƒ£ Given Values

* Initial weight: (w = 1)
* Initial bias: (b = 0)
* Learning rate: (\eta = 0.1)

Training samples (à¦à¦•à¦Ÿà¦¾ à¦à¦•à¦Ÿà¦¾ à¦•à¦°à§‡ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦¹à¦¬à§‡):

1. ((x_1=1,; y_1=2))
2. ((x_2=2,; y_2=3))

---

# âœ… SGD Step-1 (Sample 1 à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§‡)

### ğŸ”¸ Forward pass

[
\hat{y}_1 = (1)(1) + 0 = 1
]

### ğŸ”¸ Error

[
e_1 = y_1 - \hat{y}_1 = 2 - 1 = 1
]

### ğŸ”¸ Gradients

[
\frac{\partial \ell}{\partial w} = -e_1 x_1 = -1 \cdot 1 = -1
]
[
\frac{\partial \ell}{\partial b} = -e_1 = -1
]

### ğŸ”¸ Update (SGD)

[
w = 1 - 0.1(-1) = 1.1
]
[
b = 0 - 0.1(-1) = 0.1
]

---

# âœ… SGD Step-2 (Sample 2 à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§‡)

### ğŸ”¸ Forward pass

[
\hat{y}_2 = (1.1)(2) + 0.1 = 2.3
]

### ğŸ”¸ Error

[
e_2 = 3 - 2.3 = 0.7
]

### ğŸ”¸ Gradients

[
\frac{\partial \ell}{\partial w} = -e_2 x_2 = -0.7 \cdot 2 = -1.4
]
[
\frac{\partial \ell}{\partial b} = -e_2 = -0.7
]

### ğŸ”¸ Update

[
w = 1.1 - 0.1(-1.4) = 1.24
]
[
b = 0.1 - 0.1(-0.7) = 0.17
]

---

## ğŸ”¹ 3ï¸âƒ£ Parameters After One Epoch (SGD)

[
\boxed{w = 1.24,\quad b = 0.17}
]

ğŸ‘‰ à¦²à¦•à§à¦·à§à¦¯ à¦•à¦°à§‹:

* à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾ **data point à¦¦à¦¿à§Ÿà§‡à¦‡ à¦¸à¦¾à¦¥à§‡ à¦¸à¦¾à¦¥à§‡ update** à¦¹à§Ÿà§‡à¦›à§‡
* à¦à¦‡à¦Ÿà¦¾à¦‡ **Stochastic Gradient Descent**

---

## ğŸ”¹ 4ï¸âƒ£ Why This Is â€œStochasticâ€?

* à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾ sample à¦†à¦²à¦¾à¦¦à¦¾ direction à¦¦à§‡à¦–à¦¾à§Ÿ
* Update path **zig-zag** à¦•à¦°à§‡
* à¦•à¦¿à¦¨à§à¦¤à§ average direction à¦ à¦¿à¦• à¦¥à¦¾à¦•à§‡

---

## ğŸ§  Memory Trick

> **SGD = one sample â†’ one update**

---

## âœï¸ Exam-Ready One Line

> **In SGD, model parameters are updated after computing the loss gradient for each individual training example, resulting in fast but noisy convergence.**

>>>>>>> f45ebbad1686e699afe9932c4175eeff501d254b
