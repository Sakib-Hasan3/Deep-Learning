<<<<<<< HEAD
## **SGD (Stochastic Gradient Descent) **

![Image](https://miro.medium.com/1%2Af9Wbr-pYMZ2AEzS2yp8EtQ.jpeg)

![Image](https://miro.medium.com/1%2AFXHp55rpDM0tkaD5oz3Dvg.png)

![Image](https://miro.medium.com/v2/resize%3Afit%3A1200/1%2Af9Wbr-pYMZ2AEzS2yp8EtQ.jpeg)

![Image](https://ketanhdoshi.github.io/assets/images/OptimizerTechniques/Momentum-2.png)

---

## ğŸ”¹ 1ï¸âƒ£ SGD à¦•à§€?

**SGD (Stochastic Gradient Descent)** à¦¹à¦²à§‹ Gradient Descent-à¦à¦° à¦à¦•à¦Ÿà¦¿ variant, à¦¯à§‡à¦–à¦¾à¦¨à§‡
ğŸ‘‰ **à¦à¦•à¦¬à¦¾à¦°à§‡ à¦®à¦¾à¦¤à§à¦° à¦à¦•à¦Ÿà¦¿ training sample à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§‡ weight update à¦•à¦°à¦¾ à¦¹à§Ÿ**à¥¤

---

## ğŸ”¹ 2ï¸âƒ£ Basic Gradient Descent vs SGD

### ğŸ”¸ Batch Gradient Descent

* à¦ªà§à¦°à§‹ dataset à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§‡
* Update à¦§à§€à¦° à¦•à¦¿à¦¨à§à¦¤à§ stable

### ğŸ”¸ **SGD**

* **à¦à¦•à¦Ÿà¦¾ sample** à¦¦à¦¿à§Ÿà§‡ update
* Update à¦¦à§à¦°à§à¦¤ à¦•à¦¿à¦¨à§à¦¤à§ noisy

---

## ğŸ”¹ 3ï¸âƒ£ Mathematical Update Rule (SGD)

à¦§à¦°à¦¿,

* (\theta) = model parameters
* (\eta) = learning rate
* ((x_i, y_i)) = à¦à¦•à¦Ÿà¦¿à¦®à¦¾à¦¤à§à¦° data sample

à¦¤à¦¾à¦¹à¦²à§‡ SGD update:

[
\theta \leftarrow \theta - \eta \nabla J(\theta; x_i, y_i)
]

ğŸ‘‰ à¦à¦–à¦¾à¦¨à§‡ gradient à¦¹à¦¿à¦¸à¦¾à¦¬ à¦¹à§Ÿ **à¦à¦•à¦Ÿà¦¿ sample-à¦à¦° loss à¦¥à§‡à¦•à§‡**à¥¤

---

## ğŸ”¹ 4ï¸âƒ£ Intuition (à¦¸à¦¹à¦œ à¦­à¦¾à¦·à¦¾à§Ÿ)

* Batch GD = à¦ªà¦¾à¦¹à¦¾à§œ à¦¨à¦¾à¦®à¦¾, à¦ªà§à¦°à§‹ à¦®à§à¦¯à¦¾à¦ª à¦¦à§‡à¦–à§‡ à¦§à§€à¦°à§‡ à¦§à§€à¦°à§‡ à¦¨à¦¾à¦®à¦¾
* **SGD = à¦šà§‹à¦– à¦¬à¦¨à§à¦§ à¦•à¦°à§‡ à¦¦à§Œà§œà¦¾à¦¨à§‹** ğŸ˜„

  * à¦•à¦–à¦¨à§‹ à¦¡à¦¾à¦¨à§‡
  * à¦•à¦–à¦¨à§‹ à¦¬à¦¾à¦®à§‡
  * à¦•à¦¿à¦¨à§à¦¤à§ à¦¦à§à¦°à§à¦¤ à¦¨à¦¿à¦šà§‡à¦° à¦¦à¦¿à¦•à§‡ à¦¯à¦¾à§Ÿ

ğŸ‘‰ Noise à¦¥à¦¾à¦•à¦²à§‡à¦“ **average direction à¦ à¦¿à¦• à¦¥à¦¾à¦•à§‡**à¥¤

---

## ğŸ”¹ 5ï¸âƒ£ SGD Graph à¦¥à§‡à¦•à§‡ à¦•à§€ à¦¬à§‹à¦à¦¾ à¦¯à¦¾à§Ÿ?

* Path zig-zag à¦•à¦°à§‡
* Minimum-à¦à¦° à¦†à¦¶à§‡à¦ªà¦¾à¦¶à§‡ à¦˜à§à¦°à§‡ à¦¬à§‡à§œà¦¾à§Ÿ
* Exact minimum-à¦ à¦¥à¦¾à¦®à§‡ à¦¨à¦¾, à¦•à¦¿à¦¨à§à¦¤à§ à¦•à¦¾à¦›à¦¾à¦•à¦¾à¦›à¦¿ à¦¥à¦¾à¦•à§‡

---

## ğŸ”¹ 6ï¸âƒ£ Advantages of SGD

âœ” Very fast updates
âœ” Large dataset-à¦ scalable
âœ” Memory à¦•à¦® à¦²à¦¾à¦—à§‡
âœ” Local minima à¦¥à§‡à¦•à§‡ à¦¬à§‡à¦° à¦¹à¦¤à§‡ à¦¸à¦¾à¦¹à¦¾à¦¯à§à¦¯ à¦•à¦°à§‡

---

## ğŸ”¹ 7ï¸âƒ£ Disadvantages of SGD

âŒ Convergence noisy
âŒ Exact minimum-à¦ settle à¦•à¦°à§‡ à¦¨à¦¾
âŒ Learning rate à¦ à¦¿à¦• à¦¨à¦¾ à¦¹à¦²à§‡ unstable
âŒ Zig-zag path (slow fine-tuning)

---

## ğŸ”¹ 8ï¸âƒ£ SGD à¦•à¦–à¦¨ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à¦¬à§‡?

âœ… Dataset à¦–à§à¦¬ à¦¬à§œ
âœ… Online / streaming data
âœ… Simple baseline optimizer
âœ… Time constraint à¦†à¦›à§‡

---

## ğŸ”¹ 9ï¸âƒ£ SGD vs Mini-batch vs Batch

| Type          | Samples per update | Speed         | Stability |
| ------------- | ------------------ | ------------- | --------- |
| Batch GD      | All                | Slow          | High      |
| **SGD**       | **1**              | **Very Fast** | Low       |
| Mini-batch GD | 32â€“256             | Fast          | Medium    |

ğŸ‘‰ à¦¬à¦¾à¦¸à§à¦¤à¦¬à§‡ **Mini-batch** à¦¬à§‡à¦¶à¦¿ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦¹à§Ÿà¥¤

---

## ğŸ§  Memory Trick

> **SGD = Fast but noisy learner**

---

## âœï¸ Exam-Ready One Line

> **Stochastic Gradient Descent updates model parameters using one training example at a time, enabling fast and scalable learning at the cost of noisy convergence.**

---

=======
## **SGD (Stochastic Gradient Descent) **

![Image](https://miro.medium.com/1%2Af9Wbr-pYMZ2AEzS2yp8EtQ.jpeg)

![Image](https://miro.medium.com/1%2AFXHp55rpDM0tkaD5oz3Dvg.png)

![Image](https://miro.medium.com/v2/resize%3Afit%3A1200/1%2Af9Wbr-pYMZ2AEzS2yp8EtQ.jpeg)

![Image](https://ketanhdoshi.github.io/assets/images/OptimizerTechniques/Momentum-2.png)

---

## ğŸ”¹ 1ï¸âƒ£ SGD à¦•à§€?

**SGD (Stochastic Gradient Descent)** à¦¹à¦²à§‹ Gradient Descent-à¦à¦° à¦à¦•à¦Ÿà¦¿ variant, à¦¯à§‡à¦–à¦¾à¦¨à§‡
ğŸ‘‰ **à¦à¦•à¦¬à¦¾à¦°à§‡ à¦®à¦¾à¦¤à§à¦° à¦à¦•à¦Ÿà¦¿ training sample à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§‡ weight update à¦•à¦°à¦¾ à¦¹à§Ÿ**à¥¤

---

## ğŸ”¹ 2ï¸âƒ£ Basic Gradient Descent vs SGD

### ğŸ”¸ Batch Gradient Descent

* à¦ªà§à¦°à§‹ dataset à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§‡
* Update à¦§à§€à¦° à¦•à¦¿à¦¨à§à¦¤à§ stable

### ğŸ”¸ **SGD**

* **à¦à¦•à¦Ÿà¦¾ sample** à¦¦à¦¿à§Ÿà§‡ update
* Update à¦¦à§à¦°à§à¦¤ à¦•à¦¿à¦¨à§à¦¤à§ noisy

---

## ğŸ”¹ 3ï¸âƒ£ Mathematical Update Rule (SGD)

à¦§à¦°à¦¿,

* (\theta) = model parameters
* (\eta) = learning rate
* ((x_i, y_i)) = à¦à¦•à¦Ÿà¦¿à¦®à¦¾à¦¤à§à¦° data sample

à¦¤à¦¾à¦¹à¦²à§‡ SGD update:

[
\theta \leftarrow \theta - \eta \nabla J(\theta; x_i, y_i)
]

ğŸ‘‰ à¦à¦–à¦¾à¦¨à§‡ gradient à¦¹à¦¿à¦¸à¦¾à¦¬ à¦¹à§Ÿ **à¦à¦•à¦Ÿà¦¿ sample-à¦à¦° loss à¦¥à§‡à¦•à§‡**à¥¤

---

## ğŸ”¹ 4ï¸âƒ£ Intuition (à¦¸à¦¹à¦œ à¦­à¦¾à¦·à¦¾à§Ÿ)

* Batch GD = à¦ªà¦¾à¦¹à¦¾à§œ à¦¨à¦¾à¦®à¦¾, à¦ªà§à¦°à§‹ à¦®à§à¦¯à¦¾à¦ª à¦¦à§‡à¦–à§‡ à¦§à§€à¦°à§‡ à¦§à§€à¦°à§‡ à¦¨à¦¾à¦®à¦¾
* **SGD = à¦šà§‹à¦– à¦¬à¦¨à§à¦§ à¦•à¦°à§‡ à¦¦à§Œà§œà¦¾à¦¨à§‹** ğŸ˜„

  * à¦•à¦–à¦¨à§‹ à¦¡à¦¾à¦¨à§‡
  * à¦•à¦–à¦¨à§‹ à¦¬à¦¾à¦®à§‡
  * à¦•à¦¿à¦¨à§à¦¤à§ à¦¦à§à¦°à§à¦¤ à¦¨à¦¿à¦šà§‡à¦° à¦¦à¦¿à¦•à§‡ à¦¯à¦¾à§Ÿ

ğŸ‘‰ Noise à¦¥à¦¾à¦•à¦²à§‡à¦“ **average direction à¦ à¦¿à¦• à¦¥à¦¾à¦•à§‡**à¥¤

---

## ğŸ”¹ 5ï¸âƒ£ SGD Graph à¦¥à§‡à¦•à§‡ à¦•à§€ à¦¬à§‹à¦à¦¾ à¦¯à¦¾à§Ÿ?

* Path zig-zag à¦•à¦°à§‡
* Minimum-à¦à¦° à¦†à¦¶à§‡à¦ªà¦¾à¦¶à§‡ à¦˜à§à¦°à§‡ à¦¬à§‡à§œà¦¾à§Ÿ
* Exact minimum-à¦ à¦¥à¦¾à¦®à§‡ à¦¨à¦¾, à¦•à¦¿à¦¨à§à¦¤à§ à¦•à¦¾à¦›à¦¾à¦•à¦¾à¦›à¦¿ à¦¥à¦¾à¦•à§‡

---

## ğŸ”¹ 6ï¸âƒ£ Advantages of SGD

âœ” Very fast updates
âœ” Large dataset-à¦ scalable
âœ” Memory à¦•à¦® à¦²à¦¾à¦—à§‡
âœ” Local minima à¦¥à§‡à¦•à§‡ à¦¬à§‡à¦° à¦¹à¦¤à§‡ à¦¸à¦¾à¦¹à¦¾à¦¯à§à¦¯ à¦•à¦°à§‡

---

## ğŸ”¹ 7ï¸âƒ£ Disadvantages of SGD

âŒ Convergence noisy
âŒ Exact minimum-à¦ settle à¦•à¦°à§‡ à¦¨à¦¾
âŒ Learning rate à¦ à¦¿à¦• à¦¨à¦¾ à¦¹à¦²à§‡ unstable
âŒ Zig-zag path (slow fine-tuning)

---

## ğŸ”¹ 8ï¸âƒ£ SGD à¦•à¦–à¦¨ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à¦¬à§‡?

âœ… Dataset à¦–à§à¦¬ à¦¬à§œ
âœ… Online / streaming data
âœ… Simple baseline optimizer
âœ… Time constraint à¦†à¦›à§‡

---

## ğŸ”¹ 9ï¸âƒ£ SGD vs Mini-batch vs Batch

| Type          | Samples per update | Speed         | Stability |
| ------------- | ------------------ | ------------- | --------- |
| Batch GD      | All                | Slow          | High      |
| **SGD**       | **1**              | **Very Fast** | Low       |
| Mini-batch GD | 32â€“256             | Fast          | Medium    |

ğŸ‘‰ à¦¬à¦¾à¦¸à§à¦¤à¦¬à§‡ **Mini-batch** à¦¬à§‡à¦¶à¦¿ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦¹à§Ÿà¥¤

---

## ğŸ§  Memory Trick

> **SGD = Fast but noisy learner**

---

## âœï¸ Exam-Ready One Line

> **Stochastic Gradient Descent updates model parameters using one training example at a time, enabling fast and scalable learning at the cost of noisy convergence.**

---

>>>>>>> f45ebbad1686e699afe9932c4175eeff501d254b
