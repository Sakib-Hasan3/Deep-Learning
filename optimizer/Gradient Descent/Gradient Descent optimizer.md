## **Gradient Descent Optimizers â€” à¦¸à¦¹à¦œ à¦¬à§à¦¯à¦¾à¦–à§à¦¯à¦¾ (Intuition + Comparison)**

![Image](https://sebastianraschka.com/images/faq/gradient-optimization/ball.png)

![Image](https://media2.dev.to/dynamic/image/width%3D800%2Cheight%3D%2Cfit%3Dscale-down%2Cgravity%3Dauto%2Cformat%3Dauto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fp484n1db128m716iomcv.jpg)

![Image](https://miro.medium.com/v2/resize%3Afit%3A1000/1%2AX9SaxFM6_sBOAMY9TaGsKw.png)

![Image](https://mdrk.io/content/images/2024/04/momentum_optimization_trajectory-1.png)

---

## ðŸ”¹ 1) Gradient Descent à¦•à§€?

**Gradient Descent** à¦¹à¦²à§‹ à¦à¦®à¦¨ à¦à¦•à¦Ÿà¦¿ optimization à¦ªà¦¦à§à¦§à¦¤à¦¿ à¦¯à¦¾
ðŸ‘‰ **loss/cost function à¦•à¦®à¦¾à¦¨à§‹à¦° à¦œà¦¨à§à¦¯ parameters (weights, bias) à¦†à¦ªà¦¡à§‡à¦Ÿ à¦•à¦°à§‡**à¥¤

Basic update rule:
[
\theta \leftarrow \theta - \eta \nabla J(\theta)
]

* (\eta) = learning rate
* (\nabla J) = gradient (à¦¢à¦¾à¦²)

---

## ðŸ”¹ 2) Types of Gradient Descent (Data à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦°à§‡à¦° à¦­à¦¿à¦¤à§à¦¤à¦¿à¦¤à§‡)

### âœ… **Batch Gradient Descent**

* à¦ªà§à¦°à§‹ dataset à¦à¦•à¦¸à¦¾à¦¥à§‡ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦°
* Stable à¦•à¦¿à¦¨à§à¦¤à§ **slow**

**Use when:** à¦›à§‹à¦Ÿ dataset

---

### âœ… **Stochastic Gradient Descent (SGD)**

* à¦à¦•à§‡à¦•à¦Ÿà¦¾ sample à¦¦à¦¿à§Ÿà§‡ update
* **Fast à¦•à¦¿à¦¨à§à¦¤à§ noisy**

**Use when:** à¦–à§à¦¬ à¦¬à§œ dataset

---

### âœ… **Mini-batch Gradient Descent**

* à¦›à§‹à¦Ÿ batch (32, 64, 128)
* Speed + stability balanced

**ðŸ‘‰ Practical default choice**

---

## ðŸ”¹ 3) Popular Gradient Descent Optimizers

### ðŸ”¸ **SGD (Vanilla)**

[
\theta \leftarrow \theta - \eta \nabla J(\theta)
]

âœ” Simple
âŒ Zig-zag, slow convergence

---

### ðŸ”¸ **Momentum**

à¦†à¦—à§‡à¦° gradient-à¦à¦° à¦¦à¦¿à¦• à¦§à¦°à§‡ à¦à¦—à§‹à§Ÿà¥¤

[
v_t=\beta v_{t-1}+\eta \nabla J
]
[
\theta \leftarrow \theta - v_t
]

âœ” Faster
âœ” Oscillation à¦•à¦®
âŒ Extra hyperparameter

---

### ðŸ”¸ **Nesterov Accelerated Gradient (NAG)**

à¦†à¦—à§‡ â€œpeekâ€ à¦•à¦°à§‡, à¦¤à¦¾à¦°à¦ªà¦° updateà¥¤

âœ” Momentum-à¦à¦° à¦šà§‡à§Ÿà§‡à¦“ smart
âœ” Overshoot à¦•à¦®

---

### ðŸ”¸ **Adagrad**

Parameter-wise adaptive learning rateà¥¤

[
\theta \leftarrow \theta - \frac{\eta}{\sqrt{G+\epsilon}} \nabla J
]

âœ” Sparse data-à¦ à¦­à¦¾à¦²à§‹
âŒ Learning rate à¦¦à§à¦°à§à¦¤ à¦›à§‹à¦Ÿ à¦¹à§Ÿà§‡ à¦¯à¦¾à§Ÿ

---

### ðŸ”¸ **RMSProp**

Adagrad-à¦à¦° à¦¸à¦®à¦¸à§à¦¯à¦¾ à¦ à¦¿à¦• à¦•à¦°à§‡à¥¤

âœ” Stable
âœ” RNN/CNN-à¦ à¦­à¦¾à¦²à§‹
âŒ LR tuning à¦¦à¦°à¦•à¦¾à¦°

---

### ðŸ”¸ **Adam (Most Popular)**

Momentum + RMSProp à¦à¦•à¦¸à¦¾à¦¥à§‡à¥¤

[
m_t=\beta_1 m_{t-1}+(1-\beta_1)g_t
]
[
v_t=\beta_2 v_{t-1}+(1-\beta_2)g_t^2
]

âœ” Fast convergence
âœ” Default choice
âŒ Sometimes overfits / sharp minima

---

### ðŸ”¸ **AdamW**

Adam + proper weight decayà¥¤

âœ” Better generalization
âœ” Modern best practice

---

## ðŸ”¹ 4) Optimizers Comparison Table

| Optimizer | Speed         | Stability | Best Use            |
| --------- | ------------- | --------- | ------------------- |
| SGD       | Slow          | Low       | Simple baseline     |
| Momentum  | Medium        | Medium    | Faster SGD          |
| NAG       | Fast          | High      | Smooth convergence  |
| Adagrad   | Medium        | Medium    | Sparse features     |
| RMSProp   | Fast          | High      | RNN/CNN             |
| **Adam**  | **Very Fast** | **High**  | Default choice      |
| **AdamW** | **Very Fast** | **High**  | Best generalization |

---

## ðŸ”¹ 5) Which Optimizer to Use â€” When?

| Situation         | Optimizer      |
| ----------------- | -------------- |
| Just starting     | **Adam**       |
| Overfitting issue | **AdamW**      |
| Sparse data (NLP) | Adagrad / Adam |
| Very large data   | SGD + Momentum |
| RNN unstable      | RMSProp / Adam |

---

## ðŸ§  Memory Trick

> **SGD walks, Momentum runs, Adam thinks.**

---

## âœï¸ Exam-Ready One Line

> **Gradient descent optimizers iteratively update model parameters to minimize loss, with advanced variants like Adam and AdamW improving convergence speed and stability.**

à¦šà¦¾à¦“ à¦¤à§‹ à¦†à¦®à¦¿ à¦ªà¦°à§‡à¦° à¦§à¦¾à¦ªà§‡

* **Adam vs SGD numerical example**,
* **learning rate schedules**,
* à¦¬à¦¾ **exam 5-mark answer (with formulas)**
  à¦¤à§ˆà¦°à¦¿ à¦•à¦°à§‡ à¦¦à¦¿à¦¤à§‡ à¦ªà¦¾à¦°à¦¿ âœ”
