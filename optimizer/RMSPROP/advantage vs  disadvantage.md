<<<<<<< HEAD
## **RMSProp â€” Advantage vs Disadvantage (à¦¸à¦‚à¦•à§à¦·à§‡à¦ªà§‡, Exam-Ready)**

---

## âœ… **Advantages of RMSProp**

1. **Adaptive Learning Rate (per-parameter)**
   à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ parameter-à¦à¦° à¦œà¦¨à§à¦¯ à¦†à¦²à¦¾à¦¦à¦¾ learning rate adjust à¦¹à§Ÿà¥¤

2. **Adagrad-à¦à¦° à¦¸à¦®à¦¸à§à¦¯à¦¾ à¦¸à¦®à¦¾à¦§à¦¾à¦¨ à¦•à¦°à§‡**
   Learning rate à¦†à¦° à¦à¦•à¦¦à¦® 0 à¦¹à§Ÿà§‡ à¦¯à¦¾à§Ÿ à¦¨à¦¾â€”training à¦¥à§‡à¦®à§‡ à¦¯à¦¾à§Ÿ à¦¨à¦¾à¥¤

3. **Non-stationary problems-à¦ à¦­à¦¾à¦²à§‹**
   Loss landscape à¦¸à¦®à§Ÿà§‡à¦° à¦¸à¦¾à¦¥à§‡ à¦¬à¦¦à¦²à¦¾à¦²à§‡à¦“ à¦­à¦¾à¦²à§‹ à¦•à¦¾à¦œ à¦•à¦°à§‡à¥¤

4. **Zig-zag à¦•à¦®à¦¾à§Ÿ**
   Steep + narrow valley-à¦¤à§‡ movement smoother à¦¹à§Ÿà¥¤

5. **Deep Learning-à¦ à¦•à¦¾à¦°à§à¦¯à¦•à¦°**
   à¦¬à¦¿à¦¶à§‡à¦· à¦•à¦°à§‡ **RNN / LSTM**-à¦ à¦–à§à¦¬ à¦œà¦¨à¦ªà§à¦°à¦¿à§Ÿà¥¤

---

## âŒ **Disadvantages of RMSProp**

1. **Momentum à¦†à¦²à¦¾à¦¦à¦¾ à¦•à¦°à§‡ à¦¨à§‡à¦‡**
   Directional acceleration à¦¨à§‡à¦‡ (Adam-à¦ à¦à¦Ÿà¦¾ à¦†à¦›à§‡)à¥¤

2. **Hyperparameter tune à¦•à¦°à¦¤à§‡ à¦¹à§Ÿ**
   Learning rate (Î·), decay rate (Ï) à¦ à¦¿à¦• à¦•à¦°à¦¾ à¦²à¦¾à¦—à§‡à¥¤

3. **Adam-à¦à¦° à¦¤à§à¦²à¦¨à¦¾à§Ÿ à¦§à§€à¦°**
   Adam à¦¸à¦¾à¦§à¦¾à¦°à¦£à¦¤ à¦†à¦°à¦“ à¦¦à§à¦°à§à¦¤ à¦“ stable converge à¦•à¦°à§‡à¥¤

4. **Theoretical guarantee à¦¦à§à¦°à§à¦¬à¦²**
   Strong convergence proof à¦¨à§‡à¦‡ (SGD-à¦à¦° à¦®à¦¤à§‹ à¦¨à§Ÿ)à¥¤

---

## ğŸ“Š **Quick Comparison**

| Feature          | RMSProp       |
| ---------------- | ------------- |
| Learning rate    | Adaptive      |
| Sparse data      | âœ… Good        |
| Long training    | âœ… Stable      |
| Momentum         | âŒ No          |
| Used in practice | RNN, DL       |
| Replaced by      | Adam (mostly) |

---

## ğŸ§  **Memory Trick**

> **RMSProp = Adagrad with memory loss (but no momentum)**

---

## âœï¸ **Exam-Ready One-Liners**

* **Advantage:**
  *RMSProp stabilizes learning by using an exponentially decaying average of squared gradients.*

* **Disadvantage:**
  *RMSProp lacks momentum and is often outperformed by Adam in practice.*

=======
## **RMSProp â€” Advantage vs Disadvantage (à¦¸à¦‚à¦•à§à¦·à§‡à¦ªà§‡, Exam-Ready)**

---

## âœ… **Advantages of RMSProp**

1. **Adaptive Learning Rate (per-parameter)**
   à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ parameter-à¦à¦° à¦œà¦¨à§à¦¯ à¦†à¦²à¦¾à¦¦à¦¾ learning rate adjust à¦¹à§Ÿà¥¤

2. **Adagrad-à¦à¦° à¦¸à¦®à¦¸à§à¦¯à¦¾ à¦¸à¦®à¦¾à¦§à¦¾à¦¨ à¦•à¦°à§‡**
   Learning rate à¦†à¦° à¦à¦•à¦¦à¦® 0 à¦¹à§Ÿà§‡ à¦¯à¦¾à§Ÿ à¦¨à¦¾â€”training à¦¥à§‡à¦®à§‡ à¦¯à¦¾à§Ÿ à¦¨à¦¾à¥¤

3. **Non-stationary problems-à¦ à¦­à¦¾à¦²à§‹**
   Loss landscape à¦¸à¦®à§Ÿà§‡à¦° à¦¸à¦¾à¦¥à§‡ à¦¬à¦¦à¦²à¦¾à¦²à§‡à¦“ à¦­à¦¾à¦²à§‹ à¦•à¦¾à¦œ à¦•à¦°à§‡à¥¤

4. **Zig-zag à¦•à¦®à¦¾à§Ÿ**
   Steep + narrow valley-à¦¤à§‡ movement smoother à¦¹à§Ÿà¥¤

5. **Deep Learning-à¦ à¦•à¦¾à¦°à§à¦¯à¦•à¦°**
   à¦¬à¦¿à¦¶à§‡à¦· à¦•à¦°à§‡ **RNN / LSTM**-à¦ à¦–à§à¦¬ à¦œà¦¨à¦ªà§à¦°à¦¿à§Ÿà¥¤

---

## âŒ **Disadvantages of RMSProp**

1. **Momentum à¦†à¦²à¦¾à¦¦à¦¾ à¦•à¦°à§‡ à¦¨à§‡à¦‡**
   Directional acceleration à¦¨à§‡à¦‡ (Adam-à¦ à¦à¦Ÿà¦¾ à¦†à¦›à§‡)à¥¤

2. **Hyperparameter tune à¦•à¦°à¦¤à§‡ à¦¹à§Ÿ**
   Learning rate (Î·), decay rate (Ï) à¦ à¦¿à¦• à¦•à¦°à¦¾ à¦²à¦¾à¦—à§‡à¥¤

3. **Adam-à¦à¦° à¦¤à§à¦²à¦¨à¦¾à§Ÿ à¦§à§€à¦°**
   Adam à¦¸à¦¾à¦§à¦¾à¦°à¦£à¦¤ à¦†à¦°à¦“ à¦¦à§à¦°à§à¦¤ à¦“ stable converge à¦•à¦°à§‡à¥¤

4. **Theoretical guarantee à¦¦à§à¦°à§à¦¬à¦²**
   Strong convergence proof à¦¨à§‡à¦‡ (SGD-à¦à¦° à¦®à¦¤à§‹ à¦¨à§Ÿ)à¥¤

---

## ğŸ“Š **Quick Comparison**

| Feature          | RMSProp       |
| ---------------- | ------------- |
| Learning rate    | Adaptive      |
| Sparse data      | âœ… Good        |
| Long training    | âœ… Stable      |
| Momentum         | âŒ No          |
| Used in practice | RNN, DL       |
| Replaced by      | Adam (mostly) |

---

## ğŸ§  **Memory Trick**

> **RMSProp = Adagrad with memory loss (but no momentum)**

---

## âœï¸ **Exam-Ready One-Liners**

* **Advantage:**
  *RMSProp stabilizes learning by using an exponentially decaying average of squared gradients.*

* **Disadvantage:**
  *RMSProp lacks momentum and is often outperformed by Adam in practice.*

>>>>>>> f45ebbad1686e699afe9932c4175eeff501d254b
