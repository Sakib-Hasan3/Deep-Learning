<<<<<<< HEAD
---

## ğŸ¯ Problem Setup (Very Small)

à¦§à¦°à¦¿ à¦†à¦®à¦°à¦¾ à¦à¦‡ loss function minimize à¦•à¦°à¦¤à§‡ à¦šà¦¾à¦‡:


$$
J(w) = \frac{1}{2}(w-2)^2
$$

Gradient:

$$
\frac{dJ}{dw} = w-2
$$

---

### Initial values


* Initial weight: $w_0 = 0$
* Learning rate: $\eta = 0.1$
* Decay rate: $\rho = 0.9$
* $\epsilon = 10^{-8}$
* Initial squared-gradient accumulator:
  $$
  s_0 = 0
  $$

---

## ğŸ” RMSProp Formula (Reminder)


$$
s_t = \rho s_{t-1} + (1-\rho) g_t^2
$$


$$
w \leftarrow w - \frac{\eta}{\sqrt{s_t}+\epsilon} \cdot g_t
$$

---

## ğŸ§® Iteration 1

**Gradient**

$$
g_0 = w_0 - 2 = 0 - 2 = -2
$$

**Squared gradient average**

$$
s_1 = 0.9(0) + 0.1( (-2)^2 ) = 0.4
$$

**Weight update**

$$
w_1 = 0 - \frac{0.1}{\sqrt{0.4}}(-2)
$$


$$
\sqrt{0.4} \approx 0.632
$$


$$
w_1 \approx 0 + 0.316 = 0.316
$$

---

## ğŸ§® Iteration 2

**Gradient**

$$
g_1 = 0.316 - 2 = -1.684
$$

**Squared gradient average**

$$
s_2 = 0.9(0.4) + 0.1(1.684^2)
$$


$$
s_2 = 0.36 + 0.283 \approx 0.643
$$

**Weight update**

$$
w_2 = 0.316 - \frac{0.1}{\sqrt{0.643}}(-1.684)
$$


$$
\sqrt{0.643} \approx 0.802
$$


$$
w_2 \approx 0.316 + 0.21 = 0.526
$$

---

## ğŸ” Observation (Important)

| Step  | Weight |
| ----- | ------ |
| (w_0) | 0      |
| (w_1) | 0.316  |
| (w_2) | 0.526  |

* Gradient à¦•à¦®à¦›à§‡
* (s_t) smoothà¦­à¦¾à¦¬à§‡ à¦¬à¦¾à§œà¦›à§‡
* Learning rate **self-adjust** à¦¹à¦šà§à¦›à§‡
* Update stable (overshoot à¦¨à§‡à¦‡)

---

## ğŸ§  Intuition (One Line)

> RMSProp recent gradients à¦¦à§‡à¦–à§‡ step size à¦ à¦¿à¦• à¦•à¦°à§‡, à¦¤à¦¾à¦‡ learning smooth à¦“ stable à¦¹à§Ÿà¥¤

---

## âœï¸ Exam-Ready Mini Summary

**RMSProp scales the learning rate by the root mean square of recent gradients, preventing aggressive decay and stabilizing training.**

---
=======
---

## ğŸ¯ Problem Setup (Very Small)

à¦§à¦°à¦¿ à¦†à¦®à¦°à¦¾ à¦à¦‡ loss function minimize à¦•à¦°à¦¤à§‡ à¦šà¦¾à¦‡:


$$
J(w) = \frac{1}{2}(w-2)^2
$$

Gradient:

$$
\frac{dJ}{dw} = w-2
$$

---

### Initial values


* Initial weight: $w_0 = 0$
* Learning rate: $\eta = 0.1$
* Decay rate: $\rho = 0.9$
* $\epsilon = 10^{-8}$
* Initial squared-gradient accumulator:
  $$
  s_0 = 0
  $$

---

## ğŸ” RMSProp Formula (Reminder)


$$
s_t = \rho s_{t-1} + (1-\rho) g_t^2
$$


$$
w \leftarrow w - \frac{\eta}{\sqrt{s_t}+\epsilon} \cdot g_t
$$

---

## ğŸ§® Iteration 1

**Gradient**

$$
g_0 = w_0 - 2 = 0 - 2 = -2
$$

**Squared gradient average**

$$
s_1 = 0.9(0) + 0.1( (-2)^2 ) = 0.4
$$

**Weight update**

$$
w_1 = 0 - \frac{0.1}{\sqrt{0.4}}(-2)
$$


$$
\sqrt{0.4} \approx 0.632
$$


$$
w_1 \approx 0 + 0.316 = 0.316
$$

---

## ğŸ§® Iteration 2

**Gradient**

$$
g_1 = 0.316 - 2 = -1.684
$$

**Squared gradient average**

$$
s_2 = 0.9(0.4) + 0.1(1.684^2)
$$


$$
s_2 = 0.36 + 0.283 \approx 0.643
$$

**Weight update**

$$
w_2 = 0.316 - \frac{0.1}{\sqrt{0.643}}(-1.684)
$$


$$
\sqrt{0.643} \approx 0.802
$$


$$
w_2 \approx 0.316 + 0.21 = 0.526
$$

---

## ğŸ” Observation (Important)

| Step  | Weight |
| ----- | ------ |
| (w_0) | 0      |
| (w_1) | 0.316  |
| (w_2) | 0.526  |

* Gradient à¦•à¦®à¦›à§‡
* (s_t) smoothà¦­à¦¾à¦¬à§‡ à¦¬à¦¾à§œà¦›à§‡
* Learning rate **self-adjust** à¦¹à¦šà§à¦›à§‡
* Update stable (overshoot à¦¨à§‡à¦‡)

---

## ğŸ§  Intuition (One Line)

> RMSProp recent gradients à¦¦à§‡à¦–à§‡ step size à¦ à¦¿à¦• à¦•à¦°à§‡, à¦¤à¦¾à¦‡ learning smooth à¦“ stable à¦¹à§Ÿà¥¤

---

## âœï¸ Exam-Ready Mini Summary

**RMSProp scales the learning rate by the root mean square of recent gradients, preventing aggressive decay and stabilizing training.**

---
>>>>>>> f45ebbad1686e699afe9932c4175eeff501d254b
