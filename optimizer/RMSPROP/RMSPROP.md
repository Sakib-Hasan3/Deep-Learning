## **RMSProp (Root Mean Square Propagation) тАФ Graph рж╕рж╣ ржмрж┐рж╕рзНрждрж╛рж░рж┐ржд ржмрзНржпрж╛ржЦрзНржпрж╛**

![Image](https://miro.medium.com/1%2Aklmbvjq7PSj6tmuDoyTGaA.png)

![Image](https://ml-explained.com/articles/rmsprop-explained/rmsprop_example.PNG)

![Image](https://miro.medium.com/v2/resize%3Afit%3A1240/1%2AY2KPVGrVX9MQkeI8Yjy59Q.gif)

![Image](https://d2l.ai/_images/output_rmsprop_251805_15_1.svg)

---

## 1я╕ПтГг RMSProp ржХрзА?

**RMSProp** рж╣рж▓рзЛ ржПржХржЯрж┐ **adaptive gradient descent optimizer**, ржпрж╛ **Adagrad-ржПрж░ ржорзВрж▓ рж╕ржорж╕рзНржпрж╛ржЯрж╛ ржарж┐ржХ ржХрж░рж╛рж░ ржЬржирзНржп** рждрзИрж░рж┐ ржХрж░рж╛ рж╣рзЯрзЗржЫрзЗред

ЁЯСЙ Adagrad-ржП learning rate рж╕ржорзЯрзЗрж░ рж╕рж╛ржерзЗ ржЦрзБржм ржЫрзЛржЯ рж╣рзЯрзЗ ржпрж╛рзЯ
ЁЯСЙ RMSProp-ржП **рж╕рж╛ржорзНржкрзНрж░рждрж┐ржХ gradient-ржПрж░ moving average** ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛ рж╣рзЯ
ЁЯСЙ ржлрж▓рзЗ learning rate **ржкрзБрж░рзЛржкрзБрж░рж┐ ржорж░рзЗ ржпрж╛рзЯ ржирж╛**

---

## 2я╕ПтГг ржХрзЗржи RMSProp ржжрж░ржХрж╛рж░ рж╣рзЯрзЗржЫрж┐рж▓?

### Adagrad-ржПрж░ рж╕ржорж╕рзНржпрж╛:

* Squared gradient рж╕ржмрж╕ржорзЯ ржЬржорждрзЗ ржерж╛ржХрзЗ
* Denominator ржХрзНрж░ржорж╛ржЧржд ржмрзЬ рж╣рзЯ
* Learning rate тЖТ ржкрзНрж░рж╛рзЯ 0
* Training ржЖржЧрзЗржЗ ржерзЗржорзЗ ржпрж╛рзЯ

### RMSProp рж╕ржорж╛ржзрж╛ржи:

* рж╕ржм gradient ржЬржорж╛рзЯ ржирж╛
* **Exponential Moving Average (EMA)** ржирзЗрзЯ
* ржкрзБрж░рзЛржирзЛ gradient ржзрзАрж░рзЗ ржзрзАрж░рзЗ ржнрзБрж▓рзЗ ржпрж╛рзЯ

---

## 3я╕ПтГг Mathematical Formulation (Core)

ржзрж░рж┐,

* ( \theta ) = parameter
* ( g_t = \nabla J(\theta_t) ) = current gradient
* ( \eta ) = learning rate
* ( \rho ) = decay rate (рж╕рж╛ржзрж╛рж░ржгржд 0.9)
* ( \epsilon ) = small constant (ржпрзЗржоржи (10^{-8}))

### StepтАУ1: Squared gradient-ржПрж░ moving average


$$
s_t = \rho s_{t-1} + (1-\rho) g_t^2
$$

### StepтАУ2: Parameter update


$$
	heta_{t+1} = \theta_t - \frac{\eta}{\sqrt{s_t} + \epsilon} \cdot g_t
$$

---

## 4я╕ПтГг Intuition (рж╕рж╣ржЬ ржнрж╛рж╖рж╛рзЯ)

* (s_t) тЙИ recent gradient magnitude
* Gradient ржмрзЬ рж╣рж▓рзЗ тЖТ learning rate ржЫрзЛржЯ
* Gradient ржЫрзЛржЯ рж╣рж▓рзЗ тЖТ learning rate ржмрзЬ

ЁЯСЙ **Per-parameter adaptive step**, ржХрж┐ржирзНрждрзБ Adagrad-ржПрж░ ржорждрзЛ ржЬржорзЗ ржерж╛ржХрзЗ ржирж╛ред

Graph-ржП:

* SGD тЖТ zig-zag
* Adagrad тЖТ slow рж╣рзЯрзЗ ржерзЗржорзЗ ржпрж╛рзЯ
* RMSProp тЖТ smooth + stable descent

---

## 5я╕ПтГг RMSProp-ржП ╬╡ (epsilon) ржХрзЗржи ржжрж░ржХрж╛рж░?

* Division by zero ржПрзЬрж╛рждрзЗ
* Numerical stability рж░рж╛ржЦрждрзЗ
* Very small denominator prevent ржХрж░рждрзЗ

ЁЯСЙ ╬╡ рж╕рж╛ржзрж╛рж░ржгржд **(10^{-8})**

---

## 6я╕ПтГг Hyperparameters ржУ рждрж╛ржжрзЗрж░ ржнрзВржорж┐ржХрж╛

| Parameter | Meaning            | Typical value |
| --------- | ------------------ | ------------- |
| ╬╖         | Learning rate      | 0.001         |
| ╧Б         | Decay rate         | 0.9           |
| ╬╡         | Stability constant | 1eтИТ8          |

---

## 7я╕ПтГг Advantages (рж╕рзБржмрж┐ржзрж╛)

тЬФ Adagrad-ржПрж░ learning-rate-decay рж╕ржорж╕рзНржпрж╛ рж╕ржорж╛ржзрж╛ржи
тЬФ Non-stationary problems-ржП ржнрж╛рж▓рзЛ
тЬФ Zig-zag ржХржорж╛рзЯ
тЬФ Deep learning-ржП ржнрж╛рж▓рзЛ ржХрж╛ржЬ ржХрж░рзЗ

---

## 8я╕ПтГг Disadvantages (ржЕрж╕рзБржмрж┐ржзрж╛)

тЭМ Momentum explicitly ржирзЗржЗ
тЭМ Adam-ржПрж░ рждрзБрж▓ржирж╛рзЯ ржПржХржЯрзБ slow
тЭМ Learning rate ржПржЦржирзЛ tune ржХрж░рж╛ рж▓рж╛ржЧрзЗ

---

## 9я╕ПтГг RMSProp vs Adagrad vs SGD (Concept)

* **SGD:**
  Fixed learning rate, noisy path

* **Adagrad:**
  Learning rate ржХржорждрзЗ ржХржорждрзЗ ржерзЗржорзЗ ржпрж╛рзЯ

* **RMSProp:**
  Recent gradient ржжрзЗржЦрзЗ adaptive, stable learning

---

## 10я╕ПтГг ржХрзЛржерж╛рзЯ RMSProp ржнрж╛рж▓рзЛ?

тЬЕ RNN / LSTM
тЬЕ Non-stationary loss landscape
тЬЕ Deep neural networks

---

## ЁЯза ржоржирзЗ рж░рж╛ржЦрж╛рж░ ржЯрзНрж░рж┐ржХ

> **RMSProp = Adagrad but with memory loss**

(ржкрзБрж░рзЛржирзЛ gradient ржнрзБрж▓рзЗ ржпрж╛рзЯ)

---

## тЬНя╕П Exam-Ready One-Line

**RMSProp is an adaptive optimization algorithm that maintains an exponentially decaying average of squared gradients to stabilize learning and prevent aggressive learning rate decay.**

---

ржЪрж╛ржУ рждрзЛ ржЖржорж┐ ржкрж░рзЗрж░ ржзрж╛ржкрзЗ

* **RMSProp numerical example (step-by-step)**
* **RMSProp vs Adam (graph + math)**
* **Why Adam = Momentum + RMSProp**

ржпрзЗржХрзЛржирзЛржЯрж╛ ржмрзБржЭрж┐рзЯрзЗ ржжрж┐рждрзЗ ржкрж╛рж░рж┐ред
