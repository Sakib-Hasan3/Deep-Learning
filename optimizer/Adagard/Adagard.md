## **Adagrad (Adaptive Gradient Algorithm) â€” Graph à¦¸à¦¹ à¦¬à¦¿à¦¸à§à¦¤à¦¾à¦°à¦¿à¦¤ à¦¬à§à¦¯à¦¾à¦–à§à¦¯à¦¾**

![Image](https://akyrillidis.github.io/notes/AdaGrad/GDvsAdaGrad2.png)

![Image](https://editor.analyticsvidhya.com/uploads/121381obtV.gif)

![Image](https://www.researchgate.net/publication/366248133/figure/fig3/AS%3A11431281171612226%401688263275116/The-limitation-of-AdaGrad-method.png)

![Image](https://miro.medium.com/1%2ALv7-jMtHOoucryv9mUtFGg.jpeg)

---

## 1ï¸âƒ£ Adagrad à¦•à§€?

**Adagrad (Adaptive Gradient Algorithm)** à¦¹à¦²à§‹ à¦à¦•à¦Ÿà¦¿ **gradient descent optimizer**
à¦¯à§‡à¦–à¦¾à¦¨à§‡ ðŸ‘‰ **à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾ parameter-à¦à¦° à¦œà¦¨à§à¦¯ à¦†à¦²à¦¾à¦¦à¦¾ learning rate à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à¦¾ à¦¹à§Ÿ**à¥¤

à¦®à§‚à¦² à¦§à¦¾à¦°à¦£à¦¾:

> à¦¯à§‡à¦¸à¦¬ parameter-à¦ gradient à¦¬à¦¾à¦°à¦¬à¦¾à¦° à¦¬à§œ à¦¹à§Ÿ â†’ à¦¤à¦¾à¦¦à§‡à¦° learning rate à¦•à¦®à§‡
> à¦¯à§‡à¦¸à¦¬ parameter-à¦ gradient à¦•à¦®/rare â†’ à¦¤à¦¾à¦¦à§‡à¦° learning rate à¦¬à§‡à¦¶à¦¿ à¦¥à¦¾à¦•à§‡

---

## 2ï¸âƒ£ à¦•à§‡à¦¨ Adagrad à¦¦à¦°à¦•à¦¾à¦°?

SGD-à¦¤à§‡:

* à¦¸à¦¬ parameter-à¦à¦° à¦œà¦¨à§à¦¯ **à¦à¦•à¦‡ learning rate**
* Sparse feature (NLP, text) à¦¥à¦¾à¦•à¦²à§‡ à¦¸à¦®à¦¸à§à¦¯à¦¾

Adagrad:

* **Frequent feature â†’ à¦›à§‹à¦Ÿ step**
* **Rare feature â†’ à¦¬à§œ step**

ðŸ‘‰ à¦¤à¦¾à¦‡ sparse data-à¦¤à§‡ à¦–à§à¦¬ à¦­à¦¾à¦²à§‹ à¦•à¦¾à¦œ à¦•à¦°à§‡à¥¤

---

## 3ï¸âƒ£ Mathematical Formulation (Core)

à¦§à¦°à¦¿,

* ( \theta ) = parameter
* ( g_t = \nabla J(\theta_t) ) = current gradient
* ( \eta ) = initial learning rate
* ( \epsilon ) = small constant (e.g. (10^{-8}))

### Stepâ€“1: Accumulate squared gradients


$$
G_t = G_{t-1} + g_t^2
$$

### Stepâ€“2: Adaptive update


$$
	heta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t}+\epsilon} \cdot g_t
$$

ðŸ‘‰ à¦à¦–à¦¾à¦¨à§‡ **learning rate à¦¸à¦®à§Ÿà§‡à¦° à¦¸à¦¾à¦¥à§‡ à¦•à¦®à¦¤à§‡ à¦¥à¦¾à¦•à§‡**à¥¤

---

## 4ï¸âƒ£ Intuition (à¦¸à¦¹à¦œà¦­à¦¾à¦¬à§‡)

* à¦¶à§à¦°à§à¦¤à§‡ (G_t) à¦›à§‹à¦Ÿ â‡’ learning rate à¦¬à§œ
* Training à¦šà¦²à¦¾à¦° à¦¸à¦¾à¦¥à§‡ à¦¸à¦¾à¦¥à§‡ (G_t) à¦¬à¦¾à§œà§‡ â‡’ learning rate à¦›à§‹à¦Ÿ

Graph-à¦ à¦¦à§‡à¦–à¦¾ à¦¯à¦¾à§Ÿ:

* à¦¶à§à¦°à§à¦¤à§‡ à¦¦à§à¦°à§à¦¤ à¦¨à¦¾à¦®à¦¾
* à¦ªà¦°à§‡ à¦§à§€à¦°à§‡ à¦§à§€à¦°à§‡ almost à¦¥à§‡à¦®à§‡ à¦¯à¦¾à¦“à§Ÿà¦¾

---

## 5ï¸âƒ£ Example (1-Parameter Intuition)

à¦§à¦°à¦¿ gradient à¦¸à¦¬à¦¸à¦®à§Ÿ (g=1)


* Step 1:
  $$
  G_1 = 1^2 = 1,\quad \eta_{\text{eff}} = \frac{\eta}{\sqrt{1}} = \eta
  $$


* Step 10:
  $$
  G_{10} = 10,\quad \eta_{\text{eff}} = \frac{\eta}{\sqrt{10}}
  $$

ðŸ‘‰ Learning rate **monotonically decay** à¦•à¦°à§‡à¥¤

---

## 6ï¸âƒ£ Adagrad-à¦à¦° à¦¸à§à¦¬à¦¿à¦§à¦¾ (Advantages)

âœ” Parameter-wise adaptive learning rate
âœ” Sparse data-à¦¤à§‡ excellent
âœ” Learning rate manually tune à¦•à¦® à¦²à¦¾à¦—à§‡
âœ” NLP / text / recommendation systems-à¦ à¦­à¦¾à¦²à§‹

---

## 7ï¸âƒ£ Adagrad-à¦à¦° à¦…à¦¸à§à¦¬à¦¿à¦§à¦¾ (Disadvantages)

âŒ Learning rate à¦–à§à¦¬ à¦¦à§à¦°à§à¦¤ à¦›à§‹à¦Ÿ à¦¹à§Ÿà§‡ à¦¯à¦¾à§Ÿ
âŒ Deep learning-à¦ training à¦†à¦—à§‡à¦‡ à¦¥à§‡à¦®à§‡ à¦¯à§‡à¦¤à§‡ à¦ªà¦¾à¦°à§‡
âŒ Non-stationary problems-à¦ à¦–à¦¾à¦°à¦¾à¦ª

ðŸ‘‰ à¦à¦‡ à¦¸à¦®à¦¸à§à¦¯à¦¾à¦° à¦œà¦¨à§à¦¯à¦‡ **RMSProp, Adam** à¦à¦¸à§‡à¦›à§‡à¥¤

---

## 8ï¸âƒ£ Adagrad vs SGD (Quick Compare)

| Feature         | SGD    | Adagrad       |
| --------------- | ------ | ------------- |
| Learning rate   | Fixed  | Adaptive      |
| Sparse features | âŒ Weak | âœ… Strong      |
| Long training   | âœ… OK   | âŒ Stops early |
| Hyperparameters | Few    | Few           |

---

## 9ï¸âƒ£ à¦•à§‹à¦¥à¦¾à§Ÿ Adagrad à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à¦¾ à¦¹à§Ÿ?

âœ… NLP (word frequency imbalance)
âœ… Recommendation systems
âœ… Sparse feature space
âŒ Deep CNN training (modern DL)

---

## ðŸ§  Memory Trick

> **Adagrad = Frequent feature slow, rare feature fast**

---

## âœï¸ Exam-Ready One Line

> **Adagrad is an adaptive optimization algorithm that adjusts the learning rate for each parameter based on the historical sum of squared gradients, making it effective for sparse data.**

---

à¦šà¦¾à¦“ à¦¤à§‹ à¦†à¦®à¦¿ à¦ªà¦°à§‡à¦° à¦§à¦¾à¦ªà§‡

* **Adagrad numerical example**,
* **Adagrad vs RMSProp vs Adam**,
* à¦¬à¦¾ **optimizer evolution chart (SGD â†’ Adam)**
  à¦¦à§‡à¦–à¦¿à§Ÿà§‡ à¦¦à¦¿à¦¤à§‡ à¦ªà¦¾à¦°à¦¿à¥¤
