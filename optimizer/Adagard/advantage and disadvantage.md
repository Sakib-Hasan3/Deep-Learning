## **Adagrad â€” Advantages & Disadvantages (Clear + Exam-Ready)**

![Image](https://akyrillidis.github.io/notes/AdaGrad/GDvsAdaGrad2.png)

![Image](https://editor.analyticsvidhya.com/uploads/121381obtV.gif)

![Image](https://www.researchgate.net/publication/337553694/figure/fig3/AS%3A829667233771520%401574819491019/a-shows-the-advantage-of-Adagrad-with-the-adaptive-learning-rate-over-other-methods-b.png)

---

## âœ… **Advantages of Adagrad**

1. **Adaptive Learning Rate (per-parameter)**
   à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ parameter à¦¨à¦¿à¦œà§‡à¦° historical gradient à¦…à¦¨à§à¦¯à¦¾à§Ÿà§€ à¦†à¦²à¦¾à¦¦à¦¾ learning rate à¦ªà¦¾à§Ÿà¥¤

2. **Sparse Data-à¦¤à§‡ à¦–à§à¦¬ à¦­à¦¾à¦²à§‹**
   Rare features (NLP, text, recommender systems) à¦¬à§‡à¦¶à¦¿ update à¦ªà¦¾à§Ÿâ€”learning à¦•à¦¾à¦°à§à¦¯à¦•à¦° à¦¹à§Ÿà¥¤

3. **Manual Tuning à¦•à¦® à¦²à¦¾à¦—à§‡**
   à¦à¦•à¦Ÿà¦¾à¦‡ initial learning rateâ€”à¤¬à¤¾à¤•à¦¿à¦Ÿà¦¾ algorithm à¦¨à¦¿à¦œà§‡ à¦¸à¦¾à¦®à¦²à¦¾à§Ÿà¥¤

4. **Fast Initial Convergence**
   Training-à¦à¦° à¦¶à§à¦°à§à¦¤à§‡ à¦¦à§à¦°à§à¦¤ loss à¦•à¦®à§‡à¥¤

---

## âŒ **Disadvantages of Adagrad**

1. **Learning Rate à¦–à§à¦¬ à¦¦à§à¦°à§à¦¤ à¦•à¦®à§‡ à¦¯à¦¾à§Ÿ**
   Squared gradients à¦œà¦®à¦¤à§‡ à¦œà¦®à¦¤à§‡ denominator à¦¬à§œ à¦¹à§Ÿ â‡’ update à¦ªà§à¦°à¦¾à§Ÿ à¦¥à§‡à¦®à§‡ à¦¯à¦¾à§Ÿà¥¤

2. **Long Training-à¦ à¦¸à¦®à¦¸à§à¦¯à¦¾**
   Deep networks à¦¬à¦¾ à¦¦à§€à¦°à§à¦˜ training-à¦ convergence à¦†à¦—à§‡à¦‡ stagnate à¦•à¦°à§‡à¥¤

3. **Non-stationary Problems-à¦ à¦–à¦¾à¦°à¦¾à¦ª**
   Data distribution à¦¬à¦¦à¦²à¦¾à¦²à§‡ (à¦¸à¦®à§Ÿà¦­à§‡à¦¦à§‡) à¦†à¦—à§‡à¦° à¦œà¦®à¦¾ gradients à¦¸à¦®à¦¸à§à¦¯à¦¾ à¦•à¦°à§‡à¥¤

4. **Modern DL-à¦ à¦•à¦® à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦°**
   RMSProp/Adam à¦à¦‡ à¦¸à¦®à¦¸à§à¦¯à¦¾à¦—à§à¦²à§‹ à¦­à¦¾à¦²à§‹à¦­à¦¾à¦¬à§‡ à¦¸à¦®à¦¾à¦§à¦¾à¦¨ à¦•à¦°à§‡à¦›à§‡à¥¤

---

## ğŸ“Š **Quick Comparison (Adagrad vs Others)**

| Feature         | Adagrad                  |
| --------------- | ------------------------ |
| Learning rate   | Adaptive (per-parameter) |
| Sparse features | â­ Excellent              |
| Long training   | âŒ Weak                   |
| Deep learning   | âŒ Rare                   |
| Replaced by     | RMSProp, Adam            |

---

## ğŸ§  **Memory Trick**

> **Adagrad starts fast, then runs out of fuel.**

---

## âœï¸ **Exam-Ready One-Liners**

* **Advantage:**
  *Adagrad adapts the learning rate for each parameter, making it highly effective for sparse data.*

* **Disadvantage:**
  *Adagradâ€™s learning rate decays monotonically, which can cause training to stop prematurely.*

