<<<<<<< HEAD
à¦ à¦¿à¦• à¦†à¦›à§‡ ðŸ™‚
à¦à¦¬à¦¾à¦° **Exploding Gradient Problem**-à¦Ÿà¦¾ **à¦¸à¦¹à¦œ à¦¬à¦¾à¦‚à¦²à¦¾ + à¦—à¦¾à¦£à¦¿à¦¤à¦¿à¦• à¦‰à¦¦à¦¾à¦¹à¦°à¦£à¦¸à¦¹** à¦¬à§à¦à¦¿à§Ÿà§‡ à¦¦à¦¿à¦šà§à¦›à¦¿à¥¤

---

## ðŸ”¥ Exploding Gradient Problem à¦•à§€?

Backpropagation à¦•à¦°à¦¾à¦° à¦¸à¦®à§Ÿ à¦¯à¦¦à¦¿ gradient-à¦à¦° à¦®à¦¾à¦¨ **à¦–à§à¦¬ à¦¦à§à¦°à§à¦¤ (exponential à¦¹à¦¾à¦°à§‡) à¦¬à§œ à¦¹à§Ÿà§‡ à¦¯à¦¾à§Ÿ**, à¦¤à¦–à¦¨ à¦¤à¦¾à¦•à§‡ **Exploding Gradient Problem** à¦¬à¦²à§‡à¥¤
à¦à¦° à¦«à¦²à§‡ weight update à¦…à¦¸à§à¦¬à¦¾à¦­à¦¾à¦¬à¦¿à¦• à¦¬à§œ à¦¹à§Ÿ à¦à¦¬à¦‚ training **unstable à¦¹à§Ÿà§‡ à¦¯à¦¾à§Ÿ**à¥¤

---

## ðŸ§  à¦¸à¦¹à¦œ à¦—à¦¾à¦£à¦¿à¦¤à¦¿à¦• à¦¸à§‡à¦Ÿà¦†à¦ª

à¦§à¦°à¦¿ à¦à¦•à¦Ÿà¦¿ à¦–à§à¦¬ à¦¸à¦¾à¦§à¦¾à¦°à¦£ RNN-à¦à¦° à¦®à¦¤à§‹ à¦¸à¦®à§à¦ªà¦°à§à¦•:

$$
h_t = w \cdot h_{t-1}
$$

à¦¶à§‡à¦· à¦¸à¦®à§Ÿà§‡à¦° output-à¦Ÿà¦¾à¦‡ loss:
$$
L = h_T
$$

---

## â–¶ï¸ Step 1: Forward Pass

à¦§à¦°à¦¿,

* à¦ªà§à¦°à¦¾à¦¥à¦®à¦¿à¦• à¦…à¦¬à¦¸à§à¦¥à¦¾: ( h_0 = 1 )
* weight: ( w = 2 )
* à¦¸à¦®à§Ÿ à¦§à¦¾à¦ª: ( T = 5 )

à¦à¦–à¦¨ à¦¹à¦¿à¦¸à¦¾à¦¬ à¦•à¦°à¦¿:

$$
\begin{aligned}
h_1 &= 2 \times 1 = 2 \\
h_2 &= 2 \times 2 = 4 \\
h_3 &= 2 \times 4 = 8 \\
h_4 &= 2 \times 8 = 16 \\
h_5 &= 2 \times 16 = 32
\end{aligned}
$$

à¦…à¦¤à¦à¦¬,
$$
L = 32
$$

---

## â—€ï¸ Step 2: Backpropagation (Gradient à¦¬à§‡à¦° à¦•à¦°à¦¾)

à¦†à¦®à¦¾à¦¦à§‡à¦° à¦¦à¦°à¦•à¦¾à¦°:

$$
\frac{\partial L}{\partial w}
$$

Gradient à¦†à¦¸à¦²à§‡ à¦¬à¦¾à¦°à¦¬à¦¾à¦° **w à¦¦à¦¿à§Ÿà§‡ à¦—à§à¦£** à¦¹à¦¤à§‡ à¦¥à¦¾à¦•à§‡à¥¤

à¦«à¦°à§à¦®à§à¦²à¦¾:

$$
\frac{\partial L}{\partial w}
= \sum_{t=1}^{T} h_{t-1} \cdot w^{(T-t)}
$$

---

## ðŸ§® Step 3: à¦®à¦¾à¦¨ à¦¬à¦¸à¦¾à¦‡

$$
\frac{\partial L}{\partial w}
= 1 \cdot 2^4 + 2 \cdot 2^3 + 4 \cdot 2^2 + 8 \cdot 2^1 + 16 \cdot 2^0
$$

$$
= 16 + 16 + 16 + 16 + 16 = 80
$$

ðŸ“Œ **Gradient = 80**
(à¦à¦¤ à¦›à§‹à¦Ÿ à¦¨à§‡à¦Ÿà¦“à§Ÿà¦¾à¦°à§à¦•à§‡à¦° à¦œà¦¨à§à¦¯à¦“ à¦à¦Ÿà¦¾ à¦…à¦¨à§‡à¦• à¦¬à§œ)

---

## ðŸ“ˆ Step 4: Explosion à¦¬à§‹à¦à¦¾ à¦¯à¦¾à¦•

à¦¸à¦®à§Ÿ à¦§à¦¾à¦ª à¦¬à¦¾à§œà¦¾à¦²à§‡ à¦•à§€ à¦¹à§Ÿ?

| Time Step (T) | Gradient |
| ------------- | -------- |
| 5             | 80       |
| 10            | â‰ˆ 1024   |
| 20            | â‰ˆ 10â¶    |
| 50            | â‰ˆ 10Â¹âµ   |

ðŸš¨ Gradient **à¦à¦•à§à¦¸à¦ªà§‹à¦¨à§‡à¦¨à¦¶à¦¿à§Ÿà¦¾à¦²à¦¿ à¦¬à§‡à§œà§‡ à¦¯à¦¾à¦šà§à¦›à§‡** â†’ Exploding Gradient

---

## â“ à¦•à§‡à¦¨ à¦à¦®à¦¨ à¦¹à§Ÿ? (à¦®à§‚à¦² à¦•à¦¾à¦°à¦£)

Backpropagation-à¦ à¦¬à¦¾à¦°à¦¬à¦¾à¦° à¦—à§à¦£ à¦¹à¦šà§à¦›à§‡:

$$
	ext{Gradient} \propto w^T
$$

* à¦¯à¦¦à¦¿ ( |w| > 1 ) â†’ **Exploding Gradient**
* à¦¯à¦¦à¦¿ ( |w| < 1 ) â†’ **Vanishing Gradient**

---

## âš™ï¸ Weight Update-à¦ à¦¸à¦®à¦¸à§à¦¯à¦¾

SGD update:
$$
w_{new} = w - \eta \cdot \text{gradient}
$$
]

à¦§à¦°à¦¿ learning rate ( \eta = 0.01 )

$$
w_{new} = 2 - 0.01 \times 80 = 1.2
$$

à¦•à¦¿à¦¨à§à¦¤à§ à¦¯à¦¦à¦¿ gradient à¦¹à§Ÿ (10^6):

$$
w_{new} = 2 - 0.01 \times 10^6 = -9998
$$

ðŸ’¥ Training à¦ªà§à¦°à§‹ à¦­à§‡à¦™à§‡ à¦¯à¦¾à¦¬à§‡

---

## ðŸ› ï¸ Exploding Gradient à¦à¦° à¦¸à¦®à¦¾à¦§à¦¾à¦¨

### 1ï¸âƒ£ Gradient Clipping (à¦¸à¦¬à¦šà§‡à§Ÿà§‡ à¦œà¦¨à¦ªà§à¦°à¦¿à§Ÿ)


Gradient à¦à¦•à¦Ÿà¦¾ à¦¸à§€à¦®à¦¾à¦° à¦®à¦§à§à¦¯à§‡ à¦•à§‡à¦Ÿà§‡ à¦¦à§‡à¦“à§Ÿà¦¾ à¦¹à§Ÿ:

$$
g_\text{clipped} = \frac{g}{\max(1, \frac{|g|}{c})}
$$

à¦‰à¦¦à¦¾à¦¹à¦°à¦£:

* Gradient = 100
* Threshold = 5
  â†’ à¦¨à¦¤à§à¦¨ gradient = 5

---

### 2ï¸âƒ£ à¦­à¦¾à¦²à§‹ Weight Initialization

* Xavier
* He initialization

---

### 3ï¸âƒ£ LSTM / GRU à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦°

Gate à¦¥à¦¾à¦•à¦¾à¦° à¦•à¦¾à¦°à¦£à§‡ gradient à¦¨à¦¿à§Ÿà¦¨à§à¦¤à§à¦°à¦£à§‡ à¦¥à¦¾à¦•à§‡

---

### 4ï¸âƒ£ Learning Rate à¦•à¦®à¦¾à¦¨à§‹

à¦¸à¦®à§à¦ªà§‚à¦°à§à¦£ à¦¸à¦®à¦¾à¦§à¦¾à¦¨ à¦¨à¦¾, à¦•à¦¿à¦¨à§à¦¤à§ à¦•à§à¦·à¦¤à¦¿ à¦•à¦®à¦¾à§Ÿ

---

## âœï¸ à¦ªà¦°à§€à¦•à§à¦·à¦¾à¦° à¦œà¦¨à§à¦¯ à¦à¦• à¦²à¦¾à¦‡à¦¨à§‡à¦° à¦‰à¦¤à§à¦¤à¦°

> **Exploding Gradient Problem à¦˜à¦Ÿà§‡ à¦¯à¦–à¦¨ backpropagation-à¦à¦° à¦¸à¦®à§Ÿ gradient-à¦à¦° à¦®à¦¾à¦¨ à¦¬à¦¾à¦°à¦¬à¦¾à¦° à¦¬à§œ weight à¦¦à¦¿à§Ÿà§‡ à¦—à§à¦£ à¦¹à§Ÿà§‡ à¦–à§à¦¬ à¦¦à§à¦°à§à¦¤ à¦¬à§ƒà¦¦à§à¦§à¦¿ à¦ªà¦¾à§Ÿ, à¦«à¦²à§‡ training à¦…à¦¸à§à¦¥à¦¿à¦¤à¦¿à¦¶à§€à¦² à¦¹à§Ÿà§‡ à¦ªà§œà§‡à¥¤**

---

=======
---

## ðŸ”¥ Exploding Gradient Problem à¦•à§€?

Backpropagation à¦•à¦°à¦¾à¦° à¦¸à¦®à§Ÿ à¦¯à¦¦à¦¿ gradient-à¦à¦° à¦®à¦¾à¦¨ **à¦–à§à¦¬ à¦¦à§à¦°à§à¦¤ (exponential à¦¹à¦¾à¦°à§‡) à¦¬à§œ à¦¹à§Ÿà§‡ à¦¯à¦¾à§Ÿ**, à¦¤à¦–à¦¨ à¦¤à¦¾à¦•à§‡ **Exploding Gradient Problem** à¦¬à¦²à§‡à¥¤
à¦à¦° à¦«à¦²à§‡ weight update à¦…à¦¸à§à¦¬à¦¾à¦­à¦¾à¦¬à¦¿à¦• à¦¬à§œ à¦¹à§Ÿ à¦à¦¬à¦‚ training **unstable à¦¹à§Ÿà§‡ à¦¯à¦¾à§Ÿ**à¥¤

---

## ðŸ§  à¦¸à¦¹à¦œ à¦—à¦¾à¦£à¦¿à¦¤à¦¿à¦• à¦¸à§‡à¦Ÿà¦†à¦ª

à¦§à¦°à¦¿ à¦à¦•à¦Ÿà¦¿ à¦–à§à¦¬ à¦¸à¦¾à¦§à¦¾à¦°à¦£ RNN-à¦à¦° à¦®à¦¤à§‹ à¦¸à¦®à§à¦ªà¦°à§à¦•:

$$
h_t = w \cdot h_{t-1}
$$

à¦¶à§‡à¦· à¦¸à¦®à§Ÿà§‡à¦° output-à¦Ÿà¦¾à¦‡ loss:
$$
L = h_T
$$

---

## â–¶ï¸ Step 1: Forward Pass

à¦§à¦°à¦¿,

* à¦ªà§à¦°à¦¾à¦¥à¦®à¦¿à¦• à¦…à¦¬à¦¸à§à¦¥à¦¾: ( h_0 = 1 )
* weight: ( w = 2 )
* à¦¸à¦®à§Ÿ à¦§à¦¾à¦ª: ( T = 5 )

à¦à¦–à¦¨ à¦¹à¦¿à¦¸à¦¾à¦¬ à¦•à¦°à¦¿:

$$
\begin{aligned}
h_1 &= 2 \times 1 = 2 \\
h_2 &= 2 \times 2 = 4 \\
h_3 &= 2 \times 4 = 8 \\
h_4 &= 2 \times 8 = 16 \\
h_5 &= 2 \times 16 = 32
\end{aligned}
$$

à¦…à¦¤à¦à¦¬,
$$
L = 32
$$

---

## â—€ï¸ Step 2: Backpropagation (Gradient à¦¬à§‡à¦° à¦•à¦°à¦¾)

à¦†à¦®à¦¾à¦¦à§‡à¦° à¦¦à¦°à¦•à¦¾à¦°:

$$
\frac{\partial L}{\partial w}
$$

Gradient à¦†à¦¸à¦²à§‡ à¦¬à¦¾à¦°à¦¬à¦¾à¦° **w à¦¦à¦¿à§Ÿà§‡ à¦—à§à¦£** à¦¹à¦¤à§‡ à¦¥à¦¾à¦•à§‡à¥¤

à¦«à¦°à§à¦®à§à¦²à¦¾:

$$
\frac{\partial L}{\partial w}
= \sum_{t=1}^{T} h_{t-1} \cdot w^{(T-t)}
$$

---

## ðŸ§® Step 3: à¦®à¦¾à¦¨ à¦¬à¦¸à¦¾à¦‡

$$
\frac{\partial L}{\partial w}
= 1 \cdot 2^4 + 2 \cdot 2^3 + 4 \cdot 2^2 + 8 \cdot 2^1 + 16 \cdot 2^0
$$

$$
= 16 + 16 + 16 + 16 + 16 = 80
$$

ðŸ“Œ **Gradient = 80**
(à¦à¦¤ à¦›à§‹à¦Ÿ à¦¨à§‡à¦Ÿà¦“à§Ÿà¦¾à¦°à§à¦•à§‡à¦° à¦œà¦¨à§à¦¯à¦“ à¦à¦Ÿà¦¾ à¦…à¦¨à§‡à¦• à¦¬à§œ)

---

## ðŸ“ˆ Step 4: Explosion à¦¬à§‹à¦à¦¾ à¦¯à¦¾à¦•

à¦¸à¦®à§Ÿ à¦§à¦¾à¦ª à¦¬à¦¾à§œà¦¾à¦²à§‡ à¦•à§€ à¦¹à§Ÿ?

| Time Step (T) | Gradient |
| ------------- | -------- |
| 5             | 80       |
| 10            | â‰ˆ 1024   |
| 20            | â‰ˆ 10â¶    |
| 50            | â‰ˆ 10Â¹âµ   |

ðŸš¨ Gradient **à¦à¦•à§à¦¸à¦ªà§‹à¦¨à§‡à¦¨à¦¶à¦¿à§Ÿà¦¾à¦²à¦¿ à¦¬à§‡à§œà§‡ à¦¯à¦¾à¦šà§à¦›à§‡** â†’ Exploding Gradient

---

## â“ à¦•à§‡à¦¨ à¦à¦®à¦¨ à¦¹à§Ÿ? (à¦®à§‚à¦² à¦•à¦¾à¦°à¦£)

Backpropagation-à¦ à¦¬à¦¾à¦°à¦¬à¦¾à¦° à¦—à§à¦£ à¦¹à¦šà§à¦›à§‡:

$$
	ext{Gradient} \propto w^T
$$

* à¦¯à¦¦à¦¿ ( |w| > 1 ) â†’ **Exploding Gradient**
* à¦¯à¦¦à¦¿ ( |w| < 1 ) â†’ **Vanishing Gradient**

---

## âš™ï¸ Weight Update-à¦ à¦¸à¦®à¦¸à§à¦¯à¦¾

SGD update:
$$
w_{new} = w - \eta \cdot \text{gradient}
$$
]

à¦§à¦°à¦¿ learning rate ( \eta = 0.01 )

$$
w_{new} = 2 - 0.01 \times 80 = 1.2
$$

à¦•à¦¿à¦¨à§à¦¤à§ à¦¯à¦¦à¦¿ gradient à¦¹à§Ÿ (10^6):

$$
w_{new} = 2 - 0.01 \times 10^6 = -9998
$$

ðŸ’¥ Training à¦ªà§à¦°à§‹ à¦­à§‡à¦™à§‡ à¦¯à¦¾à¦¬à§‡

---

## ðŸ› ï¸ Exploding Gradient à¦à¦° à¦¸à¦®à¦¾à¦§à¦¾à¦¨

### 1ï¸âƒ£ Gradient Clipping (à¦¸à¦¬à¦šà§‡à§Ÿà§‡ à¦œà¦¨à¦ªà§à¦°à¦¿à§Ÿ)


Gradient à¦à¦•à¦Ÿà¦¾ à¦¸à§€à¦®à¦¾à¦° à¦®à¦§à§à¦¯à§‡ à¦•à§‡à¦Ÿà§‡ à¦¦à§‡à¦“à§Ÿà¦¾ à¦¹à§Ÿ:

$$
g_\text{clipped} = \frac{g}{\max(1, \frac{|g|}{c})}
$$

à¦‰à¦¦à¦¾à¦¹à¦°à¦£:

* Gradient = 100
* Threshold = 5
  â†’ à¦¨à¦¤à§à¦¨ gradient = 5

---

### 2ï¸âƒ£ à¦­à¦¾à¦²à§‹ Weight Initialization

* Xavier
* He initialization

---

### 3ï¸âƒ£ LSTM / GRU à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦°

Gate à¦¥à¦¾à¦•à¦¾à¦° à¦•à¦¾à¦°à¦£à§‡ gradient à¦¨à¦¿à§Ÿà¦¨à§à¦¤à§à¦°à¦£à§‡ à¦¥à¦¾à¦•à§‡

---

### 4ï¸âƒ£ Learning Rate à¦•à¦®à¦¾à¦¨à§‹

à¦¸à¦®à§à¦ªà§‚à¦°à§à¦£ à¦¸à¦®à¦¾à¦§à¦¾à¦¨ à¦¨à¦¾, à¦•à¦¿à¦¨à§à¦¤à§ à¦•à§à¦·à¦¤à¦¿ à¦•à¦®à¦¾à§Ÿ

---

## âœï¸ à¦ªà¦°à§€à¦•à§à¦·à¦¾à¦° à¦œà¦¨à§à¦¯ à¦à¦• à¦²à¦¾à¦‡à¦¨à§‡à¦° à¦‰à¦¤à§à¦¤à¦°

> **Exploding Gradient Problem à¦˜à¦Ÿà§‡ à¦¯à¦–à¦¨ backpropagation-à¦à¦° à¦¸à¦®à§Ÿ gradient-à¦à¦° à¦®à¦¾à¦¨ à¦¬à¦¾à¦°à¦¬à¦¾à¦° à¦¬à§œ weight à¦¦à¦¿à§Ÿà§‡ à¦—à§à¦£ à¦¹à§Ÿà§‡ à¦–à§à¦¬ à¦¦à§à¦°à§à¦¤ à¦¬à§ƒà¦¦à§à¦§à¦¿ à¦ªà¦¾à§Ÿ, à¦«à¦²à§‡ training à¦…à¦¸à§à¦¥à¦¿à¦¤à¦¿à¦¶à§€à¦² à¦¹à§Ÿà§‡ à¦ªà§œà§‡à¥¤**

---


>>>>>>> f45ebbad1686e699afe9932c4175eeff501d254b
