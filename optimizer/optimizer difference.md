---

# üî• Gradient Descent Optimizers ‚Äî Complete Comparison

---

## 1Ô∏è‚É£ Batch Gradient Descent (BGD)

**Idea:**
‡¶™‡ßÅ‡¶∞‡ßã dataset ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶è‡¶ï‡¶¨‡¶æ‡¶∞ update

**Update:**
‡¶∏‡¶¨ data ‚Üí ‡¶è‡¶ï gradient ‚Üí ‡¶è‡¶ï update

**Pros**

* Stable convergence
* Exact gradient

**Cons**

* Slow
* Large dataset-‡¶è impractical
* Memory heavy

---

## 2Ô∏è‚É£ Stochastic Gradient Descent (SGD)

**Idea:**
‡¶è‡¶ï‡¶ü‡¶æ data point ‚Üí ‡¶è‡¶ï update

**Pros**

* Fast updates
* Memory efficient
* Escapes local minima

**Cons**

* Noisy
* Zig-zag path
* Slow convergence

---

## 3Ô∏è‚É£ Mini-Batch SGD

**Idea:**
‡¶õ‡ßã‡¶ü batch (32, 64, 128) ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá update

**Pros**

* Stable + fast
* GPU friendly
* Most practical

**Cons**

* Batch size tune ‡¶ï‡¶∞‡¶§‡ßá ‡¶π‡ßü

üëâ **Industry standard base method**

---

## 4Ô∏è‚É£ SGD with Momentum

**Idea:**
‡¶Ü‡¶ó‡ßá‡¶∞ gradient ‡¶Æ‡¶®‡ßá ‡¶∞‡¶æ‡¶ñ‡ßá (velocity)

**Key Point:**
Direction + speed

**Pros**

* Zig-zag ‡¶ï‡¶Æ‡ßá
* Faster convergence
* Stable

**Cons**

* Extra hyperparameter (Œ≤)

---

## 5Ô∏è‚É£ Adagrad

**Idea:**
Per-parameter adaptive learning rate
(All past squared gradients ‡¶ú‡¶Æ‡¶æ)

**Pros**

* Sparse data-‡¶§‡ßá excellent
* Rare feature fast learns

**Cons**

* Learning rate ‡¶ñ‡ßÅ‡¶¨ ‡¶¶‡ßç‡¶∞‡ßÅ‡¶§ 0 ‡¶π‡ßü‡ßá ‡¶Ø‡¶æ‡ßü
* Long training-‡¶è ‡¶•‡ßá‡¶Æ‡ßá ‡¶Ø‡¶æ‡ßü

---

## 6Ô∏è‚É£ RMSProp

**Idea:**
Adagrad ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ **recent gradients** ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá

**Key Fix:**
Old gradients ‡¶≠‡ßÅ‡¶≤‡ßá ‡¶Ø‡¶æ‡ßü

**Pros**

* Stable learning
* Non-stationary problems-‡¶è ‡¶≠‡¶æ‡¶≤‡ßã
* Deep learning-‡¶è effective

**Cons**

* Momentum ‡¶®‡ßá‡¶á
* Adam-‡¶è‡¶∞ ‡¶§‡ßÅ‡¶≤‡¶®‡¶æ‡ßü slow

---

## 7Ô∏è‚É£ Adam (Adaptive Moment Estimation)

**Idea:**
Momentum + RMSProp

* First moment ‚Üí mean (m)
* Second moment ‚Üí variance (v)

**Pros**

* Very fast convergence
* Adaptive learning rate
* Sparse + deep networks-‡¶è excellent
* Default choice

**Cons**

* Sometimes poor generalization
* Over-adaptive

---

## 8Ô∏è‚É£ AdamW

**Idea:**
Adam + **True Weight Decay**

**Key Fix:**
Regularization gradient ‡¶•‡ßá‡¶ï‡ßá ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ

**Pros**

* Better generalization
* Correct weight decay
* Transformer models-‡¶è standard

**Cons**

* Slightly more complex

---

# üìä One-Shot Comparison Table (Very Important)

| Optimizer      | Adaptive LR | Momentum | Sparse Data | Stability | Used Today |
| -------------- | ----------- | -------- | ----------- | --------- | ---------- |
| Batch GD       | ‚ùå           | ‚ùå        | ‚ùå           | ‚úÖ         | ‚ùå          |
| SGD            | ‚ùå           | ‚ùå        | ‚ùå           | ‚ùå         | ‚ö†Ô∏è         |
| Mini-Batch SGD | ‚ùå           | ‚ùå        | ‚ùå           | ‚úÖ         | ‚úÖ          |
| SGD + Momentum | ‚ùå           | ‚úÖ        | ‚ùå           | ‚úÖ         | ‚úÖ          |
| Adagrad        | ‚úÖ           | ‚ùå        | ‚úÖ           | ‚ùå         | ‚ùå          |
| RMSProp        | ‚úÖ           | ‚ùå        | ‚úÖ           | ‚úÖ         | ‚ö†Ô∏è         |
| Adam           | ‚úÖ           | ‚úÖ        | ‚úÖ           | ‚≠ê         | ‚úÖ          |
| AdamW          | ‚úÖ           | ‚úÖ        | ‚úÖ           | ‚≠ê‚≠ê        | ‚≠ê‚≠ê‚≠ê        |

---

## üß† Memory Trick (Golden)

* **SGD** ‚Üí noisy walker
* **Momentum** ‚Üí rolling ball
* **Adagrad** ‚Üí runs out of fuel
* **RMSProp** ‚Üí forgets old mistakes
* **Adam** ‚Üí smart + fast
* **AdamW** ‚Üí smart + disciplined

---

## ‚úçÔ∏è Exam-Ready Final Lines

* **Best default optimizer:** Adam
* **Best for Transformers:** AdamW
* **Best generalization:** SGD + Momentum
* **Best for sparse data:** Adagrad / Adam

---

