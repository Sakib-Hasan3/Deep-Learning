## **SGD with Momentum ‚Äî Graph ‡¶∏‡¶π ‡¶¨‡¶ø‡¶∏‡ßç‡¶§‡¶æ‡¶∞‡¶ø‡¶§ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ**

![Image](https://www.researchgate.net/publication/372654300/figure/fig1/AS%3A11431281199684241%401697665042938/Optimization-using-SGD-vanilla-L-BFGS-L-BFGS-with-momentum-b-09-and-exact.ppm)

![Image](https://miro.medium.com/v2/resize%3Afit%3A1400/0%2AxJ77dJmXVtzQTw1m.jpg)

![Image](https://substackcdn.com/image/fetch/f_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16cb6bfd-4522-4cfd-aa4c-5d412d6a9e50_1482x796.gif)

![Image](https://blog.paperspace.com/content/images/size/w350/2018/06/optimizers7.gif)

---

## 1Ô∏è‚É£ Momentum ‡¶ï‡ßÄ? (‡¶∏‡¶Ç‡¶ï‡ßç‡¶∑‡ßá‡¶™‡ßá ‡¶∞‡¶ø‡¶ï‡ßç‡¶Ø‡¶æ‡¶™)

**SGD with Momentum** ‡¶π‡¶≤‡ßã ‡¶è‡¶Æ‡¶® ‡¶è‡¶ï‡¶ü‡¶ø optimizer ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá
üëâ ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶® gradient-‡¶è‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá **‡¶Ü‡¶ó‡ßá‡¶∞ gradient-‡¶è‡¶∞ ‡¶™‡ßç‡¶∞‡¶≠‡¶æ‡¶¨ ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü**‡•§

‡¶è‡¶∞ ‡¶Æ‡ßÇ‡¶≤ ‡¶â‡¶¶‡ßç‡¶¶‡ßá‡¶∂‡ßç‡¶Ø:

* SGD-‡¶è‡¶∞ **zig-zag movement ‡¶ï‡¶Æ‡¶æ‡¶®‡ßã**
* **‡¶¶‡ßç‡¶∞‡ßÅ‡¶§ ‡¶ì ‡¶∏‡ßç‡¶•‡¶ø‡¶§‡¶ø‡¶∂‡ßÄ‡¶≤ convergence** ‡¶™‡¶æ‡¶ì‡ßü‡¶æ

---

## 2Ô∏è‚É£ Graph ‡¶¶‡¶ø‡ßü‡ßá ‡¶¨‡ßã‡¶ù‡¶æ (‡¶∏‡¶¨‡¶ö‡ßá‡ßü‡ßá ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£)

### üîπ (a) Normal SGD (Graph intuition)

* Path ‡¶ñ‡ßÅ‡¶¨ **zig-zag**
* Narrow valley-‡¶§‡ßá ‡¶¶‡ßÅ‡¶≤‡¶§‡ßá ‡¶¶‡ßÅ‡¶≤‡¶§‡ßá ‡¶®‡¶æ‡¶Æ‡ßá
* Minimum ‡¶™‡ßá‡¶§‡ßá **‡¶¨‡ßá‡¶∂‡¶ø ‡¶∏‡¶Æ‡ßü ‡¶≤‡¶æ‡¶ó‡ßá**

üëâ ‡¶ï‡¶æ‡¶∞‡¶£: ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶æ step ‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶® gradient ‡¶¶‡ßá‡¶ñ‡ßá ‡¶®‡ßá‡ßü

---

### üîπ (b) SGD with Momentum (Graph intuition)

* Path **smooth ‡¶ì straight**
* ‡¶Ü‡¶ó‡ßá‡¶∞ direction ‡¶ß‡¶∞‡ßá ‡¶∞‡¶æ‡¶ñ‡ßá
* Valley ‡¶¨‡¶∞‡¶æ‡¶¨‡¶∞ **‡¶¶‡ßç‡¶∞‡ßÅ‡¶§ ‡¶è‡¶ó‡ßã‡ßü**

üëâ ‡¶ï‡¶æ‡¶∞‡¶£: ‡¶Ü‡¶ó‡ßá‡¶∞ velocity ‡¶Ø‡ßã‡¶ó ‡¶π‡¶ì‡ßü‡¶æ‡ßü small noise ignore ‡¶π‡ßü

---

## 3Ô∏è‚É£ Mathematical Detail (With Meaning)

‡¶ß‡¶∞‡¶ø:

* ( \theta ) = parameters
* ( \eta ) = learning rate
* ( \beta ) = momentum coefficient
* ( v ) = velocity


### Step‚Äì1: Velocity update
$$
v_t = \beta v_{t-1} + \eta \nabla J(\theta)
$$

**‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶ï‡ßÄ ‡¶π‡¶ö‡ßç‡¶õ‡ßá?**

* ( \beta v_{t-1} ) ‚Üí ‡¶Ü‡¶ó‡ßá‡¶∞ ‡¶¶‡¶ø‡¶ï‡ßá‡¶∞ ‡¶∏‡ßç‡¶Æ‡ßÉ‡¶§‡¶ø
* ( \eta \nabla J(\theta) ) ‚Üí ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶® gradient

---


### Step‚Äì2: Parameter update
$$
	heta \leftarrow \theta - v_t
$$

üëâ Parameter ‡¶è‡¶ñ‡¶® ‡¶∂‡ßÅ‡¶ß‡ßÅ gradient ‡¶®‡ßü,
üëâ **momentum ‡¶∏‡¶π direction**-‡¶è ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶π‡ßü

---

## 4Ô∏è‚É£ Velocity Concept (‡¶∏‡¶π‡¶ú ‡¶≠‡¶æ‡¶∑‡¶æ‡ßü)

* Velocity = accumulated gradient
* Gradient ‡¶¨‡¶æ‡¶∞‡¶¨‡¶æ‡¶∞ ‡¶è‡¶ï‡¶á ‡¶¶‡¶ø‡¶ï‡ßá ‡¶π‡¶≤‡ßá ‚Üí velocity ‡¶¨‡¶æ‡ßú‡ßá
* Gradient direction ‡¶¨‡¶¶‡¶≤‡¶æ‡¶≤‡ßá ‚Üí velocity ‡¶ß‡ßÄ‡¶∞‡ßá ‡¶¨‡¶¶‡¶≤‡¶æ‡ßü

üëâ ‡¶§‡¶æ‡¶á zig-zag ‡¶ï‡¶Æ‡ßá ‡¶Ø‡¶æ‡ßü

---

## 5Ô∏è‚É£ Weight ‡¶ì Bias ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ ‡¶ï‡¶∞‡ßá


Weight ($w$):
$$
v_w = \beta v_w + \eta \frac{\partial J}{\partial w}
$$
$$
w \leftarrow w - v_w
$$


Bias ($b$):
$$
v_b = \beta v_b + \eta \frac{\partial J}{\partial b}
$$
$$
b \leftarrow b - v_b
$$

---

## 6Ô∏è‚É£ Momentum Coefficient (Œ≤) ‡¶è‡¶∞ ‡¶≠‡ßÇ‡¶Æ‡¶ø‡¶ï‡¶æ

| Œ≤ value | Meaning                |
| ------- | ---------------------- |
| 0       | No momentum (pure SGD) |
| 0.5     | Weak momentum          |
| **0.9** | ‚≠ê Standard choice      |
| >0.95   | Risk of overshoot      |

üëâ ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£‡¶§ **Œ≤ = 0.9** ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü

---

## 7Ô∏è‚É£ Advantages (Graph-based explanation)

‚úî Zig-zag ‡¶ï‡¶Æ (graph‡ßá smooth path)
‚úî Faster convergence
‚úî Narrow valley ‡¶§‡ßá ‡¶ñ‡ßÅ‡¶¨ ‡¶≠‡¶æ‡¶≤‡ßã ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá
‚úî SGD-‡¶è‡¶∞ noise reduce ‡¶ï‡¶∞‡ßá

---

## 8Ô∏è‚É£ Disadvantages

‚ùå Extra hyperparameter (Œ≤)
‚ùå Improper Œ≤ ‡¶π‡¶≤‡ßá overshoot ‡¶π‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá
‚ùå Adam-‡¶è‡¶∞ ‡¶Æ‡¶§‡ßã adaptive ‡¶®‡ßü

---

## 9Ô∏è‚É£ SGD vs Momentum (Conceptual Difference)

* **SGD:**
  ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶® ‡¶¢‡¶æ‡¶≤ ‡¶¶‡ßá‡¶ñ‡ßá ‡¶è‡¶ï ‡¶ß‡¶æ‡¶™

* **SGD + Momentum:**
  ‡¶Ü‡¶ó‡ßá‡¶∞ ‡¶¢‡¶æ‡¶≤ + ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶® ‡¶¢‡¶æ‡¶≤ ‡¶¶‡ßá‡¶ñ‡ßá ‡¶¨‡ßú ‡¶ì smart ‡¶ß‡¶æ‡¶™

---

## 10Ô∏è‚É£ Real-life Analogy (Graph ‡¶Æ‡¶®‡ßá ‡¶∞‡¶æ‡¶ñ‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø)

* SGD = ‡¶π‡ßá‡¶Å‡¶ü‡ßá ‡¶™‡¶æ‡¶π‡¶æ‡ßú ‡¶®‡¶æ‡¶Æ‡¶æ (‡¶è‡¶¶‡¶ø‡¶ï-‡¶ì‡¶¶‡¶ø‡¶ï)
* Momentum = ‡¶ó‡ßú‡¶æ‡¶§‡ßá ‡¶•‡¶æ‡¶ï‡¶æ ‡¶¨‡¶≤ (‡¶è‡¶ï‡¶á ‡¶¶‡¶ø‡¶ï‡ßá ‡¶ó‡¶§‡¶ø ‡¶ú‡¶Æ‡¶æ)

---

## ‚úçÔ∏è Exam-Ready One Line

**SGD with Momentum accelerates gradient descent by accumulating past gradients, reducing oscillations and achieving faster, smoother convergence.**

---


