## **Which Activation Function to Apply â€” When & Why (Quick Guide)**

![Image](https://www.researchgate.net/publication/317679065/figure/fig10/AS%3A654765507768342%401533119670534/Activation-functions-in-comparison-Red-curves-stand-for-respectively-sigmoid.png)

![Image](https://miro.medium.com/v2/resize%3Afit%3A2000/1%2A0GBatebNQ5WohnGF8frGqg.png)

![Image](https://media.licdn.com/dms/image/v2/D4D12AQH2F3GJ9wen_Q/article-cover_image-shrink_720_1280/article-cover_image-shrink_720_1280/0/1688885174323?e=2147483647\&t=dY_S6xeNsRCIvpIrjrPFzq8qgHPgmP4e_HLaA15ufPM\&v=beta)

---

### ðŸ”¹ **1ï¸âƒ£ Sigmoid â€” à¦•à¦–à¦¨ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à¦¬à§‡?**

**Best for:**

* **Binary classification**
* **Output layer** (Yes/No, 0/1)

**Why:**

* Output à¦¦à§‡à§Ÿ **0â€“1** â†’ probability à¦¹à¦¿à¦¸à§‡à¦¬à§‡ à¦¬à§à¦¯à¦¾à¦–à§à¦¯à¦¾ à¦•à¦°à¦¾ à¦¯à¦¾à§Ÿ

**Avoid:**

* Hidden layers (vanishing gradient)

âœ… **Use when:**

> *Only one output neuron + binary decision*

---

### ðŸ”¹ **2ï¸âƒ£ Tanh â€” à¦•à¦–à¦¨ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à¦¬à§‡?**

**Best for:**

* **Hidden layers** (shallow / medium networks)

**Why:**

* Output **âˆ’1 to +1**
* Zero-centered â†’ faster learning than sigmoid

**Avoid:**

* Very deep networks (still vanishing gradient)

âœ… **Use when:**

> *Hidden layer + data normalized around 0*

---

### ðŸ”¹ **3ï¸âƒ£ ReLU â€” à¦•à¦–à¦¨ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à¦¬à§‡?**

**Best for:**

* **Hidden layers (default choice)**
* **Deep neural networks / CNN**

**Why:**

* Fast
* Vanishing gradient à¦•à¦®
* Simple

**Issue:**

* Dead neuron problem

âœ… **Use when:**

> *Deep network + speed & simplicity needed*

---

### ðŸ”¹ **4ï¸âƒ£ Leaky ReLU â€” à¦•à¦–à¦¨ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à¦¬à§‡?**

**Best for:**

* ReLU à¦•à¦¾à¦œ à¦•à¦°à¦›à§‡ à¦¨à¦¾
* Dead neuron à¦¸à¦®à¦¸à§à¦¯à¦¾ à¦¹à¦šà§à¦›à§‡

**Why:**

* Negative side-à¦à¦“ gradient à¦¥à¦¾à¦•à§‡

âœ… **Use when:**

> *ReLU dead neuron à¦¦à¦¿à¦šà§à¦›à§‡*

---

### ðŸ”¹ **5ï¸âƒ£ PReLU â€” à¦•à¦–à¦¨ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à¦¬à§‡?**

**Best for:**

* Very deep CNN
* Large dataset

**Why:**

* Negative slope à¦¨à¦¿à¦œà§‡ à¦¨à¦¿à¦œà§‡ à¦¶à§‡à¦–à§‡

**Caution:**

* Extra parameter â†’ overfitting risk

âœ… **Use when:**

> *Large data + model needs flexibility*

---

### ðŸ”¹ **6ï¸âƒ£ ELU â€” à¦•à¦–à¦¨ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à¦¬à§‡?**

**Best for:**

* Stable & smooth training
* Deep networks à¦¯à§‡à¦–à¦¾à¦¨à§‡ ReLU unstable

**Why:**

* Negative output â†’ bias shift à¦•à¦®
* Dead neuron à¦ªà§à¦°à¦¾à§Ÿ à¦¨à§‡à¦‡

**Cost:**

* ReLU à¦¥à§‡à¦•à§‡ à¦§à§€à¦°

âœ… **Use when:**

> *Stability > speed*

---

### ðŸ”¹ **7ï¸âƒ£ Softmax â€” à¦•à¦–à¦¨ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à¦¬à§‡?**

**Best for:**

* **Multiclass classification**
* **Output layer**

**Why:**

* à¦¸à¦¬ class-à¦à¦° probability à¦¦à§‡à§Ÿ
* Sum = 1

âœ… **Use when:**

> *One sample = one class (Cat/Dog/Cow)*

---

## ðŸ“Š **One-Glance Decision Table**

| Problem Type              | Activation (Hidden)           | Activation (Output) |
| ------------------------- | ----------------------------- | ------------------- |
| Binary classification     | ReLU / Tanh                   | **Sigmoid**         |
| Multiclass (single label) | ReLU / ELU                    | **Softmax**         |
| Deep CNN                  | **ReLU / Leaky ReLU / PReLU** | Softmax             |
| Shallow ANN               | Tanh                          | Sigmoid / Softmax   |
| Training unstable         | ELU                           | Depends             |
| Dead neuron issue         | Leaky ReLU / PReLU            | â€”                   |

---

## ðŸ§  **Golden Rule (Exam + Practice)**

> **Hidden layers â†’ ReLU-family**
> **Binary output â†’ Sigmoid**
> **Multiclass output â†’ Softmax**

---

## âœï¸ **Exam-Ready One Line**

> **Activation functions are chosen based on network depth and task: ReLU-family for hidden layers, sigmoid for binary output, and softmax for multiclass classification.**

