## **Softmax for Multiclass Classification ‚Äî ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡ßü ‡¶∏‡¶π‡¶ú ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ**

![Image](https://cdn.sanity.io/images/vr8gru94/production/a5b0c039f3233ca1f0e13ff3bbd58a3742545448-2048x1152.png)

![Image](https://rasbt.github.io/mlxtend/user_guide/classifier/SoftmaxRegression_files/softmax_schematic_1.png)

![Image](https://mriquestions.com/uploads/3/4/5/7/34572113/softmax-example_orig.png)

![Image](https://media.licdn.com/dms/image/v2/D4E12AQEorjkY3lYVXA/article-cover_image-shrink_600_2000/article-cover_image-shrink_600_2000/0/1722585755086?e=2147483647\&t=rcHu5Is8fake7X21p82r421XLJx3Ys0KMDl74Q2FN6g\&v=beta)

---

## üîπ ‡ßß) Softmax ‡¶ï‡ßÄ?

**Softmax** ‡¶π‡¶≤‡ßã ‡¶è‡¶ï‡¶ü‡¶ø **activation function** ‡¶Ø‡¶æ ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£‡¶§ **multiclass classification**‚Äì‡¶è‡¶∞ ‡¶ï‡ßç‡¶∑‡ßá‡¶§‡ßç‡¶∞‡ßá **output layer**-‡¶è ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡•§

üëâ ‡¶è‡¶ü‡¶ø ‡¶è‡¶ï‡¶∏‡¶æ‡¶•‡ßá ‡¶è‡¶ï‡¶æ‡¶ß‡¶ø‡¶ï class-‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø **probability** ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßá‡•§

---

## üîπ ‡ß®) ‡¶ï‡ßá‡¶® Multiclass Classification-‡¶è Softmax ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞?

‡¶ß‡¶∞‡¶ø ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ 3‡¶ü‡¶æ class ‡¶Ü‡¶õ‡ßá:

* Cat
* Dog
* Cow

Model raw output (logits) ‡¶¶‡¶ø‡¶≤:
[
z = [2.0,; 1.0,; 0.1]
]

‡¶è‡¶á ‡¶Æ‡¶æ‡¶®‡¶ó‡ßÅ‡¶≤‡ßã:

* probability ‡¶®‡¶æ
* ‡¶Ø‡ßã‡¶ó‡¶´‡¶≤ = 1 ‡¶®‡¶æ

üëâ Softmax ‡¶è‡¶ó‡ßÅ‡¶≤‡ßã‡¶ï‡ßá **probability distribution**-‡¶è ‡¶∞‡ßÇ‡¶™‡¶æ‡¶®‡ßç‡¶§‡¶∞ ‡¶ï‡¶∞‡ßá‡•§

---

## üîπ ‡ß©) Softmax-‡¶è‡¶∞ Mathematical Formula

‡¶ß‡¶∞‡¶ø ‡¶Æ‡ßã‡¶ü class ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ = (K)

[
\text{Softmax}(z_i)=\frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
]

‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá,

* (z_i) = i-th class-‡¶è‡¶∞ raw score (logit)
* Output ‡¶∏‡¶¨‡¶∏‡¶Æ‡ßü:
  [
  0 < p_i < 1
  \quad \text{‡¶è‡¶¨‡¶Ç} \quad
  \sum p_i = 1
  ]

---

## üîπ ‡ß™) Numerical Example (‡¶∏‡¶π‡¶ú)

‡¶ß‡¶∞‡¶ø logits:
[
z = [2,; 1,; 0]
]

Exponent:
[
e^2=7.39,\quad e^1=2.71,\quad e^0=1
]

Sum:
[
7.39+2.71+1=11.10
]

Softmax output:
[
p_1=\frac{7.39}{11.10}=0.67
]
[
p_2=\frac{2.71}{11.10}=0.24
]
[
p_3=\frac{1}{11.10}=0.09
]

üëâ Prediction = **Class 1 (67%)**

---

## üîπ ‡ß´) Intuition (‡¶¨‡ßã‡¶ù‡¶æ‡¶∞ ‡¶ï‡ßå‡¶∂‡¶≤)

* ‡¶¨‡ßú logit ‚áí ‡¶¨‡ßú probability
* ‡¶õ‡ßã‡¶ü logit ‚áí ‡¶õ‡ßã‡¶ü probability
* ‡¶∏‡¶¨ probability ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶≤‡ßá = **1**

üëâ ‡¶Æ‡¶æ‡¶®‡ßá:

> ‚Äú‡¶è‡¶á sample ‡¶ï‡ßã‡¶® class-‡¶è‡¶∞ ‡¶π‡¶ì‡ßü‡¶æ‡¶∞ ‡¶∏‡¶Æ‡ßç‡¶≠‡¶æ‡¶¨‡¶®‡¶æ ‡¶∏‡¶¨‡¶ö‡ßá‡ßü‡ßá ‡¶¨‡ßá‡¶∂‡¶ø?‚Äù

---

## üîπ ‡ß¨) Softmax + Cross-Entropy Loss (Best Pair)

Multiclass classification-‡¶è ‡¶∏‡¶¨‡¶ö‡ßá‡ßü‡ßá ‡¶¨‡ßá‡¶∂‡¶ø ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡ßÉ‡¶§ combination:

* **Activation:** Softmax
* **Loss:** Categorical Cross-Entropy

Loss formula (‡¶è‡¶ï ‡¶≤‡¶æ‡¶á‡¶®‡ßá):
[
L = -\sum y_i \log(p_i)
]

üëâ ‡¶è‡¶ü‡¶æ training-‡¶ï‡ßá ‡¶∏‡¶π‡¶ú ‡¶ì stable ‡¶ï‡¶∞‡ßá‡•§

---

## üîπ ‡ß≠) Softmax vs Sigmoid (Multiclass Context)

| ‡¶¨‡¶ø‡¶∑‡ßü           | Sigmoid              | Softmax                   |
| -------------- | -------------------- | ------------------------- |
| Problem type   | Binary / Multi-label | Multiclass (single label) |
| Output         | Independent          | Interdependent            |
| Sum of outputs | ‚â† 1                  | = 1                       |
| Best use       | Yes/No               | One-of-many               |

---

## üîπ ‡ßÆ) ‡¶ï‡ßã‡¶•‡¶æ‡ßü Softmax ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü?

‚úÖ Image classification (digit, object, animal)
‚úÖ Text classification (topic, sentiment classes)
‚úÖ Speech recognition
‚úÖ Any **one-label multiclass** problem

---

## üß† ‡¶Æ‡¶®‡ßá ‡¶∞‡¶æ‡¶ñ‡¶æ‡¶∞ ‡¶ü‡ßç‡¶∞‡¶ø‡¶ï

> **Sigmoid = one neuron, Softmax = team decision**

---

## ‚úçÔ∏è Exam-Ready ‡¶è‡¶ï ‡¶≤‡¶æ‡¶á‡¶®

> **Softmax is an activation function that converts raw scores into a normalized probability distribution, making it ideal for multiclass classification problems.**

---

