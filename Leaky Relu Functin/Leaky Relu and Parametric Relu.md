## **Leaky ReLU & Parametric ReLU (PReLU)**

*(Dead ReLU problem-‡¶è‡¶∞ smart solutions ‚Äî graph + intuition ‡¶∏‡¶π)*

![Image](https://www.researchgate.net/publication/376410479/figure/fig15/AS%3A11431281211482860%401702404866700/A-plot-of-the-leaky-ReLU-activation-function-with-leak-factor-1-10-and-the-ReLU.png)

![Image](https://dl.acm.org/cms/attachment/1c0498a6-7f83-4f52-b3f6-732bbe32126a/image1.jpeg)

![Image](https://www.researchgate.net/publication/358306930/figure/fig2/AS%3A1119417702318091%401643901386378/ReLU-activation-function-vs-LeakyReLU-activation-function.png)

![Image](https://www.researchgate.net/publication/334389306/figure/fig8/AS%3A779352161677313%401562823443351/llustration-of-output-of-ELU-vs-ReLU-vs-Leaky-ReLU-function-with-varying-input-values.ppm)

---

## üîπ 1Ô∏è‚É£ Why Leaky ReLU & PReLU?

**Problem with ReLU:**
[
\text{ReLU}(x)=\max(0,x)
]

* (x<0 \Rightarrow) output = 0
* gradient = 0
  üëâ neuron **dead** ‡¶π‡ßü‡ßá ‡¶Ø‡ßá‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá (‡¶Ü‡¶∞ ‡¶∂‡ßá‡¶ñ‡ßá ‡¶®‡¶æ)

**Solution idea:**
üëâ Negative side-‡¶è ‡¶è‡¶ï‡¶ü‡ßÅ ‡¶π‡¶≤‡ßá‡¶ì gradient ‡¶•‡¶æ‡¶ï‡¶§‡ßá ‡¶¶‡¶æ‡¶ì

---

## üîπ 2Ô∏è‚É£ Leaky ReLU (LReLU)

### ‚úÖ Definition

Leaky ReLU negative input-‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶õ‡ßã‡¶ü ‡¶è‡¶ï‡¶ü‡¶æ slope ‡¶¶‡ßá‡ßü‡•§

[
\text{LeakyReLU}(x)=
\begin{cases}
x, & x>0\
\alpha x, & x<0
\end{cases}
]

‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£‡¶§,
[
\alpha = 0.01
]

---

### üîπ Graph-based Intuition

* Positive side ‚Üí ReLU-‡¶è‡¶∞ ‡¶Æ‡¶§‡ßã
* Negative side ‚Üí ‡¶π‡¶æ‡¶≤‡¶ï‡¶æ ‡¶¢‡¶æ‡¶≤ (slope = Œ±)
* Gradient **‡¶ï‡¶ñ‡¶®‡ßã‡¶á 0 ‡¶π‡ßü ‡¶®‡¶æ**

üëâ Dead neuron ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ ‡¶Ö‡¶®‡ßá‡¶ï ‡¶ï‡¶Æ‡ßá ‡¶Ø‡¶æ‡ßü

---

### üîπ Derivative

[
\frac{d}{dx}=
\begin{cases}
1,& x>0\
\alpha,& x<0
\end{cases}
]

---

### ‚úÖ Advantages (Leaky ReLU)

‚úî Dead ReLU ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ ‡¶ï‡¶Æ‡¶æ‡ßü
‚úî Simple & fast
‚úî ReLU-‡¶è‡¶∞ ‡¶™‡ßç‡¶∞‡¶æ‡ßü ‡¶∏‡¶¨ ‡¶∏‡ßÅ‡¶¨‡¶ø‡¶ß‡¶æ ‡¶¨‡¶ú‡¶æ‡ßü ‡¶∞‡¶æ‡¶ñ‡ßá

---

### ‚ùå Disadvantages (Leaky ReLU)

‚ùå (\alpha) manually set ‡¶ï‡¶∞‡¶§‡ßá ‡¶π‡ßü
‚ùå Optimal (\alpha) ‡¶∏‡¶¨ dataset-‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø same ‡¶®‡¶æ

---

## üîπ 3Ô∏è‚É£ Parametric ReLU (PReLU)

### ‚úÖ Definition

PReLU-‡¶§‡ßá (\alpha) **fixed ‡¶®‡¶æ**, network ‡¶®‡¶ø‡¶ú‡ßá‡¶á ‡¶∂‡ßá‡¶ñ‡ßá‡•§

[
\text{PReLU}(x)=
\begin{cases}
x,& x>0\
\alpha x,& x<0
\end{cases}
\quad\text{where }\alpha\text{ is trainable}
]

---

### üîπ Intuition

* Model ‡¶®‡¶ø‡¶ú‡ßá‡¶á ‡¶†‡¶ø‡¶ï ‡¶ï‡¶∞‡ßá ‡¶®‡ßá‡ßü
  üëâ negative side ‡¶ï‡¶§‡¶ü‡¶æ allow ‡¶ï‡¶∞‡¶¨‡ßá
* ‡¶¨‡ßá‡¶∂‡¶ø flexible than Leaky ReLU

---

### üîπ Derivative

[
\frac{d}{dx}=
\begin{cases}
1,& x>0\
\alpha,& x<0
\end{cases}
]
‡¶è‡¶¨‡¶Ç (\alpha) ‡¶®‡¶ø‡¶ú‡ßá‡¶ì gradient ‡¶¶‡¶ø‡ßü‡ßá update ‡¶π‡ßü‡•§

---

### ‚úÖ Advantages (PReLU)

‚úî Dead neuron ‡¶™‡ßç‡¶∞‡¶æ‡ßü ‡¶™‡ßÅ‡¶∞‡ßã‡¶™‡ßÅ‡¶∞‡¶ø solve
‚úî Adaptive (data-driven)
‚úî Very good for deep CNNs

---

### ‚ùå Disadvantages (PReLU)

‚ùå Extra parameter ‚áí overfitting risk
‚ùå Slightly more computation
‚ùå Small datasets-‡¶è ‡¶≠‡¶æ‡¶≤‡ßã ‡¶®‡¶æ‡¶ì ‡¶π‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá

---

## üîπ 4Ô∏è‚É£ ReLU vs Leaky ReLU vs PReLU (Comparison)

| Feature        | ReLU      | Leaky ReLU   | PReLU      |
| -------------- | --------- | ------------ | ---------- |
| Negative slope | 0         | Fixed (0.01) | Learnable  |
| Dead neuron    | ‚ùå High    | ‚ö†Ô∏è Low       | ‚úÖ Very low |
| Parameters     | None      | None         | Extra Œ±    |
| Speed          | Very fast | Very fast    | Fast       |
| Flexibility    | Low       | Medium       | High       |

---

## üß† Memory Trick

> **ReLU kills negatives, Leaky ReLU lets them whisper, PReLU lets the network decide.**

---

## ‚úçÔ∏è Exam-Ready One Line

* **Leaky ReLU** allows a small, fixed gradient for negative inputs to mitigate dead neurons.
* **PReLU** generalizes Leaky ReLU by learning the negative slope during training, improving flexibility at the cost of extra parameters.

---

