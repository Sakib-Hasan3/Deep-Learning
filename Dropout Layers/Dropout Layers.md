---

# üîπ Dropout Layer (Deep Learning)

## ‚ùì Dropout Layer ‡¶ï‡ßÄ?

**Dropout** ‡¶π‡¶≤‡ßã ‡¶è‡¶ï‡¶ü‡¶ø **regularization technique**, ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá training-‡¶è‡¶∞ ‡¶∏‡¶Æ‡ßü **random‡¶≠‡¶æ‡¶¨‡ßá ‡¶ï‡¶ø‡¶õ‡ßÅ neuron ‡¶∏‡¶æ‡¶Æ‡ßü‡¶ø‡¶ï‡¶≠‡¶æ‡¶¨‡ßá ‡¶¨‡¶®‡ßç‡¶ß (drop)** ‡¶ï‡¶∞‡ßá ‡¶¶‡ßá‡¶ì‡ßü‡¶æ ‡¶π‡ßü‡•§

üëâ ‡¶â‡¶¶‡ßç‡¶¶‡ßá‡¶∂‡ßç‡¶Ø:

* **Overfitting ‡¶ï‡¶Æ‡¶æ‡¶®‡ßã**
* Model-‡¶ï‡ßá **robust** ‡¶¨‡¶æ‡¶®‡¶æ‡¶®‡ßã

---

## üéØ ‡¶ï‡ßá‡¶® Dropout ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞?

‡¶Ø‡¶ñ‡¶®:

* Model ‡¶ñ‡ßÅ‡¶¨ complex
* Training accuracy ‡¶Ö‡¶®‡ßá‡¶ï ‡¶¨‡ßá‡¶∂‡¶ø
* Test accuracy ‡¶ï‡¶Æ

‡¶§‡¶ñ‡¶® model **training data ‡¶Æ‡ßÅ‡¶ñ‡¶∏‡ßç‡¶• (memorize)** ‡¶ï‡¶∞‡ßá ‡¶´‡ßá‡¶≤‡ßá ‚Üí **Overfitting**

Dropout ‡¶è‡¶á ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ ‡¶ï‡¶Æ‡¶æ‡ßü‡•§

---

## üß† Intuition (‡¶∏‡¶π‡¶ú‡¶≠‡¶æ‡¶¨‡ßá ‡¶¨‡ßã‡¶ù‡¶æ ‡¶Ø‡¶æ‡¶ï)

‡¶ß‡¶∞‡ßã,

* ‡¶è‡¶ï‡¶ü‡¶ø ‡¶¶‡¶≤ ‡¶Ü‡¶õ‡ßá
* ‡¶∏‡¶¨ ‡¶∏‡¶Æ‡ßü ‡¶è‡¶ï‡¶á ‡¶≤‡ßã‡¶ï ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡¶≤‡ßá ‡¶§‡¶æ‡¶∞‡¶æ ‡¶è‡¶ï‡ßá-‡¶Ö‡¶™‡¶∞‡ßá‡¶∞ ‡¶â‡¶™‡¶∞ ‡¶®‡¶ø‡¶∞‡ßç‡¶≠‡¶∞‡¶∂‡ßÄ‡¶≤ ‡¶π‡ßü‡ßá ‡¶™‡ßú‡ßá

Dropout ‡¶ï‡¶∞‡ßá ‡¶ï‡ßÄ ‡¶π‡ßü?

* ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶¨‡¶æ‡¶∞ ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶≤‡ßã‡¶ï ‡¶Ö‡¶®‡ßÅ‡¶™‡¶∏‡ßç‡¶•‡¶ø‡¶§
* ‡¶¨‡¶æ‡¶ï‡¶ø ‡¶≤‡ßã‡¶ï‡¶¶‡ßá‡¶∞ ‡¶è‡¶ï‡¶æ ‡¶ï‡¶æ‡¶ú ‡¶∂‡¶ø‡¶ñ‡¶§‡ßá ‡¶π‡ßü

‚û°Ô∏è Neural network-‡¶è:

> Neuron ‡¶Ö‡¶®‡ßç‡¶Ø neuron-‡¶è‡¶∞ ‡¶â‡¶™‡¶∞ ‡¶Ö‡¶§‡¶ø‡¶∞‡¶ø‡¶ï‡ßç‡¶§ ‡¶®‡¶ø‡¶∞‡ßç‡¶≠‡¶∞ ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá ‡¶®‡¶æ

---

## üß© Dropout ‡¶ï‡ßÄ‡¶≠‡¶æ‡¶¨‡ßá ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá?

‡¶ß‡¶∞‡¶ø:

* Dropout rate = **0.5**

üëâ ‡¶§‡¶æ‡¶∞ ‡¶Æ‡¶æ‡¶®‡ßá:

* ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø neuron-‡¶è‡¶∞ **‡ß´‡ß¶% ‡¶∏‡¶Æ‡ßç‡¶≠‡¶æ‡¶¨‡¶®‡¶æ** ‡¶Ü‡¶õ‡ßá ‡¶¨‡¶®‡ßç‡¶ß ‡¶π‡¶ì‡ßü‡¶æ‡¶∞

Training-‡¶è‡¶∞ ‡¶∏‡¶Æ‡ßü:

* ‡¶ï‡¶ø‡¶õ‡ßÅ neuron = 0
* ‡¶ï‡¶ø‡¶õ‡ßÅ neuron = active

Testing-‡¶è‡¶∞ ‡¶∏‡¶Æ‡ßü:

* ‚ùå Dropout ‡¶•‡¶æ‡¶ï‡ßá ‡¶®‡¶æ
* ‚úîÔ∏è ‡¶∏‡¶¨ neuron active ‡¶•‡¶æ‡¶ï‡ßá

---

## üìê Mathematical Explanation

### Without Dropout:

```math
y = w_1x_1 + w_2x_2 + w_3x_3
```

---

### With Dropout Mask

‡¶ß‡¶∞‡¶ø dropout mask:
```math
m = [1, 0, 1]
```

‡¶§‡¶æ‡¶π‡¶≤‡ßá,
```math
y = w_1x_1 + 0 + w_3x_3
```

---

### Scaling (Inverted Dropout)

Training-‡¶è‡¶∞ ‡¶∏‡¶Æ‡ßü:
```math
x' = \frac{m \cdot x}{1 - p}
```

‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá:

* ( p ) = dropout rate

üìå ‡¶ï‡ßá‡¶® scale ‡¶ï‡¶∞‡¶ø?
‚û°Ô∏è ‡¶Ø‡¶æ‡¶§‡ßá training ‡¶ì testing-‡¶è output magnitude ‡¶è‡¶ï‡¶á ‡¶•‡¶æ‡¶ï‡ßá

---

## üî¢ Numerical Example

‡¶ß‡¶∞‡¶ø:

* Input = [2, 4, 6]
* Weight = [1, 1, 1]
* Dropout rate = 0.5
* Mask = [1, 0, 1]

### Output (before scaling):

```math
y = 2 + 0 + 6 = 8
```

### After scaling:

```math
y = \frac{8}{1-0.5} = 16
```

---

## üìä Conceptual Graph (Neuron Drop)

```
Without Dropout:
Input ‚Üí ‚óè ‚Üí ‚óè ‚Üí ‚óè ‚Üí Output
          \   \   \
           ‚óè ‚Üí ‚óè ‚Üí ‚óè

With Dropout (Training):
Input ‚Üí ‚óè ‚Üí ‚úñ ‚Üí ‚óè ‚Üí Output
          \   
           ‚úñ ‚Üí ‚óè
```

‚úñ = dropped neuron

---

## üìà Graph Intuition (Loss Curve)

```
Loss
‚îÇ\
‚îÇ \        Without Dropout (overfit)
‚îÇ  \______
‚îÇ     \
‚îÇ      \___ With Dropout (better generalization)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Epochs
```

üëâ Dropout:

* Training loss ‡¶è‡¶ï‡¶ü‡ßÅ ‡¶¨‡ßá‡¶∂‡¶ø
* Validation loss ‡¶ï‡¶Æ
* Generalization ‡¶≠‡¶æ‡¶≤‡ßã

---

## ‚öôÔ∏è ‡¶ï‡ßã‡¶•‡¶æ‡ßü Dropout ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü?

‚úîÔ∏è Fully Connected Layer
‚úîÔ∏è CNN (FC part)
‚ùå Output layer
‚ùå BatchNorm-‡¶è‡¶∞ ‡¶™‡¶∞‡ßá ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£‡¶§ ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞ ‡¶π‡ßü ‡¶®‡¶æ

---

## üü¢ Common Dropout Rates

| Layer           | Dropout Rate |
| --------------- | ------------ |
| Input layer     | 0.1 ‚Äì 0.2    |
| Hidden layer    | 0.3 ‚Äì 0.5    |
| Very deep model | 0.5 ‚Äì 0.6    |

---

## üëç Advantages

* Overfitting ‡¶ï‡¶Æ‡¶æ‡ßü
* Generalization ‡¶¨‡¶æ‡ßú‡¶æ‡ßü
* Ensemble-like effect ‡¶¶‡ßá‡ßü

---

## üëé Disadvantages

* Training slow ‡¶π‡ßü
* Underfitting ‡¶π‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá
* RNN-‡¶è ‡¶∏‡¶∞‡¶æ‡¶∏‡¶∞‡¶ø ‡¶≠‡¶æ‡¶≤‡ßã ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá ‡¶®‡¶æ

---

## üìù Exam-Ready Definition

> **Dropout ‡¶π‡¶≤‡ßã ‡¶è‡¶ï‡¶ü‡¶ø regularization technique ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá training-‡¶è‡¶∞ ‡¶∏‡¶Æ‡ßü random‡¶≠‡¶æ‡¶¨‡ßá ‡¶ï‡¶ø‡¶õ‡ßÅ neuron ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡ßá ‡¶¶‡¶ø‡ßü‡ßá overfitting ‡¶ï‡¶Æ‡¶æ‡¶®‡ßã ‡¶π‡ßü‡•§**

---

## üîó Dropout ‡¶¨‡¶®‡¶æ‡¶Æ Weight Decay

| Feature | Dropout         | Weight Decay    |
| ------- | --------------- | --------------- |
| Method  | Neuron off      | Weight penalty  |
| Effect  | Robust features | Smaller weights |
| Speed   | Slower          | Faster          |

---

