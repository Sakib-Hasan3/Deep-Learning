---

## ЁЯФБ Bidirectional RNN: Architecture & Intuition (ржмрж╛ржВрж▓рж╛рзЯ рж╕рж╣ржЬ ржмрзНржпрж╛ржЦрзНржпрж╛)

![Image](https://miro.medium.com/v2/resize%3Afit%3A1400/1%2Aetwtg2e2-vBQKAwt6dbJQA.png)

![Image](https://www.researchgate.net/publication/328911755/figure/fig5/AS%3A726027592536064%401550109875468/Structure-of-bidirectional-recurrent-neural-network-BiRNN.png)

![Image](https://miro.medium.com/1%2Asf4vCzcyycSe7GC3dZ2u2w.png)

---

## 1я╕ПтГг Intuition (рж╕рж╣ржЬ ржзрж╛рж░ржгрж╛)

ржзрж░рзЛ ржПржХржЯрж┐ ржмрж╛ржХрзНржп:

> **тАЬI saw her duckтАЭ**

ржПржЦрж╛ржирзЗ **duck** рж╢ржмрзНржжржЯрж╛ verb ржирж╛ nounтАФржПржЯрж╛ ржмрзЛржЭрж╛ ржпрж╛рзЯ **ржЖржЧрзЗрж░ рж╢ржмрзНржж (saw)** ржПржмржВ **ржкрж░рзЗрж░ рж╢ржмрзНржж/рж╢рзЗрж╖ context** ржжрзЗржЦрзЗред
**Unidirectional RNN** рж╢рзБржзрзБ ржмрж╛ржотЖТржбрж╛ржи ржжрзЗржЦрзЗ, ржХрж┐ржирзНрждрзБ **BiRNN** ржжрзЗржЦрзЗ:

* ржмрж╛ржотЖТржбрж╛ржи (past context)
* ржбрж╛ржитЖТржмрж╛ржо (future context)

ЁЯСЙ рждрж╛ржЗ рж╕рж┐ржжрзНржзрж╛ржирзНржд рж╣рзЯ ржмрзЗрж╢рж┐ ржирж┐рж░рзНржнрзБрж▓ред

---

## 2я╕ПтГг Architecture (ржнрж┐рждрж░рзЗрж░ ржЧржаржи)

### ЁЯФ╣ ржжрзБржЯрж┐ ржЖрж▓рж╛ржжрж╛ RNN ржерж╛ржХрзЗ

1. **Forward RNN**

   * Input: (x_1 \rightarrow x_T)
   * Hidden: (\overrightarrow{h_1}, \overrightarrow{h_2}, \dots)

2. **Backward RNN**

   * Input: (x_T \rightarrow x_1)
   * Hidden: (\overleftarrow{h_T}, \overleftarrow{h_{T-1}}, \dots)

---

### ЁЯФ╣ Hidden State Combine ржХрж░рж╛ рж╣рзЯ

ржкрзНрж░рждрж┐ржЯрж┐ time step-ржП:
[
h_t = [\overrightarrow{h_t} ; || ; \overleftarrow{h_t}]
]
(Concatenation рж╕ржмржЪрзЗрзЯрзЗ common)

рждрж╛рж░ржкрж░:
[
y_t = g(W_y h_t + b_y)
]

---

## 3я╕ПтГг Step-by-Step Flow

1. Input sequence ржжрзЗржУрзЯрж╛ рж╣рзЯ
   [
   x_1, x_2, x_3, \dots, x_T
   ]
2. Forward RNN тЖТ left to right hidden рждрзИрж░рж┐
3. Backward RNN тЖТ right to left hidden рждрзИрж░рж┐
4. ржжрзБржЗ ржжрж┐ржХрзЗрж░ hidden **merge**
5. Output layer рж╕рж┐ржжрзНржзрж╛ржирзНржд ржирзЗрзЯ

---

## 4я╕ПтГг ASCII Diagram (ржорж╛ржерж╛рзЯ ржмрж╕рж╛ржирзЛрж░ ржЬржирзНржп)

```
Forward:   x1 тЖТ x2 тЖТ x3 тЖТ x4
            тЖУ    тЖУ    тЖУ    тЖУ
           h1тЖТ  h2тЖТ  h3тЖТ  h4тЖТ

Backward:  x1 тЖР x2 тЖР x3 тЖР x4
            тЖС    тЖС    тЖС    тЖС
           тЖРh1  тЖРh2  тЖРh3  тЖРh4

Combined: [h1тЖТ||тЖРh1], [h2тЖТ||тЖРh2], ...
```

---

## 5я╕ПтГг ржХрзЗржи BiRNN ржПржд рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзА?

### тЬЕ Future context ржЬрж╛ржирзЗ

* ржмрж░рзНрждржорж╛ржи рж╕рж┐ржжрзНржзрж╛ржирзНрждрзЗ **ржЖржЧрзЗрж░ + ржкрж░рзЗрж░ рждржерзНржп**

### тЬЕ Ambiguity ржХржорзЗ

* Homonym / polysemy рж╕ржорж╕рзНржпрж╛ ржХржо

### тЬЕ Sequence labeling-ржП рж╕рзЗрж░рж╛

* POS tagging
* Named Entity Recognition
* Speech phoneme labeling

---

## 6я╕ПтГг BiRNN ржХрзЛржерж╛рзЯ ржмрзНржпржмрж╣рж╛рж░ рж╣рзЯ?

* NLP: POS, NER, sentiment (word-level)
* Speech recognition
* Bio-sequence (DNA, protein)

тЭМ **Real-time prediction-ржП рж╕ржорж╕рзНржпрж╛**
ржХрж╛рж░ржг future input ржЖржЧрзЗ ржерзЗржХрзЗржЗ ржжрж░ржХрж╛рж░ред

---

## 7я╕ПтГг Advantages vs Limitations

### тЬЕ Advantages

* Rich context (past + future)
* Higher accuracy (offline tasks)
* Better language understanding

### тЭМ Limitations

* Real-time/streaming-ржП ржмрзНржпржмрж╣рж╛рж░ ржХржарж┐ржи
* Computation & memory ржмрзЗрж╢рж┐
* Training ржзрзАрж░

---

## 8я╕ПтГг BiRNN vs Normal RNN (Quick Table)

| Feature   | RNN       | BiRNN         |
| --------- | --------- | ------------- |
| Direction | One-way   | Two-way       |
| Context   | Past only | Past + Future |
| Accuracy  | Medium    | High          |
| Real-time | Yes       | No            |
| Cost      | Low       | Higher        |

---

## ЁЯза ржПржХ рж▓рж╛ржЗржирзЗрж░ Takeaway

**Bidirectional RNN ржкрзНрж░рждрж┐ржЯрж┐ рж╢ржмрзНржж/step-ржХрзЗ ржжрзБржЗ ржжрж┐ржХрзЗрж░ context ржжрж┐рзЯрзЗ ржжрзЗржЦрзЗтАФрждрж╛ржЗ offline sequence understanding-ржП ржПржЯрж┐ рж╕рж╛ржзрж╛рж░ржг RNN-ржПрж░ ржЪрзЗрзЯрзЗ ржЕржирзЗржХ ржмрзЗрж╢рж┐ рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзАред**

---

