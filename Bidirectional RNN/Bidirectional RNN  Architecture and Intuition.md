<<<<<<< HEAD
---

## ğŸ” Bidirectional RNN: Architecture & Intuition (à¦¬à¦¾à¦‚à¦²à¦¾à§Ÿ à¦¸à¦¹à¦œ à¦¬à§à¦¯à¦¾à¦–à§à¦¯à¦¾)

![Image](https://miro.medium.com/v2/resize%3Afit%3A1400/1%2Aetwtg2e2-vBQKAwt6dbJQA.png)

![Image](https://www.researchgate.net/publication/328911755/figure/fig5/AS%3A726027592536064%401550109875468/Structure-of-bidirectional-recurrent-neural-network-BiRNN.png)

![Image](https://miro.medium.com/1%2Asf4vCzcyycSe7GC3dZ2u2w.png)

---

## 1ï¸âƒ£ Intuition (à¦¸à¦¹à¦œ à¦§à¦¾à¦°à¦£à¦¾)

à¦§à¦°à§‹ à¦à¦•à¦Ÿà¦¿ à¦¬à¦¾à¦•à§à¦¯:

> **â€œI saw her duckâ€**

à¦à¦–à¦¾à¦¨à§‡ **duck** à¦¶à¦¬à§à¦¦à¦Ÿà¦¾ verb à¦¨à¦¾ nounâ€”à¦à¦Ÿà¦¾ à¦¬à§‹à¦à¦¾ à¦¯à¦¾à§Ÿ **à¦†à¦—à§‡à¦° à¦¶à¦¬à§à¦¦ (saw)** à¦à¦¬à¦‚ **à¦ªà¦°à§‡à¦° à¦¶à¦¬à§à¦¦/à¦¶à§‡à¦· context** à¦¦à§‡à¦–à§‡à¥¤
**Unidirectional RNN** à¦¶à§à¦§à§ à¦¬à¦¾à¦®â†’à¦¡à¦¾à¦¨ à¦¦à§‡à¦–à§‡, à¦•à¦¿à¦¨à§à¦¤à§ **BiRNN** à¦¦à§‡à¦–à§‡:

* à¦¬à¦¾à¦®â†’à¦¡à¦¾à¦¨ (past context)
* à¦¡à¦¾à¦¨â†’à¦¬à¦¾à¦® (future context)

ğŸ‘‰ à¦¤à¦¾à¦‡ à¦¸à¦¿à¦¦à§à¦§à¦¾à¦¨à§à¦¤ à¦¹à§Ÿ à¦¬à§‡à¦¶à¦¿ à¦¨à¦¿à¦°à§à¦­à§à¦²à¥¤

---

## 2ï¸âƒ£ Architecture (à¦­à¦¿à¦¤à¦°à§‡à¦° à¦—à¦ à¦¨)

### ğŸ”¹ à¦¦à§à¦Ÿà¦¿ à¦†à¦²à¦¾à¦¦à¦¾ RNN à¦¥à¦¾à¦•à§‡

1. **Forward RNN**

   * Input: (x_1 \rightarrow x_T)
   * Hidden: (\overrightarrow{h_1}, \overrightarrow{h_2}, \dots)

2. **Backward RNN**

   * Input: (x_T \rightarrow x_1)
   * Hidden: (\overleftarrow{h_T}, \overleftarrow{h_{T-1}}, \dots)

---

### ğŸ”¹ Hidden State Combine à¦•à¦°à¦¾ à¦¹à§Ÿ

à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ time step-à¦:
[
h_t = [\overrightarrow{h_t} ; || ; \overleftarrow{h_t}]
]
(Concatenation à¦¸à¦¬à¦šà§‡à§Ÿà§‡ common)

à¦¤à¦¾à¦°à¦ªà¦°:
[
y_t = g(W_y h_t + b_y)
]

---

## 3ï¸âƒ£ Step-by-Step Flow

1. Input sequence à¦¦à§‡à¦“à§Ÿà¦¾ à¦¹à§Ÿ
   [
   x_1, x_2, x_3, \dots, x_T
   ]
2. Forward RNN â†’ left to right hidden à¦¤à§ˆà¦°à¦¿
3. Backward RNN â†’ right to left hidden à¦¤à§ˆà¦°à¦¿
4. à¦¦à§à¦‡ à¦¦à¦¿à¦•à§‡à¦° hidden **merge**
5. Output layer à¦¸à¦¿à¦¦à§à¦§à¦¾à¦¨à§à¦¤ à¦¨à§‡à§Ÿ

---

## 4ï¸âƒ£ ASCII Diagram (à¦®à¦¾à¦¥à¦¾à§Ÿ à¦¬à¦¸à¦¾à¦¨à§‹à¦° à¦œà¦¨à§à¦¯)

```
Forward:   x1 â†’ x2 â†’ x3 â†’ x4
            â†“    â†“    â†“    â†“
           h1â†’  h2â†’  h3â†’  h4â†’

Backward:  x1 â† x2 â† x3 â† x4
            â†‘    â†‘    â†‘    â†‘
           â†h1  â†h2  â†h3  â†h4

Combined: [h1â†’||â†h1], [h2â†’||â†h2], ...
```

---

## 5ï¸âƒ£ à¦•à§‡à¦¨ BiRNN à¦à¦¤ à¦¶à¦•à§à¦¤à¦¿à¦¶à¦¾à¦²à§€?

### âœ… Future context à¦œà¦¾à¦¨à§‡

* à¦¬à¦°à§à¦¤à¦®à¦¾à¦¨ à¦¸à¦¿à¦¦à§à¦§à¦¾à¦¨à§à¦¤à§‡ **à¦†à¦—à§‡à¦° + à¦ªà¦°à§‡à¦° à¦¤à¦¥à§à¦¯**

### âœ… Ambiguity à¦•à¦®à§‡

* Homonym / polysemy à¦¸à¦®à¦¸à§à¦¯à¦¾ à¦•à¦®

### âœ… Sequence labeling-à¦ à¦¸à§‡à¦°à¦¾

* POS tagging
* Named Entity Recognition
* Speech phoneme labeling

---

## 6ï¸âƒ£ BiRNN à¦•à§‹à¦¥à¦¾à§Ÿ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦¹à§Ÿ?

* NLP: POS, NER, sentiment (word-level)
* Speech recognition
* Bio-sequence (DNA, protein)

âŒ **Real-time prediction-à¦ à¦¸à¦®à¦¸à§à¦¯à¦¾**
à¦•à¦¾à¦°à¦£ future input à¦†à¦—à§‡ à¦¥à§‡à¦•à§‡à¦‡ à¦¦à¦°à¦•à¦¾à¦°à¥¤

---

## 7ï¸âƒ£ Advantages vs Limitations

### âœ… Advantages

* Rich context (past + future)
* Higher accuracy (offline tasks)
* Better language understanding

### âŒ Limitations

* Real-time/streaming-à¦ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦ à¦¿à¦¨
* Computation & memory à¦¬à§‡à¦¶à¦¿
* Training à¦§à§€à¦°

---

## 8ï¸âƒ£ BiRNN vs Normal RNN (Quick Table)

| Feature   | RNN       | BiRNN         |
| --------- | --------- | ------------- |
| Direction | One-way   | Two-way       |
| Context   | Past only | Past + Future |
| Accuracy  | Medium    | High          |
| Real-time | Yes       | No            |
| Cost      | Low       | Higher        |

---

## ğŸ§  à¦à¦• à¦²à¦¾à¦‡à¦¨à§‡à¦° Takeaway

**Bidirectional RNN à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ à¦¶à¦¬à§à¦¦/step-à¦•à§‡ à¦¦à§à¦‡ à¦¦à¦¿à¦•à§‡à¦° context à¦¦à¦¿à§Ÿà§‡ à¦¦à§‡à¦–à§‡â€”à¦¤à¦¾à¦‡ offline sequence understanding-à¦ à¦à¦Ÿà¦¿ à¦¸à¦¾à¦§à¦¾à¦°à¦£ RNN-à¦à¦° à¦šà§‡à§Ÿà§‡ à¦…à¦¨à§‡à¦• à¦¬à§‡à¦¶à¦¿ à¦¶à¦•à§à¦¤à¦¿à¦¶à¦¾à¦²à§€à¥¤**

---

=======
---

## ğŸ” Bidirectional RNN: Architecture & Intuition (à¦¬à¦¾à¦‚à¦²à¦¾à§Ÿ à¦¸à¦¹à¦œ à¦¬à§à¦¯à¦¾à¦–à§à¦¯à¦¾)

![Image](https://miro.medium.com/v2/resize%3Afit%3A1400/1%2Aetwtg2e2-vBQKAwt6dbJQA.png)

![Image](https://www.researchgate.net/publication/328911755/figure/fig5/AS%3A726027592536064%401550109875468/Structure-of-bidirectional-recurrent-neural-network-BiRNN.png)

![Image](https://miro.medium.com/1%2Asf4vCzcyycSe7GC3dZ2u2w.png)

---

## 1ï¸âƒ£ Intuition (à¦¸à¦¹à¦œ à¦§à¦¾à¦°à¦£à¦¾)

à¦§à¦°à§‹ à¦à¦•à¦Ÿà¦¿ à¦¬à¦¾à¦•à§à¦¯:

> **â€œI saw her duckâ€**

à¦à¦–à¦¾à¦¨à§‡ **duck** à¦¶à¦¬à§à¦¦à¦Ÿà¦¾ verb à¦¨à¦¾ nounâ€”à¦à¦Ÿà¦¾ à¦¬à§‹à¦à¦¾ à¦¯à¦¾à§Ÿ **à¦†à¦—à§‡à¦° à¦¶à¦¬à§à¦¦ (saw)** à¦à¦¬à¦‚ **à¦ªà¦°à§‡à¦° à¦¶à¦¬à§à¦¦/à¦¶à§‡à¦· context** à¦¦à§‡à¦–à§‡à¥¤
**Unidirectional RNN** à¦¶à§à¦§à§ à¦¬à¦¾à¦®â†’à¦¡à¦¾à¦¨ à¦¦à§‡à¦–à§‡, à¦•à¦¿à¦¨à§à¦¤à§ **BiRNN** à¦¦à§‡à¦–à§‡:

* à¦¬à¦¾à¦®â†’à¦¡à¦¾à¦¨ (past context)
* à¦¡à¦¾à¦¨â†’à¦¬à¦¾à¦® (future context)

ğŸ‘‰ à¦¤à¦¾à¦‡ à¦¸à¦¿à¦¦à§à¦§à¦¾à¦¨à§à¦¤ à¦¹à§Ÿ à¦¬à§‡à¦¶à¦¿ à¦¨à¦¿à¦°à§à¦­à§à¦²à¥¤

---

## 2ï¸âƒ£ Architecture (à¦­à¦¿à¦¤à¦°à§‡à¦° à¦—à¦ à¦¨)

### ğŸ”¹ à¦¦à§à¦Ÿà¦¿ à¦†à¦²à¦¾à¦¦à¦¾ RNN à¦¥à¦¾à¦•à§‡

1. **Forward RNN**

   * Input: (x_1 \rightarrow x_T)
   * Hidden: (\overrightarrow{h_1}, \overrightarrow{h_2}, \dots)

2. **Backward RNN**

   * Input: (x_T \rightarrow x_1)
   * Hidden: (\overleftarrow{h_T}, \overleftarrow{h_{T-1}}, \dots)

---

### ğŸ”¹ Hidden State Combine à¦•à¦°à¦¾ à¦¹à§Ÿ

à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ time step-à¦:
[
h_t = [\overrightarrow{h_t} ; || ; \overleftarrow{h_t}]
]
(Concatenation à¦¸à¦¬à¦šà§‡à§Ÿà§‡ common)

à¦¤à¦¾à¦°à¦ªà¦°:
[
y_t = g(W_y h_t + b_y)
]

---

## 3ï¸âƒ£ Step-by-Step Flow

1. Input sequence à¦¦à§‡à¦“à§Ÿà¦¾ à¦¹à§Ÿ
   [
   x_1, x_2, x_3, \dots, x_T
   ]
2. Forward RNN â†’ left to right hidden à¦¤à§ˆà¦°à¦¿
3. Backward RNN â†’ right to left hidden à¦¤à§ˆà¦°à¦¿
4. à¦¦à§à¦‡ à¦¦à¦¿à¦•à§‡à¦° hidden **merge**
5. Output layer à¦¸à¦¿à¦¦à§à¦§à¦¾à¦¨à§à¦¤ à¦¨à§‡à§Ÿ

---

## 4ï¸âƒ£ ASCII Diagram (à¦®à¦¾à¦¥à¦¾à§Ÿ à¦¬à¦¸à¦¾à¦¨à§‹à¦° à¦œà¦¨à§à¦¯)

```
Forward:   x1 â†’ x2 â†’ x3 â†’ x4
            â†“    â†“    â†“    â†“
           h1â†’  h2â†’  h3â†’  h4â†’

Backward:  x1 â† x2 â† x3 â† x4
            â†‘    â†‘    â†‘    â†‘
           â†h1  â†h2  â†h3  â†h4

Combined: [h1â†’||â†h1], [h2â†’||â†h2], ...
```

---

## 5ï¸âƒ£ à¦•à§‡à¦¨ BiRNN à¦à¦¤ à¦¶à¦•à§à¦¤à¦¿à¦¶à¦¾à¦²à§€?

### âœ… Future context à¦œà¦¾à¦¨à§‡

* à¦¬à¦°à§à¦¤à¦®à¦¾à¦¨ à¦¸à¦¿à¦¦à§à¦§à¦¾à¦¨à§à¦¤à§‡ **à¦†à¦—à§‡à¦° + à¦ªà¦°à§‡à¦° à¦¤à¦¥à§à¦¯**

### âœ… Ambiguity à¦•à¦®à§‡

* Homonym / polysemy à¦¸à¦®à¦¸à§à¦¯à¦¾ à¦•à¦®

### âœ… Sequence labeling-à¦ à¦¸à§‡à¦°à¦¾

* POS tagging
* Named Entity Recognition
* Speech phoneme labeling

---

## 6ï¸âƒ£ BiRNN à¦•à§‹à¦¥à¦¾à§Ÿ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦¹à§Ÿ?

* NLP: POS, NER, sentiment (word-level)
* Speech recognition
* Bio-sequence (DNA, protein)

âŒ **Real-time prediction-à¦ à¦¸à¦®à¦¸à§à¦¯à¦¾**
à¦•à¦¾à¦°à¦£ future input à¦†à¦—à§‡ à¦¥à§‡à¦•à§‡à¦‡ à¦¦à¦°à¦•à¦¾à¦°à¥¤

---

## 7ï¸âƒ£ Advantages vs Limitations

### âœ… Advantages

* Rich context (past + future)
* Higher accuracy (offline tasks)
* Better language understanding

### âŒ Limitations

* Real-time/streaming-à¦ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦ à¦¿à¦¨
* Computation & memory à¦¬à§‡à¦¶à¦¿
* Training à¦§à§€à¦°

---

## 8ï¸âƒ£ BiRNN vs Normal RNN (Quick Table)

| Feature   | RNN       | BiRNN         |
| --------- | --------- | ------------- |
| Direction | One-way   | Two-way       |
| Context   | Past only | Past + Future |
| Accuracy  | Medium    | High          |
| Real-time | Yes       | No            |
| Cost      | Low       | Higher        |

---

## ğŸ§  à¦à¦• à¦²à¦¾à¦‡à¦¨à§‡à¦° Takeaway

**Bidirectional RNN à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ à¦¶à¦¬à§à¦¦/step-à¦•à§‡ à¦¦à§à¦‡ à¦¦à¦¿à¦•à§‡à¦° context à¦¦à¦¿à§Ÿà§‡ à¦¦à§‡à¦–à§‡â€”à¦¤à¦¾à¦‡ offline sequence understanding-à¦ à¦à¦Ÿà¦¿ à¦¸à¦¾à¦§à¦¾à¦°à¦£ RNN-à¦à¦° à¦šà§‡à§Ÿà§‡ à¦…à¦¨à§‡à¦• à¦¬à§‡à¦¶à¦¿ à¦¶à¦•à§à¦¤à¦¿à¦¶à¦¾à¦²à§€à¥¤**

---

>>>>>>> f45ebbad1686e699afe9932c4175eeff501d254b
