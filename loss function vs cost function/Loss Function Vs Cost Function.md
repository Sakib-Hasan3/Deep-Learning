## **Loss Function vs Cost Function â€” à¦¸à¦¹à¦œ à¦¬à§à¦¯à¦¾à¦–à§à¦¯à¦¾ (Exam + Intuition)**

![Image](https://images.squarespace-cdn.com/content/v1/5acbdd3a25bf024c12f4c8b4/1600287667276-1LRN123BHJVQI9353VDR/Loss%2B%28Cost%29%2BFunction.png)

![Image](https://miro.medium.com/1%2AN1PyOYeog-vyytRbwEwQCQ.png)

![Image](https://miro.medium.com/v2/resize%3Afit%3A1400/1%2AXyxbSgjG5oUSQ0BdZ2DHMQ.png)

![Image](https://miro.medium.com/v2/resize%3Afit%3A1200/1%2AIO881p4fDp7NfjKMf5VHPA.png)

---

## ðŸ”¹ 1ï¸âƒ£ Loss Function à¦•à§€?

**Loss Function** à¦à¦•à¦Ÿà¦¿à¦®à¦¾à¦¤à§à¦° **data sample**â€“à¦à¦° à¦œà¦¨à§à¦¯ model à¦•à¦¤à¦Ÿà¦¾ à¦­à§à¦² à¦•à¦°à§‡à¦›à§‡ à¦¤à¦¾ à¦®à¦¾à¦ªà§‡à¥¤

### à¦¸à¦‚à¦œà§à¦žà¦¾

> **Single example-à¦à¦° error measure = Loss**

### Mathematical form

à¦§à¦°à¦¿, à¦à¦•à¦Ÿà¦¿à¦®à¦¾à¦¤à§à¦° sample ((x, y)):
[
\text{Loss} = \ell(y,\ \hat{y})
]

### à¦‰à¦¦à¦¾à¦¹à¦°à¦£

* **MSE (single sample)**
  [
  \ell = \frac{1}{2}(y-\hat{y})^2
  ]
* **Cross-Entropy (single sample)**
  [
  \ell = -\sum y_i\log(\hat{p}_i)
  ]

ðŸ‘‰ à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾ data point-à¦à¦° à¦œà¦¨à§à¦¯ à¦†à¦²à¦¾à¦¦à¦¾ à¦†à¦²à¦¾à¦¦à¦¾ loss à¦¹à§Ÿà¥¤

---

## ðŸ”¹ 2ï¸âƒ£ Cost Function à¦•à§€?

**Cost Function** à¦ªà§à¦°à§‹ **dataset**â€“à¦à¦° à¦‰à¦ªà¦° **average (à¦¬à¦¾ sum) loss**à¥¤

### à¦¸à¦‚à¦œà§à¦žà¦¾

> **All samples-à¦à¦° overall error = Cost**

### Mathematical form

à¦§à¦°à¦¿, à¦®à§‹à¦Ÿ (N) à¦Ÿà¦¿ sample:
[
\text{Cost} = \frac{1}{N}\sum_{i=1}^{N}\ell_i
]

ðŸ‘‰ Training à¦šà¦²à¦¾à¦•à¦¾à¦²à§€à¦¨ à¦†à¦®à¦°à¦¾ **cost minimize** à¦•à¦°à¦¿à¥¤

---

## ðŸ”¹ 3ï¸âƒ£ Intuition (à¦¸à¦¹à¦œ à¦­à¦¾à¦·à¦¾à§Ÿ)

* **Loss** = â€œà¦à¦‡ à¦à¦•à¦Ÿà¦¾ à¦ªà§à¦°à¦¶à§à¦¨à§‡ à¦•à¦¤ à¦¨à¦®à§à¦¬à¦° à¦•à§‡à¦Ÿà§‡à¦›à§‡?â€
* **Cost** = â€œà¦ªà§à¦°à§‹ à¦ªà¦°à§€à¦•à§à¦·à¦¾à§Ÿ à¦—à§œ à¦¨à¦®à§à¦¬à¦° à¦•à§‡à¦®à¦¨?â€

---

## ðŸ”¹ 4ï¸âƒ£ Numerical Mini Example

à¦§à¦°à¦¿ 3à¦Ÿà¦¾ sample-à¦à¦° loss:
[
\ell_1=0.2,\quad \ell_2=0.4,\quad \ell_3=0.1
]

à¦¤à¦¾à¦¹à¦²à§‡ Cost:
[
J=\frac{0.2+0.4+0.1}{3}=0.233
]

ðŸ‘‰ Optimizer (Gradient Descent) à¦à¦‡ **0.233** à¦•à¦®à¦¾à¦¤à§‡ à¦šà§‡à¦·à§à¦Ÿà¦¾ à¦•à¦°à¦¬à§‡à¥¤

---

## ðŸ”¹ 5ï¸âƒ£ Training-à¦ à¦•à§‹à¦¨à¦Ÿà¦¾ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦¹à§Ÿ?

* **Backpropagation:**
  ðŸ‘‰ Loss-à¦à¦° gradient â†’ sample-wise
* **Optimization target:**
  ðŸ‘‰ Cost function minimize à¦•à¦°à¦¾

à¦ªà§à¦°à§à¦¯à¦¾à¦•à§à¦Ÿà¦¿à¦•à§à¦¯à¦¾à¦²à¦¿:

* Mini-batch training â‡’ **Batch cost**
  [
  J_{\text{batch}}=\frac{1}{m}\sum_{i=1}^{m}\ell_i
  ]

---

## ðŸ”¹ 6ï¸âƒ£ Loss vs Cost (Side-by-Side)

| à¦¬à¦¿à¦·à§Ÿ    | Loss Function        | Cost Function           |
| ------- | -------------------- | ----------------------- |
| Scope   | Single sample        | Entire dataset / batch  |
| Purpose | Individual error     | Overall performance     |
| Used in | Gradient calculation | Optimization target     |
| Example | MSE, CE (per sample) | Average MSE, Average CE |

---

## ðŸ”¹ 7ï¸âƒ£ Common Confusion (Exam Tip âš ï¸)

à¦…à¦¨à§‡à¦• à¦¬à¦‡/à¦•à§‹à¦°à§à¦¸à§‡ **Cost** à¦†à¦° **Loss** à¦¶à¦¬à§à¦¦ à¦¦à§à¦Ÿà§‹ interchangeably à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§‡à¥¤
ðŸ‘‰ à¦•à¦¿à¦¨à§à¦¤à§ **strict definition** à¦…à¦¨à§à¦¯à¦¾à§Ÿà§€ à¦ªà¦¾à¦°à§à¦¥à¦•à§à¦¯à¦Ÿà¦¾ à¦®à¦¨à§‡ à¦°à¦¾à¦–à¦²à§‡ exam-à¦ à¦¸à§à¦¬à¦¿à¦§à¦¾ à¦¹à¦¬à§‡à¥¤

---

## ðŸ§  Memory Trick

> **Loss = one data point**
> **Cost = all data points together**

---

## âœï¸ Exam-Ready One Line

> **Loss function measures error for a single training example, whereas cost function represents the average loss over the entire dataset and is minimized during training.**


