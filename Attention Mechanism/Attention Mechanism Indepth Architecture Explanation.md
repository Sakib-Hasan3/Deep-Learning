---

# ЁЯза Attention Mechanism тАФ In-Depth Architecture Explanation (рж╕рж╣ржЬ + ржЧржнрзАрж░ржнрж╛ржмрзЗ)

## 1я╕ПтГг ржХрзЗржи Attention ржжрж░ржХрж╛рж░ ржЫрж┐рж▓?
Vanilla Seq2Seq-ржП:
[
x_1,x_2,\dots,x_T ;\xrightarrow{\text{Encoder}}; C ;\xrightarrow{\text{Decoder}}; y_1,y_2,\dots
]

рж╕ржорж╕рзНржпрж╛:

* ржкрзБрж░рзЛ sentence тЖТ ржПржХржЯрж╛ржЗ context vector (C)
* Long sentence тЖТ information loss

ЁЯСЙ Attention ржмрж▓рж▓:

> тАЬDecoder ржХрзЗржи рж╢рзБржзрзБ рж╢рзЗрж╖ hidden state ржирзЗржмрзЗ?
> ржУ рждрзЛ Encoder-ржПрж░ рж╕ржм hidden state ржжрзЗржЦрждрзЗ ржкрж╛рж░рзЗ!тАЭ

---

## 2я╕ПтГг Core Idea (ржорж╛ржирзБрж╖рзЗрж░ ржЙржжрж╛рж╣рж░ржг)

рждрзБржорж┐ ржпржжрж┐ ржПржХржЯрж╛ paragraph ржкрзЬрзЗ ржкрзНрж░рж╢рзНржирзЗрж░ ржЙрждрзНрждрж░ ржжрж╛ржУ:

* ржкрзБрж░рзЛ paragraph ржорж╛ржерж╛рзЯ рж░рж╛ржЦрзЛ ржирж╛
* ржкрзНрж░рж╢рзНржи ржЕржирзБржпрж╛рзЯрзА ржирж┐рж░рзНржжрж┐рж╖рзНржЯ рж▓рж╛ржЗржи-ржП тАЬржоржирзЛржпрзЛржЧтАЭ ржжрж╛ржУ

Attention ржарж┐ржХ ржПржЗ ржХрж╛ржЬржЯрж╛ржЗ ржХрж░рзЗред

---

## 3я╕ПтГг Full Architecture (Step-by-Step)

![Image](https://images.openai.com/static-rsc-3/WDuHb64OVwEt0Dbfie7hNwtoCGvOKHFlgQCPeYn5XL78wA6oR2HOoPJP_2-FVdzTrlBXi7-DRySEQO2LP6p8F9BpETqN0Dl_i9A27o8jZOM?purpose=fullsize\&v=1)

![Image](https://lena-voita.github.io/resources/lectures/seq2seq/attention/attn_for_steps/6-min.png)

![Image](https://www.researchgate.net/publication/365484579/figure/fig4/AS%3A11431281098009354%401668752357661/Heat-map-of-self-attention-on-the-decoder.jpg)

![Image](https://www.researchgate.net/publication/335781217/figure/fig13/AS%3A802477117038592%401568336862070/Figure-A2-Each-heatmap-shows-the-proportion-of-attention-originating-from-the-given.ppm)

---

# Step 1 тАФ Encoder

Input:
[
x_1,x_2,\dots,x_T
]

Encoder produces:
[
h_1,h_2,\dots,h_T
]

тЪая╕П ржПржмрж╛рж░ ржЖрж░ рж╢рзБржзрзБ (h_T) ржирзЗржУрзЯрж╛ рж╣ржмрзЗ ржирж╛ред
рж╕ржм hidden state рж╕ржВрж░ржХрзНрж╖ржг ржХрж░рж╛ рж╣ржмрзЗред

---

# Step 2 тАФ Decoder State

Decoder at time step (t):

[
s_t
]

ржПржЯрж╛ржЗ рж╣ржмрзЗ query (ржХрж┐ ржЬрж╛ржирждрзЗ ржЪрж╛рзЯ)ред

---

# Step 3 тАФ Alignment Score (Energy Calculation)

ржкрзНрж░рждрж┐ржЯрж┐ encoder hidden (h_i)-ржПрж░ рж╕рж╛ржерзЗ score ржмрзЗрж░ ржХрж░рж╛ рж╣рзЯ:

[
e_{t,i} = \text{score}(s_t, h_i)
]

Popular score functions:

### 1) Dot Product:

[
e_{t,i} = s_t^\top h_i
]

### 2) General:

[
e_{t,i} = s_t^\top W h_i
]

### 3) Additive (Bahdanau):

[
e_{t,i} = v^\top \tanh(W_1 s_t + W_2 h_i)
]

ЁЯСЙ ржПржЯрж╛ ржмрж▓рзЗ:

> тАЬржПржЗ input position ржХрждржЯрж╛ relevant?тАЭ

---

# Step 4 тАФ Softmax (Attention Weights)

[
\alpha_{t,i} = \frac{\exp(e_{t,i})}{\sum_j \exp(e_{t,j})}
]

ржПржЧрзБрж▓рзЛ рж╣рж▓рзЛ **attention weights**

* рж╕ржмржЧрзБрж▓рзЛрж░ ржпрзЛржЧржлрж▓ = 1
* Probability distribution

ЁЯСЙ ржпрзЗржЯрж╛ ржмрзЗрж╢рж┐ relevant тЖТ weight ржмрзЗрж╢рж┐

---

# Step 5 тАФ Context Vector (Dynamic!)

[
c_t = \sum_i \alpha_{t,i} h_i
]

тЪая╕П ржПржЯрж╛ ржЖрж░ fixed ржирзЯ
ржкрзНрж░рждрж┐ржЯрж┐ decoder step-ржП ржЖрж▓рж╛ржжрж╛ context рждрзИрж░рж┐ рж╣рзЯ

---

# Step 6 тАФ Output Generation

Decoder update:

[
\tilde{s_t} = \tanh(W_c [s_t || c_t])
]

[
y_t = \text{softmax}(W_o \tilde{s_t})
]

---

# ЁЯФД ржкрзБрж░рзЛ Flow (ржПржХ ржиржЬрж░рзЗ)

```
Encoder:
x1 тЖТ h1
x2 тЖТ h2
x3 тЖТ h3
...

Decoder Step t:
1. Compute score(s_t, h_i)
2. Softmax тЖТ attention weights
3. Weighted sum тЖТ context c_t
4. Combine with s_t
5. Predict y_t
```

---

# ЁЯзо Mini Intuition Example (No heavy math)

ржзрж░рж┐ encoder hidden states:

[
h_1=2,; h_2=5,; h_3=1
]

Decoder state:

[
s_t=4
]

Score (dot product):

[
e_{t,i}=s_t \cdot h_i
]

[
= (8,;20,;4)
]

Softmax ржжрж┐рж▓рзЗ:
[
\alpha \approx (0.0003,;0.999,;0.00005)
]

Context:
[
c_t \approx 5
]

ЁЯСЙ Decoder ржмрзБржЭрзЗржЫрзЗ:

> ржжрзНржмрж┐рждрзАрзЯ рж╢ржмрзНржжржЯрж╛ржЗ рж╕ржмржЪрзЗрзЯрзЗ ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг

---

# 4я╕ПтГг Why Attention Works So Well

| Vanilla Seq2Seq    | With Attention     |
| ------------------ | ------------------ |
| Fixed context      | Dynamic context    |
| Info loss          | Full memory access |
| No alignment       | Explicit alignment |
| Long sentence fail | Long sentence ok   |

---

# 5я╕ПтГг Types of Attention

### ЁЯФ╣ Bahdanau (Additive)

* Original attention
* More flexible

### ЁЯФ╣ Luong (Multiplicative)

* Faster
* Dot-product based

### ЁЯФ╣ Self-Attention

* Transformer-ржПрж░ ржнрж┐рждрзНрждрж┐
* Encoder-ржПрж░ ржнрзЗрждрж░рзЗржЗ attention

---

# 6я╕ПтГг Attention Heatmap (Interpretability)

Attention matrix visualize ржХрж░рж▓рзЗ ржжрзЗржЦрж╛ ржпрж╛рзЯ:

* ржХрзЛржи output word
* input-ржПрж░ ржХрзЛржи ржЕржВрж╢рзЗ focus ржХрж░рзЗржЫрзЗ

ржПржЯрж╛ explainability ржжрзЗрзЯред

---

# 7я╕ПтГг Limitations of Attention

* Computation heavy (O(T┬▓))
* Memory intensive
* Very long sequence-ржП expensive

ЁЯСЙ ржПржЦрж╛ржи ржерзЗржХрзЗржЗ ржПрж╕рзЗржЫрзЗ:

* Scaled Dot-Product Attention
* Multi-Head Attention
* Transformer

---

# ЁЯза One-Line Takeaway

**Attention decoder-ржХрзЗ ржкрзБрж░рзЛ input sequence ржерзЗржХрзЗ ржкрзНрж░рзЯрзЛржЬржирзАрзЯ ржЕржВрж╢ ржмрзЗржЫрзЗ ржирзЗржУрзЯрж╛рж░ ржХрзНрж╖ржорждрж╛ ржжрзЗрзЯтАФржПрждрзЗ context bottleneck ржнрзЗржЩрзЗ ржпрж╛рзЯ ржПржмржВ alignment рждрзИрж░рж┐ рж╣рзЯред**

---


