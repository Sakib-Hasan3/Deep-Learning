---

## ЁЯза In-depth Intuition of EncoderтАУDecoder (Seq2Seq) тАФ рж╕рж╣ржЬ ржХрж┐ржирзНрждрзБ ржЧржнрзАрж░ржнрж╛ржмрзЗ

![Image](https://miro.medium.com/v2/resize%3Afit%3A1400/1%2A1JcHGUU7rFgtXC_mydUA_Q.jpeg)

![Image](https://www.scaler.com/topics/images/encoder-decoder-rnn.webp)

![Image](https://miro.medium.com/v2/resize%3Afit%3A1400/0%2A_WwXiu8vgB5S-0Pr)

![Image](https://lena-voita.github.io/resources/lectures/seq2seq/general/enc_dec_linear_out-min.png)

---

## 1я╕ПтГг рж╕ржорж╕рзНржпрж╛ ржХрзА рж╕ржорж╛ржзрж╛ржи ржХрж░рждрзЗ ржЪрж╛рзЯ Seq2Seq?

ржЕржирзЗржХ ржХрж╛ржЬрзЗржЗ **input ржУ output-ржПрж░ ржжрзИрж░рзНржШрзНржп ржПржХ ржирзЯ**:

* Machine Translation
  *тАЬI love youтАЭ* тЖТ *тАЬржЖржорж┐ рждрзЛржорж╛ржХрзЗ ржнрж╛рж▓рзЛржмрж╛рж╕рж┐тАЭ*
* Speech тЖТ Text
* Text тЖТ Summary
* Question тЖТ Answer

ЁЯСЙ Traditional ANN/CNN ржПржЦрж╛ржирзЗ ржХрж╛ржЬ ржХрж░рзЗ ржирж╛ред
ЁЯСЙ Seq2Seq ржПржЗ mismatch-ржЯрж╛ржЗ рж╕ржорж╛ржзрж╛ржи ржХрж░рзЗред

---

## 2я╕ПтГг Big Picture (ржПржХ рж▓рж╛ржЗржирзЗрж░ mental model)

**Encoder = ржмрзЛржЭрзЗ (understand)**
**Decoder = ржмрж▓рзЗ (generate)**

Encoder ржкрзБрж░рзЛ input ржкрзЬрзЗ **тАЬржорж╛ржирзЗтАЭ рждрзИрж░рж┐ ржХрж░рзЗ**,
Decoder рж╕рзЗржЗ ржорж╛ржирзЗ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ output ржмрж╛ржирж╛рзЯред

---

## 3я╕ПтГг Encoder: тАЬржмрзЛржЭрж╛рж░ ржорзЗрж╢рж┐ржитАЭ ЁЯзй

### Encoder ржХрзА ржХрж░рзЗ?

Encoder ржПржХржЯрж┐ RNN/LSTM/GRU ржпрж╛ input sequence-ржЯрж╛ **ржПржХржЯрж┐ ржПржХржЯрж┐ ржХрж░рзЗ ржкрзЬрзЗ**ред


ржзрж░рж╛ ржпрж╛ржХ input:
$$
x_1, x_2, x_3, \dots, x_T
$$


Encoder ржкрзНрж░рждрж┐ржЯрж┐ step-ржП hidden state ржмрж╛ржирж╛рзЯ:
$$
h_t = f(x_t, h_{t-1})
$$


рж╢рзЗрж╖рзЗ:
$$
	ext{Context Vector } C = h_T
$$

ЁЯСЙ **ржПржЗ (C)**-рждрзЗржЗ ржкрзБрж░рзЛ input-ржПрж░ essence ржЬржорж╛ ржерж╛ржХрзЗред

---

### Encoder-ржПрж░ Intuition

* Encoder = ржорж╛ржирзБрж╖рзЗрж░ ржорждрзЛ **рж╢рзЗрж╖ ржкрж░рзНржпржирзНржд рж╢рзЛржирж╛**
* ржХржерж╛ рж╢рзЗрж╖ рж╣рж▓рзЗ ржорж╛ржерж╛рзЯ ржПржХржЯрж╛ **summary ржзрж╛рж░ржгрж╛** ржерж╛ржХрзЗ
* ржПржЗ summary-ржЯрж╛ржЗ context vector

ЁЯУМ ржЙржжрж╛рж╣рж░ржг
Sentence: *тАЬThe cat sat on the matтАЭ*
Encoder рж╢рзЗрж╖рзЗ ржмрзЛржЭрзЗ:

> тАЬржПржХржЯрж╛ ржмрж┐рзЬрж╛рж▓, ржмрж╕рж╛, ржорж╛ржжрзБрж░тАЭтАФржПржЗ overall meaning

---

## 4я╕ПтГг Context Vector: рж╕ржмржЪрзЗрзЯрзЗ ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг ржзрж╛рж░ржгрж╛ ЁЯОп

### Context Vector ржХрзА?

* Fixed-length vector
* ржкрзБрж░рзЛ input sequence-ржПрж░ compressed meaning

ЁЯУж рждрзБржорж┐ ржнрж╛ржмрждрзЗ ржкрж╛рж░рзЛ:

* Encoder тЖТ **ZIP file ржмрж╛ржирж╛рзЯ**
* Decoder тЖТ рж╕рзЗржЗ ZIP ржЦрзБрж▓рзЗ output рж▓рзЗржЦрзЗ

тЪая╕П рж╕ржорж╕рзНржпрж╛:

* Input ржЦрзБржм ржмрзЬ рж╣рж▓рзЗ тЖТ рж╕ржм рждржерзНржп ржПржЗ ржПржХ vector-ржП ржврзЛржХрж╛ржирзЛ ржХржарж┐ржи
  (ржПржЦрж╛ржи ржерзЗржХрзЗржЗ ржкрж░рзЗ **Attention** ржПрж╕рзЗржЫрзЗ)

---

## 5я╕ПтГг Decoder: тАЬржмрж▓рж╛рж░ ржорзЗрж╢рж┐ржитАЭ ЁЯЧгя╕П

### Decoder ржХрзАржнрж╛ржмрзЗ ржХрж╛ржЬ ржХрж░рзЗ?


Decoder ржЖрж░рзЗржХржЯрж╛ RNN/LSTM:

$$
s_t = g(y_{t-1}, s_{t-1}, C)
$$
$$
y_t = \text{softmax}(W s_t)
$$

ржорж╛ржирзЗ:

* ржЖржЧрзЗрж░ output
* ржЖржЧрзЗрж░ hidden
* Encoder-ржПрж░ context
  тЖТ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ **ржкрж░рзЗрж░ рж╢ржмрзНржж ржЕржирзБржорж╛ржи ржХрж░рзЗ**

---

### Decoder-ржПрж░ Intuition

* Decoder ржарж┐ржХ ржорж╛ржирзБрж╖рзЗрж░ ржорждрзЛ ржХржерж╛ ржмрж▓рзЗ:

  * ржЖржЧрзЗрж░ рж╢ржмрзНржж ржоржирзЗ рж░рж╛ржЦрзЗ
  * ржорзВрж▓ ржмржХрзНрждржмрзНржп (context) ржорж╛ржерж╛рзЯ рж░рж╛ржЦрзЗ
  * ржзрж╛ржкрзЗ ржзрж╛ржкрзЗ ржмрж╛ржХрзНржп ржмрж╛ржирж╛рзЯ

ЁЯУМ ржЙржжрж╛рж╣рж░ржг (Translation):

* Step 1: тАЬржЖржорж┐тАЭ
* Step 2: тАЬржЖржорж┐ рждрзЛржорж╛ржХрзЗтАЭ
* Step 3: тАЬржЖржорж┐ рждрзЛржорж╛ржХрзЗ ржнрж╛рж▓рзЛржмрж╛рж╕рж┐тАЭ

---

## 6я╕ПтГг Start & End Token тАФ ржХрзЗржи ржжрж░ржХрж╛рж░?

Decoder ржЬрж╛ржиржмрзЗ ржХржЦржи рж╢рзБрж░рзБ/рж╢рзЗрж╖ ржХрж░ржмрзЗ ржХрзАржнрж╛ржмрзЗ?

* `<SOS>` = Start of Sentence
* `<EOS>` = End of Sentence

Decoder:

* `<SOS>` ржжрж┐рзЯрзЗ рж╢рзБрж░рзБ
* `<EOS>` ржП ржерж╛ржорзЗ

ЁЯСЙ ржПржХржжржо ржорж╛ржирзБрж╖рзЗрж░ ржмрж╛ржХрзНржп ржмрж▓рж╛рж░ ржорждрзЛред

---

## 7я╕ПтГг Training vs Inference (ржЦрзБржм ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг)

### ЁЯФ╣ Training (Teacher Forcing)

* ржЖрж╕рж▓ target рж╢ржмрзНржж decoder-ржХрзЗ ржжрзЗржУрзЯрж╛ рж╣рзЯ
* Fast & stable learning

### ЁЯФ╣ Inference (Real use)

* Decoder ржирж┐ржЬрзЗрж░ ржЖржЧрзЗрж░ prediction-ржЯрж╛ржЗ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ
* Error accumulate рж╣рждрзЗ ржкрж╛рж░рзЗ

---

## 8я╕ПтГг ржХрзЗржи Simple Seq2Seq-ржП рж╕ржорж╕рзНржпрж╛ рж╣рзЯ?

### тЭМ Bottleneck Problem

* ржкрзБрж░рзЛ input тЖТ ржПржХржЯрж╛ржЗ context vector
* Long sentence тЖТ рждржерзНржп рж╣рж╛рж░рж╛рзЯ

### тЭМ Long-range dependency

* рж╢рзБрж░рзБ ржжрж┐ржХрзЗрж░ рж╢ржмрзНржж ржнрзБрж▓рзЗ ржпрж╛рзЯ

ЁЯСЙ рж╕ржорж╛ржзрж╛ржи:

* **Attention Mechanism**
* **Transformer**

---

## 9я╕ПтГг EncoderтАУDecoder ржХрзЗ ржПржХрж╕рж╛ржерзЗ ржХрж▓рзНржкржирж╛ ржХрж░рж▓рзЗ

```
Input sentence тФАтФАтЦ║ [ Encoder ]
                     тФВ
                     тЦ╝
               Context Vector
                     тФВ
                     тЦ╝
Output sentence тЧДтФАтФА [ Decoder ]
```

Encoder = **reader**
Decoder = **writer**
Context = **understanding**

---

## 10я╕ПтГг ржПржХ рж▓рж╛ржЗржирзЗрж░ Takeaway

**Seq2Seq EncoderтАУDecoder ржПржоржи ржПржХржЯрж┐ ржХрж╛ржарж╛ржорзЛ ржпрзЗржЦрж╛ржирзЗ ржПржХржЯрж┐ ржирзЗржЯржУрзЯрж╛рж░рзНржХ тАЬржмрзЛржЭрзЗтАЭ ржЖрж░рзЗржХржЯрж┐ ржирзЗржЯржУрзЯрж╛рж░рзНржХ рж╕рзЗржЗ ржмрзЛржЭрж╛ржкрзЬрж╛ ржерзЗржХрзЗ тАЬржмрж▓рзЗтАЭтАФржПржмржВ ржПржЗ ржзрж╛рж░ржгрж╛ржЗ ржЖржзрзБржирж┐ржХ NLP-ржПрж░ ржнрж┐рждрзНрждрж┐ред**

---

