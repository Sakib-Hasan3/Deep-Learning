---

## ЁЯФ┤ Problems With EncoderтАУDecoder Architecture (In-depth & Easy)

![Image](https://raw.githubusercontent.com/JingchaoZhang/JingchaoZhang.github.io/master/images/cs224n-lecture8/2.png)

![Image](https://miro.medium.com/1%2Aa_wG1k0PZwxRRSsX0D8pXw.png)

![Image](https://miro.medium.com/0%2AJ52rIrSrdXtsoe51.png)

![Image](https://miro.medium.com/1%2AIwYX_a5neui46OLQCLSJdw.jpeg)

---

## 1я╕ПтГг Fixed-Length Context Vector Bottleneck (рж╕ржмржЪрзЗрзЯрзЗ ржмрзЬ рж╕ржорж╕рзНржпрж╛)

### ЁЯФН рж╕ржорж╕рзНржпрж╛ ржХрзА?

Encoder ржкрзБрж░рзЛ input sequence-ржХрзЗ **ржПржХржЯрж╛ fixed-size context vector (C)**-ржП compress ржХрж░рзЗред

ЁЯСЙ Input ржЫрзЛржЯ рж╣рж▓рзЗ ржарж┐ржХ ржЖржЫрзЗ
ЁЯСЙ Input ржмрзЬ рж╣рж▓рзЗ тЖТ **рж╕ржм рждржерзНржп ржзрж░рзЗ рж░рж╛ржЦрж╛ ржЕрж╕ржорзНржнржм**

---

### ЁЯза ржХрзЗржи рж╣рзЯ?

$$
x_1, x_2, \dots, x_T \;\xrightarrow{\text{Encoder}}\; C \;\xrightarrow{\text{Decoder}}\; y_1, y_2, \dots
$$

* ржпржд ржмрзЬ sentence
* рждржд ржмрзЗрж╢рж┐ рждржерзНржп
* ржХрж┐ржирзНрждрзБ vector size ржПржХржЗ

ЁЯУж ржпрзЗржи:

> ржПржХржЯрж╛ ржкрзБрж░рзЛ ржмржЗ ржПржХ рж▓рж╛ржЗржирзЗ рж╕ржВржХрзНрж╖рзЗржк ржХрж░рж╛

---

### тЭМ ржлрж▓рж╛ржлрж▓

* Long sentence тЖТ ржнрзБрж▓ output
* Translation quality drop
* Early words ржнрзБрж▓рзЗ ржпрж╛рзЯ

---

## 2я╕ПтГг Long-Range Dependency Problem

### ЁЯФН рж╕ржорж╕рзНржпрж╛ ржХрзА?

Encoder рж╢рзБрж░рзБ ржжрж┐ржХрзЗрж░ рж╢ржмрзНржжржЧрзБрж▓рзЛрж░ ржкрзНрж░ржнрж╛ржм **decoder ржкрж░рзНржпржирзНржд ржарж┐ржХржнрж╛ржмрзЗ ржкрзМржБржЫрж╛рзЯ ржирж╛**ред

---

### ЁЯза ржХрзЗржи?

* RNN/LSTM sequential
* Gradient decay рж╣рзЯ (vanishing gradient)
* Context vector рж╢рзЗрж╖ hidden-ржПрж░ ржЙржкрж░ ржирж┐рж░рзНржнрж░рж╢рзАрж▓

---

### ЁЯУМ ржЙржжрж╛рж╣рж░ржг

> *тАЬThe book that you gave me yesterday тАж is very interestingтАЭ*

Decoder рж╢рзЗрж╖рзЗрж░ ржжрж┐ржХрзЗ ржПрж╕рзЗ **тАЬbookтАЭ** ржнрзБрж▓рзЗ ржпрзЗрждрзЗ ржкрж╛рж░рзЗред

---

## 3я╕ПтГг Information Loss (Compression Error)

### ЁЯФН рж╕ржорж╕рзНржпрж╛ ржХрзА?

Encoder рж╕ржм hidden state ржлрзЗрж▓рзЗ ржжрж┐рзЯрзЗ **рж╢рзБржзрзБ рж╢рзЗрж╖ржЯрж╛ рж░рж╛ржЦрзЗ**ред

---

### ЁЯза Intuition

Encoder-ржПрж░ hidden states:

$$
h_1, h_2, h_3, \dots, h_T
$$

ржХрж┐ржирзНрждрзБ decoder ржкрж╛рзЯ:

$$
C = h_T
$$

ЁЯСЙ ржорж╛ржЭржЦрж╛ржирзЗрж░ valuable information рж╣рж╛рж░рж┐рзЯрзЗ ржпрж╛рзЯред

---

### тЭМ ржлрж▓рж╛ржлрж▓

* Word alignment ржЦрж╛рж░рж╛ржк
* Translation unnatural рж╣рзЯ

---

## 4я╕ПтГг Exposure Bias (Training vs Inference Gap)

### ЁЯФН рж╕ржорж╕рзНржпрж╛ ржХрзА?

Training ржЖрж░ real-use (inference)-ржПрж░ рж╕ржорзЯ decoder **ржнрж┐ржирзНржиржнрж╛ржмрзЗ ржХрж╛ржЬ ржХрж░рзЗ**ред

---

### ЁЯза ржХрзЗржи?

* **Training (Teacher Forcing)**:
  decoder рж╕ржмрж╕ржорзЯ **true previous word** ржкрж╛рзЯ
* **Inference**:
  decoder ржирж┐ржЬрзЗрж░ ржЖржЧрзЗрж░ prediction ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ

---

### тЭМ ржлрж▓рж╛ржлрж▓

* ржПржХржЯрж╛ ржнрзБрж▓ рж╣рж▓рзЗржЗ
* ржкрж░рзЗрж░ рж╕ржм рж╢ржмрзНржжрзЗ error ржЬржорждрзЗ ржерж╛ржХрзЗ

ЁЯУМ ржпрзЗржи:

> ржкрж░рзАржХрзНрж╖рж╛рзЯ рж╕ржмрж╕ржорзЯ hint ржжрж┐рзЯрзЗ ржкрзЬрж╛ржирзЛ,
> ржХрж┐ржирзНрждрзБ ржкрж░рзАржХрзНрж╖рж╛рж░ рж╣рж▓рзЗ hint ржирзЗржЗ

---

## 5я╕ПтГг Error Accumulation in Decoder

### ЁЯФН рж╕ржорж╕рзНржпрж╛ ржХрзА?

Decoder ржПржХржмрж╛рж░ ржнрзБрж▓ ржХрж░рж▓рзЗ, рж╕рзЗржЗ ржнрзБрж▓ржЯрж╛ржЗ ржкрж░рзЗрж░ step-ржП input рж╣рзЯред

---

### тЭМ ржлрж▓рж╛ржлрж▓

* Sentence drift
* Meaning рж╕ржорзНржкрзВрж░рзНржг ржмржжрж▓рзЗ ржпрж╛рзЯ
* EOS ржнрзБрж▓ ржЬрж╛рзЯржЧрж╛рзЯ ржЖрж╕рзЗ

---

## 6я╕ПтГг Sequential Computation (Slow Training)

### ЁЯФН рж╕ржорж╕рзНржпрж╛ ржХрзА?

Encoder ржПржмржВ DecoderтАФржжрзБржЯрзЛржЗ **sequential**ред

---

### ЁЯза ржХрзЗржи?


$$
h_t \text{ depends on } h_{t-1}
$$

ЁЯСЙ Parallel ржХрж░рж╛ ржпрж╛рзЯ ржирж╛

---

### тЭМ ржлрж▓рж╛ржлрж▓

* Training ржзрзАрж░
* Long sentence тЖТ latency ржмрзЗрж╢рж┐
* GPU underutilized

---

## 7я╕ПтГг Difficulty With Very Long Sequences

### ЁЯФН рж╕ржорж╕рзНржпрж╛ ржХрзА?

Long paragraph, long audio, document-level task-ржП Seq2Seq ржнрж╛рж▓рзЛ ржХрж╛ржЬ ржХрж░рзЗ ржирж╛ред

---

### тЭМ ржХрж╛рж░ржг

* Context bottleneck
* Memory limit
* Gradient issues

---

## 8я╕ПтГг Alignment Problem (Who attends to whom?)

### ЁЯФН рж╕ржорж╕рзНржпрж╛ ржХрзА?

Decoder ржЬрж╛ржирзЗ ржирж╛:

* ржХрзЛржи output word
* input-ржПрж░ ржХрзЛржи ржЕржВрж╢рзЗрж░ рж╕рж╛ржерзЗ рж╕ржорзНржкрж░рзНржХрж┐ржд

---

### ЁЯУМ ржЙржжрж╛рж╣рж░ржг (Translation)

English тЖТ Bangla
рж╢ржмрзНржжржЧрзБрж▓рзЛрж░ order ржЖрж▓рж╛ржжрж╛, ржХрж┐ржирзНрждрзБ vanilla Seq2Seq-ржП **alignment ржирзЗржЗ**ред

---

## 9я╕ПтГг Hard to Interpret (Black Box)

### ЁЯФН рж╕ржорж╕рзНржпрж╛ ржХрзА?

* Context vector ржХрзА ржзрж╛рж░ржг ржХрж░ржЫрзЗ ржмрзЛржЭрж╛ ржпрж╛рзЯ ржирж╛
* ржХрзЛржи input word output-ржП ржкрзНрж░ржнрж╛ржм ржлрзЗрж▓рзЗржЫрзЗ unclear

---

## ЁЯФз ржПрж╕ржм рж╕ржорж╕рзНржпрж╛рж░ рж╕ржорж╛ржзрж╛ржи ржХрзА?

| Problem            | Solution                |
| ------------------ | ----------------------- |
| Context bottleneck | **Attention mechanism** |
| Long dependency    | Attention, BiLSTM       |
| Alignment          | Attention               |
| Exposure bias      | Scheduled sampling      |
| Speed              | **Transformer**         |
| Interpretability   | Attention weights       |

ЁЯСЙ ржПржЗ рж╕ржорж╕рзНржпрж╛ржЧрзБрж▓рзЛ ржерзЗржХрзЗржЗ ржПрж╕рзЗржЫрзЗ:

* **Attention-based Seq2Seq**
* **Transformer architecture**

---

## ЁЯза ржПржХ рж▓рж╛ржЗржирзЗрж░ Takeaway

**Vanilla EncoderтАУDecoder рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзА ржзрж╛рж░ржгрж╛ рж╣рж▓рзЗржУ, fixed context vector ржУ sequential nature-ржПрж░ ржХрж╛рж░ржгрзЗ long sequence ржУ real-world task-ржП рж╕рзАржорж╛ржмржжрзНржзтАФржПржЗ рж╕рзАржорж╛ржмржжрзНржзрждрж╛ржЗ Attention ржУ Transformer-ржПрж░ ржЬржирзНржо ржжрж┐рзЯрзЗржЫрзЗред**

---


