<<<<<<< HEAD
---

## ğŸ”´ Problems With Encoderâ€“Decoder Architecture (In-depth & Easy)

![Image](https://raw.githubusercontent.com/JingchaoZhang/JingchaoZhang.github.io/master/images/cs224n-lecture8/2.png)

![Image](https://miro.medium.com/1%2Aa_wG1k0PZwxRRSsX0D8pXw.png)

![Image](https://miro.medium.com/0%2AJ52rIrSrdXtsoe51.png)

![Image](https://miro.medium.com/1%2AIwYX_a5neui46OLQCLSJdw.jpeg)

---

## 1ï¸âƒ£ Fixed-Length Context Vector Bottleneck (à¦¸à¦¬à¦šà§‡à§Ÿà§‡ à¦¬à§œ à¦¸à¦®à¦¸à§à¦¯à¦¾)

### ğŸ” à¦¸à¦®à¦¸à§à¦¯à¦¾ à¦•à§€?

Encoder à¦ªà§à¦°à§‹ input sequence-à¦•à§‡ **à¦à¦•à¦Ÿà¦¾ fixed-size context vector (C)**-à¦ compress à¦•à¦°à§‡à¥¤

ğŸ‘‰ Input à¦›à§‹à¦Ÿ à¦¹à¦²à§‡ à¦ à¦¿à¦• à¦†à¦›à§‡
ğŸ‘‰ Input à¦¬à§œ à¦¹à¦²à§‡ â†’ **à¦¸à¦¬ à¦¤à¦¥à§à¦¯ à¦§à¦°à§‡ à¦°à¦¾à¦–à¦¾ à¦…à¦¸à¦®à§à¦­à¦¬**

---

### ğŸ§  à¦•à§‡à¦¨ à¦¹à§Ÿ?

$$
x_1, x_2, \dots, x_T \;\xrightarrow{\text{Encoder}}\; C \;\xrightarrow{\text{Decoder}}\; y_1, y_2, \dots
$$

* à¦¯à¦¤ à¦¬à§œ sentence
* à¦¤à¦¤ à¦¬à§‡à¦¶à¦¿ à¦¤à¦¥à§à¦¯
* à¦•à¦¿à¦¨à§à¦¤à§ vector size à¦à¦•à¦‡

ğŸ“¦ à¦¯à§‡à¦¨:

> à¦à¦•à¦Ÿà¦¾ à¦ªà§à¦°à§‹ à¦¬à¦‡ à¦à¦• à¦²à¦¾à¦‡à¦¨à§‡ à¦¸à¦‚à¦•à§à¦·à§‡à¦ª à¦•à¦°à¦¾

---

### âŒ à¦«à¦²à¦¾à¦«à¦²

* Long sentence â†’ à¦­à§à¦² output
* Translation quality drop
* Early words à¦­à§à¦²à§‡ à¦¯à¦¾à§Ÿ

---

## 2ï¸âƒ£ Long-Range Dependency Problem

### ğŸ” à¦¸à¦®à¦¸à§à¦¯à¦¾ à¦•à§€?

Encoder à¦¶à§à¦°à§ à¦¦à¦¿à¦•à§‡à¦° à¦¶à¦¬à§à¦¦à¦—à§à¦²à§‹à¦° à¦ªà§à¦°à¦­à¦¾à¦¬ **decoder à¦ªà¦°à§à¦¯à¦¨à§à¦¤ à¦ à¦¿à¦•à¦­à¦¾à¦¬à§‡ à¦ªà§Œà¦à¦›à¦¾à§Ÿ à¦¨à¦¾**à¥¤

---

### ğŸ§  à¦•à§‡à¦¨?

* RNN/LSTM sequential
* Gradient decay à¦¹à§Ÿ (vanishing gradient)
* Context vector à¦¶à§‡à¦· hidden-à¦à¦° à¦‰à¦ªà¦° à¦¨à¦¿à¦°à§à¦­à¦°à¦¶à§€à¦²

---

### ğŸ“Œ à¦‰à¦¦à¦¾à¦¹à¦°à¦£

> *â€œThe book that you gave me yesterday â€¦ is very interestingâ€*

Decoder à¦¶à§‡à¦·à§‡à¦° à¦¦à¦¿à¦•à§‡ à¦à¦¸à§‡ **â€œbookâ€** à¦­à§à¦²à§‡ à¦¯à§‡à¦¤à§‡ à¦ªà¦¾à¦°à§‡à¥¤

---

## 3ï¸âƒ£ Information Loss (Compression Error)

### ğŸ” à¦¸à¦®à¦¸à§à¦¯à¦¾ à¦•à§€?

Encoder à¦¸à¦¬ hidden state à¦«à§‡à¦²à§‡ à¦¦à¦¿à§Ÿà§‡ **à¦¶à§à¦§à§ à¦¶à§‡à¦·à¦Ÿà¦¾ à¦°à¦¾à¦–à§‡**à¥¤

---

### ğŸ§  Intuition

Encoder-à¦à¦° hidden states:

$$
h_1, h_2, h_3, \dots, h_T
$$

à¦•à¦¿à¦¨à§à¦¤à§ decoder à¦ªà¦¾à§Ÿ:

$$
C = h_T
$$

ğŸ‘‰ à¦®à¦¾à¦à¦–à¦¾à¦¨à§‡à¦° valuable information à¦¹à¦¾à¦°à¦¿à§Ÿà§‡ à¦¯à¦¾à§Ÿà¥¤

---

### âŒ à¦«à¦²à¦¾à¦«à¦²

* Word alignment à¦–à¦¾à¦°à¦¾à¦ª
* Translation unnatural à¦¹à§Ÿ

---

## 4ï¸âƒ£ Exposure Bias (Training vs Inference Gap)

### ğŸ” à¦¸à¦®à¦¸à§à¦¯à¦¾ à¦•à§€?

Training à¦†à¦° real-use (inference)-à¦à¦° à¦¸à¦®à§Ÿ decoder **à¦­à¦¿à¦¨à§à¦¨à¦­à¦¾à¦¬à§‡ à¦•à¦¾à¦œ à¦•à¦°à§‡**à¥¤

---

### ğŸ§  à¦•à§‡à¦¨?

* **Training (Teacher Forcing)**:
  decoder à¦¸à¦¬à¦¸à¦®à§Ÿ **true previous word** à¦ªà¦¾à§Ÿ
* **Inference**:
  decoder à¦¨à¦¿à¦œà§‡à¦° à¦†à¦—à§‡à¦° prediction à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§‡

---

### âŒ à¦«à¦²à¦¾à¦«à¦²

* à¦à¦•à¦Ÿà¦¾ à¦­à§à¦² à¦¹à¦²à§‡à¦‡
* à¦ªà¦°à§‡à¦° à¦¸à¦¬ à¦¶à¦¬à§à¦¦à§‡ error à¦œà¦®à¦¤à§‡ à¦¥à¦¾à¦•à§‡

ğŸ“Œ à¦¯à§‡à¦¨:

> à¦ªà¦°à§€à¦•à§à¦·à¦¾à§Ÿ à¦¸à¦¬à¦¸à¦®à§Ÿ hint à¦¦à¦¿à§Ÿà§‡ à¦ªà§œà¦¾à¦¨à§‹,
> à¦•à¦¿à¦¨à§à¦¤à§ à¦ªà¦°à§€à¦•à§à¦·à¦¾à¦° à¦¹à¦²à§‡ hint à¦¨à§‡à¦‡

---

## 5ï¸âƒ£ Error Accumulation in Decoder

### ğŸ” à¦¸à¦®à¦¸à§à¦¯à¦¾ à¦•à§€?

Decoder à¦à¦•à¦¬à¦¾à¦° à¦­à§à¦² à¦•à¦°à¦²à§‡, à¦¸à§‡à¦‡ à¦­à§à¦²à¦Ÿà¦¾à¦‡ à¦ªà¦°à§‡à¦° step-à¦ input à¦¹à§Ÿà¥¤

---

### âŒ à¦«à¦²à¦¾à¦«à¦²

* Sentence drift
* Meaning à¦¸à¦®à§à¦ªà§‚à¦°à§à¦£ à¦¬à¦¦à¦²à§‡ à¦¯à¦¾à§Ÿ
* EOS à¦­à§à¦² à¦œà¦¾à§Ÿà¦—à¦¾à§Ÿ à¦†à¦¸à§‡

---

## 6ï¸âƒ£ Sequential Computation (Slow Training)

### ğŸ” à¦¸à¦®à¦¸à§à¦¯à¦¾ à¦•à§€?

Encoder à¦à¦¬à¦‚ Decoderâ€”à¦¦à§à¦Ÿà§‹à¦‡ **sequential**à¥¤

---

### ğŸ§  à¦•à§‡à¦¨?


$$
h_t \text{ depends on } h_{t-1}
$$

ğŸ‘‰ Parallel à¦•à¦°à¦¾ à¦¯à¦¾à§Ÿ à¦¨à¦¾

---

### âŒ à¦«à¦²à¦¾à¦«à¦²

* Training à¦§à§€à¦°
* Long sentence â†’ latency à¦¬à§‡à¦¶à¦¿
* GPU underutilized

---

## 7ï¸âƒ£ Difficulty With Very Long Sequences

### ğŸ” à¦¸à¦®à¦¸à§à¦¯à¦¾ à¦•à§€?

Long paragraph, long audio, document-level task-à¦ Seq2Seq à¦­à¦¾à¦²à§‹ à¦•à¦¾à¦œ à¦•à¦°à§‡ à¦¨à¦¾à¥¤

---

### âŒ à¦•à¦¾à¦°à¦£

* Context bottleneck
* Memory limit
* Gradient issues

---

## 8ï¸âƒ£ Alignment Problem (Who attends to whom?)

### ğŸ” à¦¸à¦®à¦¸à§à¦¯à¦¾ à¦•à§€?

Decoder à¦œà¦¾à¦¨à§‡ à¦¨à¦¾:

* à¦•à§‹à¦¨ output word
* input-à¦à¦° à¦•à§‹à¦¨ à¦…à¦‚à¦¶à§‡à¦° à¦¸à¦¾à¦¥à§‡ à¦¸à¦®à§à¦ªà¦°à§à¦•à¦¿à¦¤

---

### ğŸ“Œ à¦‰à¦¦à¦¾à¦¹à¦°à¦£ (Translation)

English â†’ Bangla
à¦¶à¦¬à§à¦¦à¦—à§à¦²à§‹à¦° order à¦†à¦²à¦¾à¦¦à¦¾, à¦•à¦¿à¦¨à§à¦¤à§ vanilla Seq2Seq-à¦ **alignment à¦¨à§‡à¦‡**à¥¤

---

## 9ï¸âƒ£ Hard to Interpret (Black Box)

### ğŸ” à¦¸à¦®à¦¸à§à¦¯à¦¾ à¦•à§€?

* Context vector à¦•à§€ à¦§à¦¾à¦°à¦£ à¦•à¦°à¦›à§‡ à¦¬à§‹à¦à¦¾ à¦¯à¦¾à§Ÿ à¦¨à¦¾
* à¦•à§‹à¦¨ input word output-à¦ à¦ªà§à¦°à¦­à¦¾à¦¬ à¦«à§‡à¦²à§‡à¦›à§‡ unclear

---

## ğŸ”§ à¦à¦¸à¦¬ à¦¸à¦®à¦¸à§à¦¯à¦¾à¦° à¦¸à¦®à¦¾à¦§à¦¾à¦¨ à¦•à§€?

| Problem            | Solution                |
| ------------------ | ----------------------- |
| Context bottleneck | **Attention mechanism** |
| Long dependency    | Attention, BiLSTM       |
| Alignment          | Attention               |
| Exposure bias      | Scheduled sampling      |
| Speed              | **Transformer**         |
| Interpretability   | Attention weights       |

ğŸ‘‰ à¦à¦‡ à¦¸à¦®à¦¸à§à¦¯à¦¾à¦—à§à¦²à§‹ à¦¥à§‡à¦•à§‡à¦‡ à¦à¦¸à§‡à¦›à§‡:

* **Attention-based Seq2Seq**
* **Transformer architecture**

---

## ğŸ§  à¦à¦• à¦²à¦¾à¦‡à¦¨à§‡à¦° Takeaway

**Vanilla Encoderâ€“Decoder à¦¶à¦•à§à¦¤à¦¿à¦¶à¦¾à¦²à§€ à¦§à¦¾à¦°à¦£à¦¾ à¦¹à¦²à§‡à¦“, fixed context vector à¦“ sequential nature-à¦à¦° à¦•à¦¾à¦°à¦£à§‡ long sequence à¦“ real-world task-à¦ à¦¸à§€à¦®à¦¾à¦¬à¦¦à§à¦§â€”à¦à¦‡ à¦¸à§€à¦®à¦¾à¦¬à¦¦à§à¦§à¦¤à¦¾à¦‡ Attention à¦“ Transformer-à¦à¦° à¦œà¦¨à§à¦® à¦¦à¦¿à§Ÿà§‡à¦›à§‡à¥¤**

---


=======
---

## ğŸ”´ Problems With Encoderâ€“Decoder Architecture (In-depth & Easy)

![Image](https://raw.githubusercontent.com/JingchaoZhang/JingchaoZhang.github.io/master/images/cs224n-lecture8/2.png)

![Image](https://miro.medium.com/1%2Aa_wG1k0PZwxRRSsX0D8pXw.png)

![Image](https://miro.medium.com/0%2AJ52rIrSrdXtsoe51.png)

![Image](https://miro.medium.com/1%2AIwYX_a5neui46OLQCLSJdw.jpeg)

---

## 1ï¸âƒ£ Fixed-Length Context Vector Bottleneck (à¦¸à¦¬à¦šà§‡à§Ÿà§‡ à¦¬à§œ à¦¸à¦®à¦¸à§à¦¯à¦¾)

### ğŸ” à¦¸à¦®à¦¸à§à¦¯à¦¾ à¦•à§€?

Encoder à¦ªà§à¦°à§‹ input sequence-à¦•à§‡ **à¦à¦•à¦Ÿà¦¾ fixed-size context vector (C)**-à¦ compress à¦•à¦°à§‡à¥¤

ğŸ‘‰ Input à¦›à§‹à¦Ÿ à¦¹à¦²à§‡ à¦ à¦¿à¦• à¦†à¦›à§‡
ğŸ‘‰ Input à¦¬à§œ à¦¹à¦²à§‡ â†’ **à¦¸à¦¬ à¦¤à¦¥à§à¦¯ à¦§à¦°à§‡ à¦°à¦¾à¦–à¦¾ à¦…à¦¸à¦®à§à¦­à¦¬**

---

### ğŸ§  à¦•à§‡à¦¨ à¦¹à§Ÿ?

$$
x_1, x_2, \dots, x_T \;\xrightarrow{\text{Encoder}}\; C \;\xrightarrow{\text{Decoder}}\; y_1, y_2, \dots
$$

* à¦¯à¦¤ à¦¬à§œ sentence
* à¦¤à¦¤ à¦¬à§‡à¦¶à¦¿ à¦¤à¦¥à§à¦¯
* à¦•à¦¿à¦¨à§à¦¤à§ vector size à¦à¦•à¦‡

ğŸ“¦ à¦¯à§‡à¦¨:

> à¦à¦•à¦Ÿà¦¾ à¦ªà§à¦°à§‹ à¦¬à¦‡ à¦à¦• à¦²à¦¾à¦‡à¦¨à§‡ à¦¸à¦‚à¦•à§à¦·à§‡à¦ª à¦•à¦°à¦¾

---

### âŒ à¦«à¦²à¦¾à¦«à¦²

* Long sentence â†’ à¦­à§à¦² output
* Translation quality drop
* Early words à¦­à§à¦²à§‡ à¦¯à¦¾à§Ÿ

---

## 2ï¸âƒ£ Long-Range Dependency Problem

### ğŸ” à¦¸à¦®à¦¸à§à¦¯à¦¾ à¦•à§€?

Encoder à¦¶à§à¦°à§ à¦¦à¦¿à¦•à§‡à¦° à¦¶à¦¬à§à¦¦à¦—à§à¦²à§‹à¦° à¦ªà§à¦°à¦­à¦¾à¦¬ **decoder à¦ªà¦°à§à¦¯à¦¨à§à¦¤ à¦ à¦¿à¦•à¦­à¦¾à¦¬à§‡ à¦ªà§Œà¦à¦›à¦¾à§Ÿ à¦¨à¦¾**à¥¤

---

### ğŸ§  à¦•à§‡à¦¨?

* RNN/LSTM sequential
* Gradient decay à¦¹à§Ÿ (vanishing gradient)
* Context vector à¦¶à§‡à¦· hidden-à¦à¦° à¦‰à¦ªà¦° à¦¨à¦¿à¦°à§à¦­à¦°à¦¶à§€à¦²

---

### ğŸ“Œ à¦‰à¦¦à¦¾à¦¹à¦°à¦£

> *â€œThe book that you gave me yesterday â€¦ is very interestingâ€*

Decoder à¦¶à§‡à¦·à§‡à¦° à¦¦à¦¿à¦•à§‡ à¦à¦¸à§‡ **â€œbookâ€** à¦­à§à¦²à§‡ à¦¯à§‡à¦¤à§‡ à¦ªà¦¾à¦°à§‡à¥¤

---

## 3ï¸âƒ£ Information Loss (Compression Error)

### ğŸ” à¦¸à¦®à¦¸à§à¦¯à¦¾ à¦•à§€?

Encoder à¦¸à¦¬ hidden state à¦«à§‡à¦²à§‡ à¦¦à¦¿à§Ÿà§‡ **à¦¶à§à¦§à§ à¦¶à§‡à¦·à¦Ÿà¦¾ à¦°à¦¾à¦–à§‡**à¥¤

---

### ğŸ§  Intuition

Encoder-à¦à¦° hidden states:

$$
h_1, h_2, h_3, \dots, h_T
$$

à¦•à¦¿à¦¨à§à¦¤à§ decoder à¦ªà¦¾à§Ÿ:

$$
C = h_T
$$

ğŸ‘‰ à¦®à¦¾à¦à¦–à¦¾à¦¨à§‡à¦° valuable information à¦¹à¦¾à¦°à¦¿à§Ÿà§‡ à¦¯à¦¾à§Ÿà¥¤

---

### âŒ à¦«à¦²à¦¾à¦«à¦²

* Word alignment à¦–à¦¾à¦°à¦¾à¦ª
* Translation unnatural à¦¹à§Ÿ

---

## 4ï¸âƒ£ Exposure Bias (Training vs Inference Gap)

### ğŸ” à¦¸à¦®à¦¸à§à¦¯à¦¾ à¦•à§€?

Training à¦†à¦° real-use (inference)-à¦à¦° à¦¸à¦®à§Ÿ decoder **à¦­à¦¿à¦¨à§à¦¨à¦­à¦¾à¦¬à§‡ à¦•à¦¾à¦œ à¦•à¦°à§‡**à¥¤

---

### ğŸ§  à¦•à§‡à¦¨?

* **Training (Teacher Forcing)**:
  decoder à¦¸à¦¬à¦¸à¦®à§Ÿ **true previous word** à¦ªà¦¾à§Ÿ
* **Inference**:
  decoder à¦¨à¦¿à¦œà§‡à¦° à¦†à¦—à§‡à¦° prediction à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§‡

---

### âŒ à¦«à¦²à¦¾à¦«à¦²

* à¦à¦•à¦Ÿà¦¾ à¦­à§à¦² à¦¹à¦²à§‡à¦‡
* à¦ªà¦°à§‡à¦° à¦¸à¦¬ à¦¶à¦¬à§à¦¦à§‡ error à¦œà¦®à¦¤à§‡ à¦¥à¦¾à¦•à§‡

ğŸ“Œ à¦¯à§‡à¦¨:

> à¦ªà¦°à§€à¦•à§à¦·à¦¾à§Ÿ à¦¸à¦¬à¦¸à¦®à§Ÿ hint à¦¦à¦¿à§Ÿà§‡ à¦ªà§œà¦¾à¦¨à§‹,
> à¦•à¦¿à¦¨à§à¦¤à§ à¦ªà¦°à§€à¦•à§à¦·à¦¾à¦° à¦¹à¦²à§‡ hint à¦¨à§‡à¦‡

---

## 5ï¸âƒ£ Error Accumulation in Decoder

### ğŸ” à¦¸à¦®à¦¸à§à¦¯à¦¾ à¦•à§€?

Decoder à¦à¦•à¦¬à¦¾à¦° à¦­à§à¦² à¦•à¦°à¦²à§‡, à¦¸à§‡à¦‡ à¦­à§à¦²à¦Ÿà¦¾à¦‡ à¦ªà¦°à§‡à¦° step-à¦ input à¦¹à§Ÿà¥¤

---

### âŒ à¦«à¦²à¦¾à¦«à¦²

* Sentence drift
* Meaning à¦¸à¦®à§à¦ªà§‚à¦°à§à¦£ à¦¬à¦¦à¦²à§‡ à¦¯à¦¾à§Ÿ
* EOS à¦­à§à¦² à¦œà¦¾à§Ÿà¦—à¦¾à§Ÿ à¦†à¦¸à§‡

---

## 6ï¸âƒ£ Sequential Computation (Slow Training)

### ğŸ” à¦¸à¦®à¦¸à§à¦¯à¦¾ à¦•à§€?

Encoder à¦à¦¬à¦‚ Decoderâ€”à¦¦à§à¦Ÿà§‹à¦‡ **sequential**à¥¤

---

### ğŸ§  à¦•à§‡à¦¨?


$$
h_t \text{ depends on } h_{t-1}
$$

ğŸ‘‰ Parallel à¦•à¦°à¦¾ à¦¯à¦¾à§Ÿ à¦¨à¦¾

---

### âŒ à¦«à¦²à¦¾à¦«à¦²

* Training à¦§à§€à¦°
* Long sentence â†’ latency à¦¬à§‡à¦¶à¦¿
* GPU underutilized

---

## 7ï¸âƒ£ Difficulty With Very Long Sequences

### ğŸ” à¦¸à¦®à¦¸à§à¦¯à¦¾ à¦•à§€?

Long paragraph, long audio, document-level task-à¦ Seq2Seq à¦­à¦¾à¦²à§‹ à¦•à¦¾à¦œ à¦•à¦°à§‡ à¦¨à¦¾à¥¤

---

### âŒ à¦•à¦¾à¦°à¦£

* Context bottleneck
* Memory limit
* Gradient issues

---

## 8ï¸âƒ£ Alignment Problem (Who attends to whom?)

### ğŸ” à¦¸à¦®à¦¸à§à¦¯à¦¾ à¦•à§€?

Decoder à¦œà¦¾à¦¨à§‡ à¦¨à¦¾:

* à¦•à§‹à¦¨ output word
* input-à¦à¦° à¦•à§‹à¦¨ à¦…à¦‚à¦¶à§‡à¦° à¦¸à¦¾à¦¥à§‡ à¦¸à¦®à§à¦ªà¦°à§à¦•à¦¿à¦¤

---

### ğŸ“Œ à¦‰à¦¦à¦¾à¦¹à¦°à¦£ (Translation)

English â†’ Bangla
à¦¶à¦¬à§à¦¦à¦—à§à¦²à§‹à¦° order à¦†à¦²à¦¾à¦¦à¦¾, à¦•à¦¿à¦¨à§à¦¤à§ vanilla Seq2Seq-à¦ **alignment à¦¨à§‡à¦‡**à¥¤

---

## 9ï¸âƒ£ Hard to Interpret (Black Box)

### ğŸ” à¦¸à¦®à¦¸à§à¦¯à¦¾ à¦•à§€?

* Context vector à¦•à§€ à¦§à¦¾à¦°à¦£ à¦•à¦°à¦›à§‡ à¦¬à§‹à¦à¦¾ à¦¯à¦¾à§Ÿ à¦¨à¦¾
* à¦•à§‹à¦¨ input word output-à¦ à¦ªà§à¦°à¦­à¦¾à¦¬ à¦«à§‡à¦²à§‡à¦›à§‡ unclear

---

## ğŸ”§ à¦à¦¸à¦¬ à¦¸à¦®à¦¸à§à¦¯à¦¾à¦° à¦¸à¦®à¦¾à¦§à¦¾à¦¨ à¦•à§€?

| Problem            | Solution                |
| ------------------ | ----------------------- |
| Context bottleneck | **Attention mechanism** |
| Long dependency    | Attention, BiLSTM       |
| Alignment          | Attention               |
| Exposure bias      | Scheduled sampling      |
| Speed              | **Transformer**         |
| Interpretability   | Attention weights       |

ğŸ‘‰ à¦à¦‡ à¦¸à¦®à¦¸à§à¦¯à¦¾à¦—à§à¦²à§‹ à¦¥à§‡à¦•à§‡à¦‡ à¦à¦¸à§‡à¦›à§‡:

* **Attention-based Seq2Seq**
* **Transformer architecture**

---

## ğŸ§  à¦à¦• à¦²à¦¾à¦‡à¦¨à§‡à¦° Takeaway

**Vanilla Encoderâ€“Decoder à¦¶à¦•à§à¦¤à¦¿à¦¶à¦¾à¦²à§€ à¦§à¦¾à¦°à¦£à¦¾ à¦¹à¦²à§‡à¦“, fixed context vector à¦“ sequential nature-à¦à¦° à¦•à¦¾à¦°à¦£à§‡ long sequence à¦“ real-world task-à¦ à¦¸à§€à¦®à¦¾à¦¬à¦¦à§à¦§â€”à¦à¦‡ à¦¸à§€à¦®à¦¾à¦¬à¦¦à§à¦§à¦¤à¦¾à¦‡ Attention à¦“ Transformer-à¦à¦° à¦œà¦¨à§à¦® à¦¦à¦¿à§Ÿà§‡à¦›à§‡à¥¤**

---


>>>>>>> f45ebbad1686e699afe9932c4175eeff501d254b
