# Self-Attention Layer ‚Äî Deep Detailed Working (Concept ‚Üí Math ‚Üí Intuition)

Self-Attention ‡¶π‡¶≤‡ßã ‡¶è‡¶Æ‡¶® ‡¶è‡¶ï‡¶ü‡¶ø mechanism ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá **‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø token ‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡ßá‡¶∞ ‡¶Ö‡¶®‡ßç‡¶Ø ‡¶∏‡¶¨ token-‡¶ï‡ßá weighted ‡¶≠‡¶æ‡¶¨‡ßá ‡¶¶‡ßá‡¶ñ‡ßá** ‡¶è‡¶¨‡¶Ç ‡¶®‡¶ø‡¶ú‡ßá‡¶∞ ‡¶®‡¶§‡ßÅ‡¶® contextual representation ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßá‡•§

---

# 1Ô∏è‚É£ Full Computational Flow

```text
Input Embeddings (X)
        ‚Üì
Linear Projection ‚Üí Q, K, V
        ‚Üì
Score = QK·µÄ
        ‚Üì
Scale by ‚àöd_k
        ‚Üì
Softmax
        ‚Üì
Weighted Sum with V
        ‚Üì
Output (Contextual Embedding)
```

---

# 2Ô∏è‚É£ Matrix-Level View

‡¶ß‡¶∞‡¶ø:

- Sequence length = $n$
- Model dimension = $d_{model}$
- Head dimension = $d_k$

$$
X \in \mathbb{R}^{n \times d_{model}}
$$

Projection:

$$
Q = XW_Q
$$
$$
K = XW_K
$$
$$
V = XW_V
$$

$$
W_Q, W_K, W_V \in \mathbb{R}^{d_{model} \times d_k}
$$

---

# 3Ô∏è‚É£ Attention Score Matrix

$$
S = QK^{\top}
$$

Shape:

$$
(n \times d_k)(d_k \times n) = n \times n
$$

‡¶è‡¶ü‡¶æ ‡¶è‡¶ï‡¶ü‡¶ø **token-to-token interaction matrix**‡•§

---

# 4Ô∏è‚É£ Scaling

$$
S' = \frac{QK^{\top}}{\sqrt{d_k}}
$$

### ‡¶ï‡ßá‡¶®?

Dot product-‡¶è‡¶∞ variance \approx $d_k$ proportional‡•§
Scaling ‡¶ï‡¶∞‡¶≤‡ßá variance \approx 1 ‡¶•‡¶æ‡¶ï‡ßá ‚Üí stable softmax‡•§

---

# 5Ô∏è‚É£ Softmax

$$
A = \mathrm{softmax}(S')
$$

Row-wise normalize:

- ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø row sum = 1
- ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø token ‡¶Ö‡¶®‡ßç‡¶Ø token-‡¶ï‡ßá ‡¶ï‡¶§ weight ‡¶¶‡¶ø‡¶ö‡ßç‡¶õ‡ßá ‡¶∏‡ßá‡¶ü‡¶æ‡¶∞ probability

---

# 6Ô∏è‚É£ Final Output

$$
Z = AV
$$

Shape:

$$
(n \times n)(n \times d_k) = n \times d_k
$$

---

# üî∑ Graphical Conceptual View

## Token Interaction Graph

![Image](https://ars.els-cdn.com/content/image/1-s2.0-S0031320319303851-gr1.jpg)

![Image](https://www.researchgate.net/publication/344485949/figure/fig2/AS%3A943478821371906%401601954288398/Heatmap-of-the-self-attention-weight-matrix-Each-row-shows-the-attention-distribution-a.ppm)

![Image](https://i.sstatic.net/TBpsF.png)

![Image](https://i.sstatic.net/jWduk.png)

---

# 7Ô∏è‚É£ Attention Heatmap Interpretation

‡¶ß‡¶∞‡ßã sentence:

> ‚ÄúThe animal didn‚Äôt cross the street because it was tired‚Äù

Attention matrix-‡¶è‡¶∞ ‡¶è‡¶ï‡¶ü‡¶ø heatmap ‡¶¶‡ßá‡¶ñ‡¶§‡ßá ‡¶è‡¶Æ‡¶® ‡¶π‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá:

| From \ To | animal | street | it   |
| --------- | ------ | ------ | ---- |
| it        | 0.65   | 0.10   | 0.25 |

‡¶Æ‡¶æ‡¶®‡ßá:

- "it" ‚Üí "animal"-‡¶è ‡¶¨‡ßá‡¶∂‡¶ø weight ‡¶¶‡¶ø‡¶ö‡ßç‡¶õ‡ßá
- Coreference resolve ‡¶π‡¶ö‡ßç‡¶õ‡ßá

---

# 8Ô∏è‚É£ Geometric Interpretation

Dot product:

$$
q_i \cdot k_j
$$

- Angle ‡¶õ‡ßã‡¶ü ‡¶π‡¶≤‡ßá similarity ‡¶¨‡ßá‡¶∂‡¶ø
- Angle ‡¶¨‡ßú ‡¶π‡¶≤‡ßá similarity ‡¶ï‡¶Æ

Attention essentially vector similarity measure‡•§

---

# 9Ô∏è‚É£ Probabilistic View

Softmax ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶™‡¶∞‡ßá:

$$
a_{ij} = \frac{e^{\mathrm{score}_{ij}}}{\sum_j e^{\mathrm{score}_{ij}}}
$$

‡¶è‡¶ü‡¶æ conditional distribution:

> "Given token i, how important is token j?"

---

# üîü Why It Is Powerful

Self-Attention:

‚úî Long-range dependency capture
‚úî Fully parallel
‚úî No recurrence
‚úî Context-aware embedding

---

# 11Ô∏è‚É£ Multi-Head Extension

Instead of single:

$$
\mathrm{Attention}(Q,K,V)
$$

We use:

$$
\mathrm{head}_i = \mathrm{Attention}(QW_i^Q, KW_i^K, VW_i^V)
$$

$$
\mathrm{MultiHead} = \mathrm{Concat}(\mathrm{head}_1,\dots,\mathrm{head}_h)W_O
$$

Different heads learn:

- Syntax relation
- Coreference
- Position-based pattern
- Semantic similarity

---

# 12Ô∏è‚É£ Complexity Analysis

Time complexity:

$$
O(n^2 d_k)
$$

Problem:

- Long sequence ‚Üí quadratic memory

Solution variants:

- Sparse attention
- Linear attention
- Flash attention

---

# 13Ô∏è‚É£ Variance Insight (Why Scaling Works)

If:

$$
\mathrm{Var}(q_i)=1,\quad \mathrm{Var}(k_i)=1
$$

Then:

$$
\mathrm{Var}(q\cdot k) = d_k
$$

Divide by $\sqrt{d_k}$:

$$
\mathrm{Var} \approx 1
$$

Softmax stable ‡¶•‡¶æ‡¶ï‡ßá‡•§

---

# 14Ô∏è‚É£ Full Formula (Compact Form)

$$
Z = \mathrm{softmax}\left(\frac{QK^{\top}}{\sqrt{d_k}}\right)V
$$

---

# 15Ô∏è‚É£ Conceptual Summary

Self-Attention =

- Similarity computation
- Probability distribution
- Weighted aggregation
- Context fusion

---
