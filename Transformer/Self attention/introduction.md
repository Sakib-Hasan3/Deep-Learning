# Self-Attention Layer Working (Step-by-Step Deep Explanation)

Self-Attention à¦¹à¦²à§‹ Transformer-à¦à¦° à¦®à§‚à¦² à¦¶à¦•à§à¦¤à¦¿à¥¤
à¦à¦Ÿà¦¾ à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ token-à¦•à§‡ à¦¬à¦¾à¦•à§à¦¯à§‡à¦° à¦…à¦¨à§à¦¯ à¦¸à¦¬ token-à¦à¦° à¦¸à¦¾à¦¥à§‡ à¦¸à¦®à§à¦ªà¦°à§à¦• à¦¬à§à¦à¦¤à§‡ à¦¸à¦¾à¦¹à¦¾à¦¯à§à¦¯ à¦•à¦°à§‡à¥¤

---

# ðŸ”· High-Level Idea

à¦§à¦°à§‹ à¦¬à¦¾à¦•à§à¦¯:

> "The cat sat on the mat"

Self-attention à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ à¦¶à¦¬à§à¦¦à¦•à§‡ à¦œà¦¿à¦œà§à¦žà§‡à¦¸ à¦•à¦°à§‡:

> "à¦†à¦®à¦¾à¦° à¦…à¦°à§à¦¥ à¦¬à§‹à¦à¦¾à¦° à¦œà¦¨à§à¦¯ à¦¬à¦¾à¦•à§à¦¯à§‡à¦° à¦•à§‹à¦¨ à¦¶à¦¬à§à¦¦à¦—à§à¦²à§‹ à¦—à§à¦°à§à¦¤à§à¦¬à¦ªà§‚à¦°à§à¦£?"

---

# ðŸ”· Step 1: Input Representation

à¦ªà§à¦°à¦¥à¦®à§‡ à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ token embedding vector-à¦ à¦°à§‚à¦ªà¦¾à¦¨à§à¦¤à¦° à¦¹à§Ÿà¥¤

à¦§à¦°à§‹ input matrix:

$$
X \in \mathbb{R}^{n \times d}
$$

* n = token à¦¸à¦‚à¦–à§à¦¯à¦¾
* d = embedding dimension

---

# ðŸ”· Step 2: Q, K, V à¦¤à§ˆà¦°à¦¿ à¦•à¦°à¦¾

à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ input vector à¦¥à§‡à¦•à§‡ à¦¤à¦¿à¦¨à¦Ÿà¦¿ projection à¦¤à§ˆà¦°à¦¿ à¦¹à§Ÿ:

$$
Q = XW_Q
$$

$$
K = XW_K
$$

$$
V = XW_V
$$

à¦à¦—à§à¦²à§‹ learnable weight matrixà¥¤

---

# ðŸ”· Step 3: Similarity Calculation

à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ token à¦…à¦¨à§à¦¯ à¦¸à¦¬ token-à¦à¦° à¦¸à¦¾à¦¥à§‡ similarity à¦¬à§‡à¦° à¦•à¦°à§‡:

$$
Score = QK^T
$$

à¦à¦¤à§‡ à¦à¦•à¦Ÿà¦¿ attention score matrix à¦ªà¦¾à¦“à§Ÿà¦¾ à¦¯à¦¾à§Ÿà¥¤

---

# ðŸ”· Step 4: Scaling

$$
ScaledScore = \frac{QK^T}{\sqrt{d_k}}
$$

à¦•à§‡à¦¨ divide à¦•à¦°à¦¾ à¦¹à§Ÿ?

* Large dimension à¦¹à¦²à§‡ dot product à¦¬à§œ à¦¹à§Ÿà§‡ à¦¯à¦¾à§Ÿ
* Softmax saturation à¦à§œà¦¾à¦¨à§‹à¦° à¦œà¦¨à§à¦¯ scaling à¦•à¦°à¦¾ à¦¹à§Ÿ

---

# ðŸ”· Step 5: Softmax

$$
AttentionWeights = \operatorname{softmax}(\text{ScaledScore})
$$

à¦à¦–à¦¨ à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ row-à¦à¦° sum = 1
à¦®à¦¾à¦¨à§‡ probability distribution à¦¤à§ˆà¦°à¦¿ à¦¹à¦²à§‹à¥¤

---

# ðŸ”· Step 6: Weighted Sum

$$
Output = AttentionWeights \cdot V
$$

à¦à¦–à¦¨ à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ token à¦¨à¦¤à§à¦¨ contextual representation à¦ªà¦¾à§Ÿà¥¤

---

# ðŸ”· Full Formula

$$
\mathrm{Attention}(Q,K,V) = \operatorname{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

---

# ðŸ”· Visual Concept

![Image](https://images.openai.com/static-rsc-3/WDuHb64OVwEt0Dbfie7hNwtoCGvOKHFlgQCPeYn5XL78wA6oR2HOoPJP_2-FVdzTrlBXi7-DRySEQO2LP6p8F9BpETqN0Dl_i9A27o8jZOM?purpose=fullsize\&v=1)

![Image](https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png)

![Image](https://www.researchgate.net/publication/391696986/figure/fig4/AS%3A11431281435777087%401747128418823/Diagram-of-the-query-key-value-and-self-attention-mechanism-The-input-vectors-x-is.ppm)

![Image](https://media.licdn.com/dms/image/v2/D4D12AQEKQJbcosu6Dg/article-cover_image-shrink_600_2000/article-cover_image-shrink_600_2000/0/1681906088994?e=2147483647\&t=OGG4B2Bfop5gUUCto1Ifo2PImkodyRwY3soYkDxH0aw\&v=beta)

---

# ðŸ”· Intuitive Understanding

à¦§à¦°à§‹ à¦¶à¦¬à§à¦¦ = "it"

Self-attention à¦¹à¦¿à¦¸à¦¾à¦¬ à¦•à¦°à¦¬à§‡:

| Word | Importance |
| ---- | ---------- |
| The  | low        |
| cat  | high       |
| sat  | medium     |

à¦¤à¦¾à¦°à¦ªà¦° weighted average à¦•à¦°à§‡ à¦¨à¦¤à§à¦¨ embedding à¦¤à§ˆà¦°à¦¿ à¦•à¦°à¦¬à§‡à¥¤

---

# ðŸ”· Matrix Perspective

Input:
$$
X = [x_1, x_2, x_3]
$$

Attention weight matrix:

$$
A =
\\begin{bmatrix}
0.1 & 0.7 & 0.2 \\
0.3 & 0.4 & 0.3 \\
0.2 & 0.5 & 0.3
\\end{bmatrix}
$$

Output:

$$
Z = A \cdot V
$$

---

# ðŸ”· Why Self-Attention Powerful?

âœ” Long-range dependency capture
âœ” Parallel computation
âœ” Context-aware embedding
âœ” No recurrence needed

---

# ðŸ”· Self-Attention vs RNN

| RNN                  | Self-Attention |
| -------------------- | -------------- |
| Sequential           | Parallel       |
| Long dependency weak | Strong         |
| Slow                 | Fast           |

---

# ðŸ”· Multi-Head Extension

à¦à¦•à¦Ÿà¦¾ attention à¦¨à¦¾, à¦¬à¦°à¦‚ multiple head parallel run à¦•à¦°à§‡:

[
MultiHead = Concat(head_1,...,head_h)W_O
]

à¦à¦¤à§‡ à¦¬à¦¿à¦­à¦¿à¦¨à§à¦¨ à¦§à¦°à¦¨à§‡à¦° relation à¦à¦•à¦¸à¦¾à¦¥à§‡ à¦¶à§‡à¦–à¦¾ à¦¯à¦¾à§Ÿà¥¤

---

# ðŸ”· Practical Intuition

Self-attention =

> "Context à¦¤à§ˆà¦°à¦¿ à¦•à¦°à¦¾ layer"

Feed Forward =

> "Context refine à¦•à¦°à¦¾ layer"

---


