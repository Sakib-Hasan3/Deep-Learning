---


# üî∑ Setup

‡¶ß‡¶∞‡¶ø:

- Sequence length = 2 tokens
- Model dimension $d_{model} = 4$
- Number of heads $h = 2$

‡¶§‡¶æ‡¶π‡¶≤‡ßá:

$$
d_k = \dfrac{d_{model}}{h} = \dfrac{4}{2} = 2
$$

‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø head 2-dimensional space-‡¶è ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡¶¨‡ßá‡•§

---

# üî∑ Input Matrix

‡¶ß‡¶∞‡¶ø ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶¶‡ßÅ‡¶á‡¶ü‡¶æ token embedding:

$$
X =
\begin{bmatrix}
1 & 0 & 1 & 0 \\
0 & 1 & 0 & 1
\end{bmatrix}
$$

Shape: $(2 \times 4)$

---

# üî∑ Step 1: Split into 2 Heads

‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶ß‡¶∞‡¶õ‡¶ø projection ‡¶è‡¶Æ‡¶®‡¶≠‡¶æ‡¶¨‡ßá ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡ßá‡¶õ‡ßá ‡¶Ø‡ßá‡¶®:

Head 1 ‚Üí ‡¶™‡ßç‡¶∞‡¶•‡¶Æ 2 dimension ‡¶®‡ßá‡ßü
Head 2 ‚Üí ‡¶∂‡ßá‡¶∑ 2 dimension ‡¶®‡ßá‡ßü

---

## üîπ Head 1 Input

$$
X_1 =
\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}
$$

---

## üîπ Head 2 Input

$$
X_2 =
\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}
$$

(‡¶∏‡¶π‡¶ú ‡¶¨‡ßã‡¶ù‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶è‡¶ï‡¶á ‡¶ß‡¶∞‡¶≤‡¶æ‡¶Æ)

---

# üî∑ Step 2: Self-Attention per Head

### Head 1

### $Q = K = V = X_1$

$$
Q_1 = K_1 = V_1 =
\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}
$$

---

### Score:

$$
S = QK^{\top} =
\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}
$$

---

### Scaling

$$
\sqrt{d_k} = \sqrt{2} \approx 1.414
$$

$$
S' =
\begin{bmatrix}
0.707 & 0 \\
0 & 0.707
\end{bmatrix}
$$

---

### Softmax (row-wise)

Row1: $\mathrm{softmax}(0.707,0) \approx (0.67,\;0.33)$

Row2: $\mathrm{softmax}(0,0.707) \approx (0.33,\;0.67)$

Attention matrix:

$$
A_1 =
\begin{bmatrix}
0.67 & 0.33 \\
0.33 & 0.67
\end{bmatrix}
$$

---

### Output:

$$
Z_1 = A_1 V_1
$$

Token1 output:

$$
0.67[1,0] + 0.33[0,1] = [0.67,\;0.33]
$$

Token2 output:

$$
0.33[1,0] + 0.67[0,1] = [0.33,\;0.67]
$$

---

## üî∑ Head 2

‡¶è‡¶ï‡¶á process (‡¶ß‡¶∞‡¶ø different projection ‡¶π‡¶≤‡ßá ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ relation ‡¶∂‡¶ø‡¶ñ‡¶§)

‡¶ß‡¶∞‡¶ø output:

$$
Z_2 =
\begin{bmatrix}
0.60 & 0.40 \\
0.40 & 0.60
\end{bmatrix}
$$

---

# üî∑ Step 3: Concatenate Heads

Head1 output (2 dim)
Head2 output (2 dim)

Concat ‡¶ï‡¶∞‡¶≤‡ßá:

Token1:

$$
[0.67,\;0.33,\;0.60,\;0.40]
$$

Token2:

$$
[0.33,\;0.67,\;0.40,\;0.60]
$$

Shape ‡¶Ü‡¶¨‡¶æ‡¶∞ $(2 \times 4)$

---

# üî∑ Step 4: Final Linear Projection

$$
\mathrm{Output} = \mathrm{Concat}(Z_1,Z_2)W_O
$$

‡¶è‡¶ñ‡¶æ‡¶®‡ßá $W_O$ learnable matrix ‡¶Ø‡¶æ final mixing ‡¶ï‡¶∞‡ßá‡•§

---

# üî∑ ‡¶ï‡ßÄ ‡¶¨‡ßã‡¶ù‡¶æ ‡¶ó‡ßá‡¶≤?

Multi-Head Attention:

- ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø head ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ representation space-‡¶è ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá
- ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ relation capture ‡¶ï‡¶∞‡ßá
- ‡¶∂‡ßá‡¶∑‡ßá ‡¶∏‡¶¨ combine ‡¶π‡ßü

---

# üî∑ Intuitive Meaning

Head 1 ‚Üí syntax ‡¶ß‡¶∞‡¶õ‡ßá
Head 2 ‚Üí semantic similarity ‡¶ß‡¶∞‡¶õ‡ßá

‡¶è‡¶ï‡¶∏‡¶æ‡¶•‡ßá ‚Üí richer contextual embedding

---

# üî∑ Why Better Than Single Head?

Single head = ‡¶è‡¶ï ‡¶ß‡¶∞‡¶®‡ßá‡¶∞ similarity

Multi-head =

- Multiple subspace learning
- More expressive
- More stable training

---

