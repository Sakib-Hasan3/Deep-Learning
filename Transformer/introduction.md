<<<<<<< HEAD
---

# ğŸ”· Transformer à¦•à§€? (à¦¬à¦¾à¦‚à¦²à¦¾à§Ÿ à¦¸à¦¹à¦œ à¦•à¦¿à¦¨à§à¦¤à§ à¦—à¦­à§€à¦° à¦¬à§à¦¯à¦¾à¦–à§à¦¯à¦¾)

## 1ï¸âƒ£ à¦•à§‡à¦¨ Transformer à¦¦à¦°à¦•à¦¾à¦° à¦¹à§Ÿà§‡à¦›à¦¿à¦²?

à¦†à¦—à§‡ à¦†à¦®à¦°à¦¾ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à¦¤à¦¾à¦®:

* RNN
* LSTM
* GRU
* Seq2Seq + Attention

à¦¸à¦®à¦¸à§à¦¯à¦¾ à¦›à¦¿à¦²:

* Sequential computation (à¦§à§€à¦°)
* Long sequence-à¦ gradient à¦¸à¦®à¦¸à§à¦¯à¦¾
* Context bottleneck

ğŸ‘‰ Transformer à¦à¦‡ à¦¸à¦¬ à¦¸à¦®à¦¸à§à¦¯à¦¾ à¦¦à§‚à¦° à¦•à¦°à§‡à¥¤

---

## 2ï¸âƒ£ Transformer-à¦à¦° à¦®à§‚à¦² à¦§à¦¾à¦°à¦£à¦¾

Transformer-à¦à¦° à¦­à¦¿à¦¤à§à¦¤à¦¿ à¦¹à¦²à§‹:

> **â€œAttention is All You Needâ€**

à¦®à¦¾à¦¨à§‡:

* RNN à¦¨à§‡à¦‡
* Recurrence à¦¨à§‡à¦‡
* Convolution à¦¨à§‡à¦‡
* à¦¶à§à¦§à§ Attention + Feed Forward Network

---

## 3ï¸âƒ£ Transformer Architecture (High Level)

![Image](https://images.openai.com/static-rsc-3/WDuHb64OVwEt0Dbfie7hNwtoCGvOKHFlgQCPeYn5XL78wA6oR2HOoPJP_2-FVdzTrlBXi7-DRySEQO2LP6p8F9BpETqN0Dl_i9A27o8jZOM?purpose=fullsize\&v=1)

![Image](https://sebastianraschka.com/images/blog/2023/self-attention-from-scratch/summary.png)

![Image](https://d2l.ai/_images/multi-head-attention.svg)

![Image](https://uvadlc-notebooks.readthedocs.io/en/latest/_images/transformer_architecture.svg)

Transformer à¦¦à§à¦‡ à¦­à¦¾à¦—à§‡ à¦¬à¦¿à¦­à¦•à§à¦¤:

### ğŸ”¹ Encoder Stack

### ğŸ”¹ Decoder Stack

à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾ stack-à¦ à¦à¦•à¦¾à¦§à¦¿à¦• identical layer à¦¥à¦¾à¦•à§‡à¥¤

---

# ğŸ”· Encoder Structure

à¦à¦•à¦Ÿà¦¿ encoder layer-à¦ à¦¥à¦¾à¦•à§‡:

1. **Multi-Head Self-Attention**
2. **Feed Forward Neural Network**
3. Residual connection
4. Layer normalization

Input â†’ Attention â†’ Add & Norm â†’ FFN â†’ Add & Norm

---

# ğŸ”· Decoder Structure

Decoder layer-à¦ à¦¥à¦¾à¦•à§‡:

1. Masked Multi-Head Self-Attention
2. Encoder-Decoder Attention
3. Feed Forward
4. Residual + LayerNorm

---

## 4ï¸âƒ£ Self-Attention à¦•à§€?

à¦§à¦°à¦¿ sentence:

> "The animal didn't cross the street because it was too tired"

Transformer à¦à¦–à¦¾à¦¨à§‡ à¦¶à¦¿à¦–à¦¬à§‡:

* â€œitâ€ à¦¶à¦¬à§à¦¦à¦Ÿà¦¿ â€œanimalâ€-à¦•à§‡ refer à¦•à¦°à¦›à§‡

Self-attention à¦•à¦°à§‡:

* à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ à¦¶à¦¬à§à¦¦ à¦…à¦¨à§à¦¯ à¦¸à¦¬ à¦¶à¦¬à§à¦¦à§‡à¦° à¦¸à¦¾à¦¥à§‡ à¦¸à¦®à§à¦ªà¦°à§à¦• à¦®à¦¾à¦ªà¦¤à§‡ à¦ªà¦¾à¦°à§‡

---

## 5ï¸âƒ£ Attention à¦•à¦¿à¦­à¦¾à¦¬à§‡ à¦•à¦¾à¦œ à¦•à¦°à§‡?

à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ input à¦¥à§‡à¦•à§‡ 3à¦Ÿà¦¾ vector à¦¤à§ˆà¦°à¦¿ à¦¹à§Ÿ:

* Query (Q)
* Key (K)
* Value (V)

Formula:

$$
	ext{Attention}(Q, K, V) = \text{softmax}\left(\frac{Q K^{\top}}{\sqrt{d_k}}\right) V
$$

à¦à¦–à¦¾à¦¨à§‡:

* QKáµ€ = similarity score
* âˆšd = scaling factor
* V = weighted sum output

---

## 6ï¸âƒ£ Multi-Head Attention

à¦à¦•à¦Ÿà¦¾ attention à¦¨à¦¾ à¦•à¦°à§‡:

* à¦à¦•à¦¾à¦§à¦¿à¦• attention head à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à¦¾ à¦¹à§Ÿ
* à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ head à¦­à¦¿à¦¨à§à¦¨ relationship à¦¶à§‡à¦–à§‡

à¦¯à§‡à¦®à¦¨:

* Grammar head
* Semantic head
* Long-distance relation head

---

## 7ï¸âƒ£ Transformer à¦•à§‡à¦¨ à¦à¦¤ à¦¶à¦•à§à¦¤à¦¿à¦¶à¦¾à¦²à§€?

### âœ… Parallel Processing

* à¦¸à¦¬ token à¦à¦•à¦¸à¦¾à¦¥à§‡ à¦ªà§à¦°à¦¸à§‡à¦¸ à¦¹à§Ÿ
* GPU efficient

### âœ… Long-Range Dependency à¦¶à§‡à¦–à§‡

* à¦ªà§à¦°à¦¥à¦® à¦¶à¦¬à§à¦¦ à¦¸à¦°à¦¾à¦¸à¦°à¦¿ à¦¶à§‡à¦· à¦¶à¦¬à§à¦¦à§‡à¦° à¦¸à¦¾à¦¥à§‡ à¦¯à§à¦•à§à¦¤ à¦¹à¦¤à§‡ à¦ªà¦¾à¦°à§‡

### âœ… No Recurrence

* Gradient à¦¸à¦®à¦¸à§à¦¯à¦¾ à¦•à¦®

### âœ… Highly Scalable

* GPT, BERT, T5â€”à¦¸à¦¬ Transformer à¦­à¦¿à¦¤à§à¦¤à¦¿à¦•

---

## 8ï¸âƒ£ RNN vs Transformer (à¦¤à§à¦²à¦¨à¦¾)

| Feature         | RNN          | Transformer    |
| --------------- | ------------ | -------------- |
| Processing      | Sequential   | Parallel       |
| Long Dependency | à¦¦à§à¦°à§à¦¬à¦²       | à¦¶à¦•à§à¦¤à¦¿à¦¶à¦¾à¦²à§€      |
| Speed           | à¦§à§€à¦°          | à¦¦à§à¦°à§à¦¤          |
| Memory          | Hidden state | Full attention |
| Scalability     | à¦¸à§€à¦®à¦¿à¦¤        | à¦¬à¦¿à¦¶à¦¾à¦²          |

---

## 9ï¸âƒ£ Transformer à¦•à§‹à¦¥à¦¾à§Ÿ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦¹à§Ÿ?

* GPT (ChatGPT)
* BERT
* Machine Translation
* Text Summarization
* Image (Vision Transformer)
* Speech

---

## ğŸ”Ÿ à¦à¦• à¦²à¦¾à¦‡à¦¨à§‡à¦° à¦¸à¦¾à¦°à¦¾à¦‚à¦¶

**Transformer à¦¹à¦²à§‹ attention-à¦­à¦¿à¦¤à§à¦¤à¦¿à¦• à¦à¦®à¦¨ à¦à¦•à¦Ÿà¦¿ architecture à¦¯à¦¾ à¦ªà§à¦°à§‹ sequence à¦à¦•à¦¸à¦¾à¦¥à§‡ à¦¬à§à¦à¦¤à§‡ à¦ªà¦¾à¦°à§‡ à¦à¦¬à¦‚ RNN-à¦à¦° à¦¸à§€à¦®à¦¾à¦¬à¦¦à§à¦§à¦¤à¦¾ à¦¦à§‚à¦° à¦•à¦°à§‡ à¦†à¦§à§à¦¨à¦¿à¦• NLP-à¦à¦° à¦­à¦¿à¦¤à§à¦¤à¦¿ à¦¤à§ˆà¦°à¦¿ à¦•à¦°à§‡à¦›à§‡à¥¤**

---
=======

---

# ğŸ”· Transformer à¦•à§€? (à¦¬à¦¾à¦‚à¦²à¦¾à§Ÿ à¦¸à¦¹à¦œ à¦•à¦¿à¦¨à§à¦¤à§ à¦—à¦­à§€à¦° à¦¬à§à¦¯à¦¾à¦–à§à¦¯à¦¾)

## 1ï¸âƒ£ à¦•à§‡à¦¨ Transformer à¦¦à¦°à¦•à¦¾à¦° à¦¹à§Ÿà§‡à¦›à¦¿à¦²?

à¦†à¦—à§‡ à¦†à¦®à¦°à¦¾ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à¦¤à¦¾à¦®:

* RNN
* LSTM
* GRU
* Seq2Seq + Attention

à¦¸à¦®à¦¸à§à¦¯à¦¾ à¦›à¦¿à¦²:

* Sequential computation (à¦§à§€à¦°)
* Long sequence-à¦ gradient à¦¸à¦®à¦¸à§à¦¯à¦¾
* Context bottleneck

ğŸ‘‰ Transformer à¦à¦‡ à¦¸à¦¬ à¦¸à¦®à¦¸à§à¦¯à¦¾ à¦¦à§‚à¦° à¦•à¦°à§‡à¥¤

---

## 2ï¸âƒ£ Transformer-à¦à¦° à¦®à§‚à¦² à¦§à¦¾à¦°à¦£à¦¾

Transformer-à¦à¦° à¦­à¦¿à¦¤à§à¦¤à¦¿ à¦¹à¦²à§‹:

> **â€œAttention is All You Needâ€**

à¦®à¦¾à¦¨à§‡:

* RNN à¦¨à§‡à¦‡
* Recurrence à¦¨à§‡à¦‡
* Convolution à¦¨à§‡à¦‡
* à¦¶à§à¦§à§ Attention + Feed Forward Network

---

## 3ï¸âƒ£ Transformer Architecture (High Level)

![Image](https://images.openai.com/static-rsc-3/WDuHb64OVwEt0Dbfie7hNwtoCGvOKHFlgQCPeYn5XL78wA6oR2HOoPJP_2-FVdzTrlBXi7-DRySEQO2LP6p8F9BpETqN0Dl_i9A27o8jZOM?purpose=fullsize\&v=1)

![Image](https://sebastianraschka.com/images/blog/2023/self-attention-from-scratch/summary.png)

![Image](https://d2l.ai/_images/multi-head-attention.svg)

![Image](https://uvadlc-notebooks.readthedocs.io/en/latest/_images/transformer_architecture.svg)

Transformer à¦¦à§à¦‡ à¦­à¦¾à¦—à§‡ à¦¬à¦¿à¦­à¦•à§à¦¤:

### ğŸ”¹ Encoder Stack

### ğŸ”¹ Decoder Stack

à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾ stack-à¦ à¦à¦•à¦¾à¦§à¦¿à¦• identical layer à¦¥à¦¾à¦•à§‡à¥¤

---

# ğŸ”· Encoder Structure

à¦à¦•à¦Ÿà¦¿ encoder layer-à¦ à¦¥à¦¾à¦•à§‡:

1. **Multi-Head Self-Attention**
2. **Feed Forward Neural Network**
3. Residual connection
4. Layer normalization

Input â†’ Attention â†’ Add & Norm â†’ FFN â†’ Add & Norm

---

# ğŸ”· Decoder Structure

Decoder layer-à¦ à¦¥à¦¾à¦•à§‡:

1. Masked Multi-Head Self-Attention
2. Encoder-Decoder Attention
3. Feed Forward
4. Residual + LayerNorm

---

## 4ï¸âƒ£ Self-Attention à¦•à§€?

à¦§à¦°à¦¿ sentence:

> "The animal didn't cross the street because it was too tired"

Transformer à¦à¦–à¦¾à¦¨à§‡ à¦¶à¦¿à¦–à¦¬à§‡:

* â€œitâ€ à¦¶à¦¬à§à¦¦à¦Ÿà¦¿ â€œanimalâ€-à¦•à§‡ refer à¦•à¦°à¦›à§‡

Self-attention à¦•à¦°à§‡:

* à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ à¦¶à¦¬à§à¦¦ à¦…à¦¨à§à¦¯ à¦¸à¦¬ à¦¶à¦¬à§à¦¦à§‡à¦° à¦¸à¦¾à¦¥à§‡ à¦¸à¦®à§à¦ªà¦°à§à¦• à¦®à¦¾à¦ªà¦¤à§‡ à¦ªà¦¾à¦°à§‡

---

## 5ï¸âƒ£ Attention à¦•à¦¿à¦­à¦¾à¦¬à§‡ à¦•à¦¾à¦œ à¦•à¦°à§‡?

à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ input à¦¥à§‡à¦•à§‡ 3à¦Ÿà¦¾ vector à¦¤à§ˆà¦°à¦¿ à¦¹à§Ÿ:

* Query (Q)
* Key (K)
* Value (V)

Formula:

$$
	ext{Attention}(Q, K, V) = \text{softmax}\left(\frac{Q K^{\top}}{\sqrt{d_k}}\right) V
$$

à¦à¦–à¦¾à¦¨à§‡:

* QKáµ€ = similarity score
* âˆšd = scaling factor
* V = weighted sum output

---

## 6ï¸âƒ£ Multi-Head Attention

à¦à¦•à¦Ÿà¦¾ attention à¦¨à¦¾ à¦•à¦°à§‡:

* à¦à¦•à¦¾à¦§à¦¿à¦• attention head à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à¦¾ à¦¹à§Ÿ
* à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ head à¦­à¦¿à¦¨à§à¦¨ relationship à¦¶à§‡à¦–à§‡

à¦¯à§‡à¦®à¦¨:

* Grammar head
* Semantic head
* Long-distance relation head

---

## 7ï¸âƒ£ Transformer à¦•à§‡à¦¨ à¦à¦¤ à¦¶à¦•à§à¦¤à¦¿à¦¶à¦¾à¦²à§€?

### âœ… Parallel Processing

* à¦¸à¦¬ token à¦à¦•à¦¸à¦¾à¦¥à§‡ à¦ªà§à¦°à¦¸à§‡à¦¸ à¦¹à§Ÿ
* GPU efficient

### âœ… Long-Range Dependency à¦¶à§‡à¦–à§‡

* à¦ªà§à¦°à¦¥à¦® à¦¶à¦¬à§à¦¦ à¦¸à¦°à¦¾à¦¸à¦°à¦¿ à¦¶à§‡à¦· à¦¶à¦¬à§à¦¦à§‡à¦° à¦¸à¦¾à¦¥à§‡ à¦¯à§à¦•à§à¦¤ à¦¹à¦¤à§‡ à¦ªà¦¾à¦°à§‡

### âœ… No Recurrence

* Gradient à¦¸à¦®à¦¸à§à¦¯à¦¾ à¦•à¦®

### âœ… Highly Scalable

* GPT, BERT, T5â€”à¦¸à¦¬ Transformer à¦­à¦¿à¦¤à§à¦¤à¦¿à¦•

---

## 8ï¸âƒ£ RNN vs Transformer (à¦¤à§à¦²à¦¨à¦¾)

| Feature         | RNN          | Transformer    |
| --------------- | ------------ | -------------- |
| Processing      | Sequential   | Parallel       |
| Long Dependency | à¦¦à§à¦°à§à¦¬à¦²       | à¦¶à¦•à§à¦¤à¦¿à¦¶à¦¾à¦²à§€      |
| Speed           | à¦§à§€à¦°          | à¦¦à§à¦°à§à¦¤          |
| Memory          | Hidden state | Full attention |
| Scalability     | à¦¸à§€à¦®à¦¿à¦¤        | à¦¬à¦¿à¦¶à¦¾à¦²          |

---

## 9ï¸âƒ£ Transformer à¦•à§‹à¦¥à¦¾à§Ÿ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦¹à§Ÿ?

* GPT (ChatGPT)
* BERT
* Machine Translation
* Text Summarization
* Image (Vision Transformer)
* Speech

---

## ğŸ”Ÿ à¦à¦• à¦²à¦¾à¦‡à¦¨à§‡à¦° à¦¸à¦¾à¦°à¦¾à¦‚à¦¶

**Transformer à¦¹à¦²à§‹ attention-à¦­à¦¿à¦¤à§à¦¤à¦¿à¦• à¦à¦®à¦¨ à¦à¦•à¦Ÿà¦¿ architecture à¦¯à¦¾ à¦ªà§à¦°à§‹ sequence à¦à¦•à¦¸à¦¾à¦¥à§‡ à¦¬à§à¦à¦¤à§‡ à¦ªà¦¾à¦°à§‡ à¦à¦¬à¦‚ RNN-à¦à¦° à¦¸à§€à¦®à¦¾à¦¬à¦¦à§à¦§à¦¤à¦¾ à¦¦à§‚à¦° à¦•à¦°à§‡ à¦†à¦§à§à¦¨à¦¿à¦• NLP-à¦à¦° à¦­à¦¿à¦¤à§à¦¤à¦¿ à¦¤à§ˆà¦°à¦¿ à¦•à¦°à§‡à¦›à§‡à¥¤**

---



>>>>>>> f45ebbad1686e699afe9932c4175eeff501d254b
