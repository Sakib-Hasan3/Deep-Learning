
---

# ЁЯФ╖ Transformer ржХрзА? (ржмрж╛ржВрж▓рж╛рзЯ рж╕рж╣ржЬ ржХрж┐ржирзНрждрзБ ржЧржнрзАрж░ ржмрзНржпрж╛ржЦрзНржпрж╛)

## 1я╕ПтГг ржХрзЗржи Transformer ржжрж░ржХрж╛рж░ рж╣рзЯрзЗржЫрж┐рж▓?

ржЖржЧрзЗ ржЖржорж░рж╛ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рждрж╛ржо:

* RNN
* LSTM
* GRU
* Seq2Seq + Attention

рж╕ржорж╕рзНржпрж╛ ржЫрж┐рж▓:

* Sequential computation (ржзрзАрж░)
* Long sequence-ржП gradient рж╕ржорж╕рзНржпрж╛
* Context bottleneck

ЁЯСЙ Transformer ржПржЗ рж╕ржм рж╕ржорж╕рзНржпрж╛ ржжрзВрж░ ржХрж░рзЗред

---

## 2я╕ПтГг Transformer-ржПрж░ ржорзВрж▓ ржзрж╛рж░ржгрж╛

Transformer-ржПрж░ ржнрж┐рждрзНрждрж┐ рж╣рж▓рзЛ:

> **тАЬAttention is All You NeedтАЭ**

ржорж╛ржирзЗ:

* RNN ржирзЗржЗ
* Recurrence ржирзЗржЗ
* Convolution ржирзЗржЗ
* рж╢рзБржзрзБ Attention + Feed Forward Network

---

## 3я╕ПтГг Transformer Architecture (High Level)

![Image](https://images.openai.com/static-rsc-3/WDuHb64OVwEt0Dbfie7hNwtoCGvOKHFlgQCPeYn5XL78wA6oR2HOoPJP_2-FVdzTrlBXi7-DRySEQO2LP6p8F9BpETqN0Dl_i9A27o8jZOM?purpose=fullsize\&v=1)

![Image](https://sebastianraschka.com/images/blog/2023/self-attention-from-scratch/summary.png)

![Image](https://d2l.ai/_images/multi-head-attention.svg)

![Image](https://uvadlc-notebooks.readthedocs.io/en/latest/_images/transformer_architecture.svg)

Transformer ржжрзБржЗ ржнрж╛ржЧрзЗ ржмрж┐ржнржХрзНржд:

### ЁЯФ╣ Encoder Stack

### ЁЯФ╣ Decoder Stack

ржкрзНрж░рждрж┐ржЯрж╛ stack-ржП ржПржХрж╛ржзрж┐ржХ identical layer ржерж╛ржХрзЗред

---

# ЁЯФ╖ Encoder Structure

ржПржХржЯрж┐ encoder layer-ржП ржерж╛ржХрзЗ:

1. **Multi-Head Self-Attention**
2. **Feed Forward Neural Network**
3. Residual connection
4. Layer normalization

Input тЖТ Attention тЖТ Add & Norm тЖТ FFN тЖТ Add & Norm

---

# ЁЯФ╖ Decoder Structure

Decoder layer-ржП ржерж╛ржХрзЗ:

1. Masked Multi-Head Self-Attention
2. Encoder-Decoder Attention
3. Feed Forward
4. Residual + LayerNorm

---

## 4я╕ПтГг Self-Attention ржХрзА?

ржзрж░рж┐ sentence:

> "The animal didn't cross the street because it was too tired"

Transformer ржПржЦрж╛ржирзЗ рж╢рж┐ржЦржмрзЗ:

* тАЬitтАЭ рж╢ржмрзНржжржЯрж┐ тАЬanimalтАЭ-ржХрзЗ refer ржХрж░ржЫрзЗ

Self-attention ржХрж░рзЗ:

* ржкрзНрж░рждрж┐ржЯрж┐ рж╢ржмрзНржж ржЕржирзНржп рж╕ржм рж╢ржмрзНржжрзЗрж░ рж╕рж╛ржерзЗ рж╕ржорзНржкрж░рзНржХ ржорж╛ржкрждрзЗ ржкрж╛рж░рзЗ

---

## 5я╕ПтГг Attention ржХрж┐ржнрж╛ржмрзЗ ржХрж╛ржЬ ржХрж░рзЗ?

ржкрзНрж░рждрж┐ржЯрж┐ input ржерзЗржХрзЗ 3ржЯрж╛ vector рждрзИрж░рж┐ рж╣рзЯ:

* Query (Q)
* Key (K)
* Value (V)

Formula:

$$
	ext{Attention}(Q, K, V) = \text{softmax}\left(\frac{Q K^{\top}}{\sqrt{d_k}}\right) V
$$

ржПржЦрж╛ржирзЗ:

* QKс╡А = similarity score
* тИЪd = scaling factor
* V = weighted sum output

---

## 6я╕ПтГг Multi-Head Attention

ржПржХржЯрж╛ attention ржирж╛ ржХрж░рзЗ:

* ржПржХрж╛ржзрж┐ржХ attention head ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛ рж╣рзЯ
* ржкрзНрж░рждрж┐ржЯрж┐ head ржнрж┐ржирзНржи relationship рж╢рзЗржЦрзЗ

ржпрзЗржоржи:

* Grammar head
* Semantic head
* Long-distance relation head

---

## 7я╕ПтГг Transformer ржХрзЗржи ржПржд рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзА?

### тЬЕ Parallel Processing

* рж╕ржм token ржПржХрж╕рж╛ржерзЗ ржкрзНрж░рж╕рзЗрж╕ рж╣рзЯ
* GPU efficient

### тЬЕ Long-Range Dependency рж╢рзЗржЦрзЗ

* ржкрзНрж░ржержо рж╢ржмрзНржж рж╕рж░рж╛рж╕рж░рж┐ рж╢рзЗрж╖ рж╢ржмрзНржжрзЗрж░ рж╕рж╛ржерзЗ ржпрзБржХрзНржд рж╣рждрзЗ ржкрж╛рж░рзЗ

### тЬЕ No Recurrence

* Gradient рж╕ржорж╕рзНржпрж╛ ржХржо

### тЬЕ Highly Scalable

* GPT, BERT, T5тАФрж╕ржм Transformer ржнрж┐рждрзНрждрж┐ржХ

---

## 8я╕ПтГг RNN vs Transformer (рждрзБрж▓ржирж╛)

| Feature         | RNN          | Transformer    |
| --------------- | ------------ | -------------- |
| Processing      | Sequential   | Parallel       |
| Long Dependency | ржжрзБрж░рзНржмрж▓       | рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзА      |
| Speed           | ржзрзАрж░          | ржжрзНрж░рзБржд          |
| Memory          | Hidden state | Full attention |
| Scalability     | рж╕рзАржорж┐ржд        | ржмрж┐рж╢рж╛рж▓          |

---

## 9я╕ПтГг Transformer ржХрзЛржерж╛рзЯ ржмрзНржпржмрж╣рж╛рж░ рж╣рзЯ?

* GPT (ChatGPT)
* BERT
* Machine Translation
* Text Summarization
* Image (Vision Transformer)
* Speech

---

## ЁЯФЯ ржПржХ рж▓рж╛ржЗржирзЗрж░ рж╕рж╛рж░рж╛ржВрж╢

**Transformer рж╣рж▓рзЛ attention-ржнрж┐рждрзНрждрж┐ржХ ржПржоржи ржПржХржЯрж┐ architecture ржпрж╛ ржкрзБрж░рзЛ sequence ржПржХрж╕рж╛ржерзЗ ржмрзБржЭрждрзЗ ржкрж╛рж░рзЗ ржПржмржВ RNN-ржПрж░ рж╕рзАржорж╛ржмржжрзНржзрждрж╛ ржжрзВрж░ ржХрж░рзЗ ржЖржзрзБржирж┐ржХ NLP-ржПрж░ ржнрж┐рждрзНрждрж┐ рждрзИрж░рж┐ ржХрж░рзЗржЫрзЗред**

---



