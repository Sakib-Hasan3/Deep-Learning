<<<<<<< HEAD
---

# ğŸ”¹ Weight Initialization Techniques

## â“ Weight Initialization à¦•à§€?

Neural Network train à¦•à¦°à¦¾à¦° à¦†à¦—à§‡ à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾ neuron-à¦à¦° weight-à¦ **à¦¶à§à¦°à§à¦° à¦®à¦¾à¦¨ à¦¬à¦¸à¦¾à¦¨à§‹à¦•à§‡** Weight Initialization à¦¬à¦²à§‡à¥¤

à¦­à§à¦² initialization à¦¹à¦²à§‡:

* âŒ Vanishing Gradient
* âŒ Exploding Gradient
* âŒ Training slow à¦¬à¦¾ fail

---

## ğŸ¯ à¦²à¦•à§à¦·à§à¦¯ (Why important?)

Weight à¦à¦®à¦¨ à¦¹à¦¤à§‡ à¦¹à¦¬à§‡ à¦¯à§‡à¦¨:

* Gradient à¦–à§à¦¬ à¦›à§‹à¦Ÿ à¦¨à¦¾ à¦¹à§Ÿ
* Gradient à¦–à§à¦¬ à¦¬à§œ à¦¨à¦¾ à¦¹à§Ÿ
* Signal forward & backward à¦¦à§à¦‡ à¦¦à¦¿à¦•à§‡à¦‡ stable à¦¥à¦¾à¦•à§‡

---

## ğŸš« à¦–à¦¾à¦°à¦¾à¦ª à¦‰à¦¦à¦¾à¦¹à¦°à¦£

### 1ï¸âƒ£ à¦¸à¦¬ weight = 0

```math
w = 0
```

à¦¸à¦®à¦¸à§à¦¯à¦¾:

* à¦¸à¦¬ neuron à¦à¦•à¦‡ output à¦¦à§‡à¦¬à§‡
* à¦¸à¦¬ gradient à¦à¦•à¦‡ à¦¹à¦¬à§‡
* Network à¦•à¦¿à¦›à§à¦‡ à¦¶à¦¿à¦–à¦¬à§‡ à¦¨à¦¾
  â¡ï¸ **Symmetry Problem**

---

### 2ï¸âƒ£ à¦–à§à¦¬ à¦¬à§œ random weight

```math
w \sim \mathcal{N}(0, 10)
```

à¦¸à¦®à¦¸à§à¦¯à¦¾:

* Activation saturate à¦•à¦°à§‡
* Exploding Gradient

---

## âœ… à¦­à¦¾à¦²à§‹ Initialization Techniques

---

## 1ï¸âƒ£ Random Initialization (Basic)

```math
w \sim \mathcal{N}(0, 0.01)
```

âœ”ï¸ Symmetry à¦­à¦¾à¦™à§‡
âŒ Deep network-à¦à¦° à¦œà¦¨à§à¦¯ à¦­à¦¾à¦²à§‹ à¦¨à¦¾

---

## 2ï¸âƒ£ Xavier / Glorot Initialization

### ğŸ¯ à¦•à¦–à¦¨ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦°?

* **Sigmoid**
* **Tanh**

---

### ğŸ“ Formula

```math
w \sim U\left(-\sqrt{\frac{6}{n_{in}+n_{out}}},
\sqrt{\frac{6}{n_{in}+n_{out}}}\right)
```

```math
w \sim \mathcal{N}\left(0, \frac{2}{n_{in}+n_{out}}\right)
```

---

### ğŸ§  Intuition

* Forward à¦“ backward signal-à¦à¦° variance à¦¸à¦®à¦¾à¦¨ à¦°à¦¾à¦–à§‡
* Gradient neither vanish nor explode

---

### ğŸ”¢ Example

à¦§à¦°à¦¿:

* Input neuron = 100
* Output neuron = 50

```math
\sqrt{\frac{6}{150}} = \sqrt{0.04} = 0.2
```

Weight range:
```math
[-0.2, +0.2]
```

---

## 3ï¸âƒ£ He Initialization

### ğŸ¯ à¦•à¦–à¦¨ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦°?

* **ReLU**
* **Leaky ReLU**
* **ELU**

---

### ğŸ“ Formula

```math
w \sim \mathcal{N}\left(0, \frac{2}{n_{in}}\right)
```

```math
w \sim U\left(-\sqrt{\frac{6}{n_{in}}},
\sqrt{\frac{6}{n_{in}}}\right)
```

---

### ğŸ§  Intuition

ReLU à¦…à¦°à§à¦§à§‡à¦• neuron deactivate à¦•à¦°à§‡ (negative à¦…à¦‚à¦¶)

â¡ï¸ à¦¤à¦¾à¦‡ variance à¦¬à¦¾à§œà¦¾à¦¨à§‹ à¦¦à¦°à¦•à¦¾à¦°

---

### ğŸ”¢ Example

à¦§à¦°à¦¿:

* Input neuron = 100

```math
\sqrt{\frac{6}{100}} = \sqrt{0.06} \approx 0.245
```

Weight range:
```math
[-0.245, +0.245]
```

---

## 4ï¸âƒ£ LeCun Initialization

### ğŸ¯ à¦•à¦–à¦¨ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦°?

* **SELU**
* **Self-Normalizing Networks**

---

### ğŸ“ Formula

```math
w \sim \mathcal{N}\left(0, \frac{1}{n_{in}}\right)
```

```math
w \sim U\left(-\sqrt{\frac{3}{n_{in}}},
\sqrt{\frac{3}{n_{in}}}\right)
```

---

## 5ï¸âƒ£ Orthogonal Initialization

```math
W^T W = I
```

âœ”ï¸ Gradient stable à¦¥à¦¾à¦•à§‡
âœ”ï¸ RNN-à¦ à¦­à¦¾à¦²à§‹ à¦•à¦¾à¦œ à¦•à¦°à§‡
âŒ Compute expensive

---

## ğŸ“Š Comparison Table

| Initialization | Best For         | Activation    | Problem Solved  |
| -------------- | ---------------- | ------------- | --------------- |
| Random         | Shallow NN       | Any           | Symmetry        |
| Xavier         | Medium / Deep    | Sigmoid, Tanh | Vanishing       |
| He             | Deep NN          | ReLU family   | Vanishing       |
| LeCun          | Self-normalizing | SELU          | Stable variance |
| Orthogonal     | RNN              | Any           | Exploding       |

---

## ğŸ§  à¦ªà¦°à§€à¦•à§à¦·à¦¾à¦° à¦œà¦¨à§à¦¯ Key Points

* Sigmoid/Tanh â†’ **Xavier**
* ReLU family â†’ **He**
* SELU â†’ **LeCun**
* RNN â†’ **Orthogonal**

---

## ğŸ”— Weight Init vs Gradient Problem

| Weight      | Result             |
| ----------- | ------------------ |
| Too small   | Vanishing Gradient |
| Too large   | Exploding Gradient |
| Proper init | Stable training    |

---

## âœï¸ à¦à¦• à¦²à¦¾à¦‡à¦¨à§‡à¦° à¦¸à¦‚à¦œà§à¦à¦¾

> **Weight Initialization à¦¹à¦² neural network training à¦¶à§à¦°à§ à¦•à¦°à¦¾à¦° à¦†à¦—à§‡ à¦à¦®à¦¨à¦­à¦¾à¦¬à§‡ weight à¦¸à§‡à¦Ÿ à¦•à¦°à¦¾ à¦¯à¦¾à¦¤à§‡ forward à¦“ backward signal stable à¦¥à¦¾à¦•à§‡à¥¤**

---

=======
---

# ğŸ”¹ Weight Initialization Techniques

## â“ Weight Initialization à¦•à§€?

Neural Network train à¦•à¦°à¦¾à¦° à¦†à¦—à§‡ à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾ neuron-à¦à¦° weight-à¦ **à¦¶à§à¦°à§à¦° à¦®à¦¾à¦¨ à¦¬à¦¸à¦¾à¦¨à§‹à¦•à§‡** Weight Initialization à¦¬à¦²à§‡à¥¤

à¦­à§à¦² initialization à¦¹à¦²à§‡:

* âŒ Vanishing Gradient
* âŒ Exploding Gradient
* âŒ Training slow à¦¬à¦¾ fail

---

## ğŸ¯ à¦²à¦•à§à¦·à§à¦¯ (Why important?)

Weight à¦à¦®à¦¨ à¦¹à¦¤à§‡ à¦¹à¦¬à§‡ à¦¯à§‡à¦¨:

* Gradient à¦–à§à¦¬ à¦›à§‹à¦Ÿ à¦¨à¦¾ à¦¹à§Ÿ
* Gradient à¦–à§à¦¬ à¦¬à§œ à¦¨à¦¾ à¦¹à§Ÿ
* Signal forward & backward à¦¦à§à¦‡ à¦¦à¦¿à¦•à§‡à¦‡ stable à¦¥à¦¾à¦•à§‡

---

## ğŸš« à¦–à¦¾à¦°à¦¾à¦ª à¦‰à¦¦à¦¾à¦¹à¦°à¦£

### 1ï¸âƒ£ à¦¸à¦¬ weight = 0

```math
w = 0
```

à¦¸à¦®à¦¸à§à¦¯à¦¾:

* à¦¸à¦¬ neuron à¦à¦•à¦‡ output à¦¦à§‡à¦¬à§‡
* à¦¸à¦¬ gradient à¦à¦•à¦‡ à¦¹à¦¬à§‡
* Network à¦•à¦¿à¦›à§à¦‡ à¦¶à¦¿à¦–à¦¬à§‡ à¦¨à¦¾
  â¡ï¸ **Symmetry Problem**

---

### 2ï¸âƒ£ à¦–à§à¦¬ à¦¬à§œ random weight

```math
w \sim \mathcal{N}(0, 10)
```

à¦¸à¦®à¦¸à§à¦¯à¦¾:

* Activation saturate à¦•à¦°à§‡
* Exploding Gradient

---

## âœ… à¦­à¦¾à¦²à§‹ Initialization Techniques

---

## 1ï¸âƒ£ Random Initialization (Basic)

```math
w \sim \mathcal{N}(0, 0.01)
```

âœ”ï¸ Symmetry à¦­à¦¾à¦™à§‡
âŒ Deep network-à¦à¦° à¦œà¦¨à§à¦¯ à¦­à¦¾à¦²à§‹ à¦¨à¦¾

---

## 2ï¸âƒ£ Xavier / Glorot Initialization

### ğŸ¯ à¦•à¦–à¦¨ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦°?

* **Sigmoid**
* **Tanh**

---

### ğŸ“ Formula

```math
w \sim U\left(-\sqrt{\frac{6}{n_{in}+n_{out}}},
\sqrt{\frac{6}{n_{in}+n_{out}}}\right)
```

```math
w \sim \mathcal{N}\left(0, \frac{2}{n_{in}+n_{out}}\right)
```

---

### ğŸ§  Intuition

* Forward à¦“ backward signal-à¦à¦° variance à¦¸à¦®à¦¾à¦¨ à¦°à¦¾à¦–à§‡
* Gradient neither vanish nor explode

---

### ğŸ”¢ Example

à¦§à¦°à¦¿:

* Input neuron = 100
* Output neuron = 50

```math
\sqrt{\frac{6}{150}} = \sqrt{0.04} = 0.2
```

Weight range:
```math
[-0.2, +0.2]
```

---

## 3ï¸âƒ£ He Initialization

### ğŸ¯ à¦•à¦–à¦¨ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦°?

* **ReLU**
* **Leaky ReLU**
* **ELU**

---

### ğŸ“ Formula

```math
w \sim \mathcal{N}\left(0, \frac{2}{n_{in}}\right)
```

```math
w \sim U\left(-\sqrt{\frac{6}{n_{in}}},
\sqrt{\frac{6}{n_{in}}}\right)
```

---

### ğŸ§  Intuition

ReLU à¦…à¦°à§à¦§à§‡à¦• neuron deactivate à¦•à¦°à§‡ (negative à¦…à¦‚à¦¶)

â¡ï¸ à¦¤à¦¾à¦‡ variance à¦¬à¦¾à§œà¦¾à¦¨à§‹ à¦¦à¦°à¦•à¦¾à¦°

---

### ğŸ”¢ Example

à¦§à¦°à¦¿:

* Input neuron = 100

```math
\sqrt{\frac{6}{100}} = \sqrt{0.06} \approx 0.245
```

Weight range:
```math
[-0.245, +0.245]
```

---

## 4ï¸âƒ£ LeCun Initialization

### ğŸ¯ à¦•à¦–à¦¨ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦°?

* **SELU**
* **Self-Normalizing Networks**

---

### ğŸ“ Formula

```math
w \sim \mathcal{N}\left(0, \frac{1}{n_{in}}\right)
```

```math
w \sim U\left(-\sqrt{\frac{3}{n_{in}}},
\sqrt{\frac{3}{n_{in}}}\right)
```

---

## 5ï¸âƒ£ Orthogonal Initialization

```math
W^T W = I
```

âœ”ï¸ Gradient stable à¦¥à¦¾à¦•à§‡
âœ”ï¸ RNN-à¦ à¦­à¦¾à¦²à§‹ à¦•à¦¾à¦œ à¦•à¦°à§‡
âŒ Compute expensive

---

## ğŸ“Š Comparison Table

| Initialization | Best For         | Activation    | Problem Solved  |
| -------------- | ---------------- | ------------- | --------------- |
| Random         | Shallow NN       | Any           | Symmetry        |
| Xavier         | Medium / Deep    | Sigmoid, Tanh | Vanishing       |
| He             | Deep NN          | ReLU family   | Vanishing       |
| LeCun          | Self-normalizing | SELU          | Stable variance |
| Orthogonal     | RNN              | Any           | Exploding       |

---

## ğŸ§  à¦ªà¦°à§€à¦•à§à¦·à¦¾à¦° à¦œà¦¨à§à¦¯ Key Points

* Sigmoid/Tanh â†’ **Xavier**
* ReLU family â†’ **He**
* SELU â†’ **LeCun**
* RNN â†’ **Orthogonal**

---

## ğŸ”— Weight Init vs Gradient Problem

| Weight      | Result             |
| ----------- | ------------------ |
| Too small   | Vanishing Gradient |
| Too large   | Exploding Gradient |
| Proper init | Stable training    |

---

## âœï¸ à¦à¦• à¦²à¦¾à¦‡à¦¨à§‡à¦° à¦¸à¦‚à¦œà§à¦à¦¾

> **Weight Initialization à¦¹à¦² neural network training à¦¶à§à¦°à§ à¦•à¦°à¦¾à¦° à¦†à¦—à§‡ à¦à¦®à¦¨à¦­à¦¾à¦¬à§‡ weight à¦¸à§‡à¦Ÿ à¦•à¦°à¦¾ à¦¯à¦¾à¦¤à§‡ forward à¦“ backward signal stable à¦¥à¦¾à¦•à§‡à¥¤**

---

>>>>>>> f45ebbad1686e699afe9932c4175eeff501d254b
