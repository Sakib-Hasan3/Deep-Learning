---

# üîπ Weight Initialization Techniques

## ‚ùì Weight Initialization ‡¶ï‡ßÄ?

Neural Network train ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶Ü‡¶ó‡ßá ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶æ neuron-‡¶è‡¶∞ weight-‡¶è **‡¶∂‡ßÅ‡¶∞‡ßÅ‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶¨‡¶∏‡¶æ‡¶®‡ßã‡¶ï‡ßá** Weight Initialization ‡¶¨‡¶≤‡ßá‡•§

‡¶≠‡ßÅ‡¶≤ initialization ‡¶π‡¶≤‡ßá:

* ‚ùå Vanishing Gradient
* ‚ùå Exploding Gradient
* ‚ùå Training slow ‡¶¨‡¶æ fail

---

## üéØ ‡¶≤‡¶ï‡ßç‡¶∑‡ßç‡¶Ø (Why important?)

Weight ‡¶è‡¶Æ‡¶® ‡¶π‡¶§‡ßá ‡¶π‡¶¨‡ßá ‡¶Ø‡ßá‡¶®:

* Gradient ‡¶ñ‡ßÅ‡¶¨ ‡¶õ‡ßã‡¶ü ‡¶®‡¶æ ‡¶π‡ßü
* Gradient ‡¶ñ‡ßÅ‡¶¨ ‡¶¨‡ßú ‡¶®‡¶æ ‡¶π‡ßü
* Signal forward & backward ‡¶¶‡ßÅ‡¶á ‡¶¶‡¶ø‡¶ï‡ßá‡¶á stable ‡¶•‡¶æ‡¶ï‡ßá

---

## üö´ ‡¶ñ‡¶æ‡¶∞‡¶æ‡¶™ ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£

### 1Ô∏è‚É£ ‡¶∏‡¶¨ weight = 0

```math
w = 0
```

‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ:

* ‡¶∏‡¶¨ neuron ‡¶è‡¶ï‡¶á output ‡¶¶‡ßá‡¶¨‡ßá
* ‡¶∏‡¶¨ gradient ‡¶è‡¶ï‡¶á ‡¶π‡¶¨‡ßá
* Network ‡¶ï‡¶ø‡¶õ‡ßÅ‡¶á ‡¶∂‡¶ø‡¶ñ‡¶¨‡ßá ‡¶®‡¶æ
  ‚û°Ô∏è **Symmetry Problem**

---

### 2Ô∏è‚É£ ‡¶ñ‡ßÅ‡¶¨ ‡¶¨‡ßú random weight

```math
w \sim \mathcal{N}(0, 10)
```

‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ:

* Activation saturate ‡¶ï‡¶∞‡ßá
* Exploding Gradient

---

## ‚úÖ ‡¶≠‡¶æ‡¶≤‡ßã Initialization Techniques

---

## 1Ô∏è‚É£ Random Initialization (Basic)

```math
w \sim \mathcal{N}(0, 0.01)
```

‚úîÔ∏è Symmetry ‡¶≠‡¶æ‡¶ô‡ßá
‚ùå Deep network-‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶≠‡¶æ‡¶≤‡ßã ‡¶®‡¶æ

---

## 2Ô∏è‚É£ Xavier / Glorot Initialization

### üéØ ‡¶ï‡¶ñ‡¶® ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞?

* **Sigmoid**
* **Tanh**

---

### üìê Formula

```math
w \sim U\left(-\sqrt{\frac{6}{n_{in}+n_{out}}},
\sqrt{\frac{6}{n_{in}+n_{out}}}\right)
```

```math
w \sim \mathcal{N}\left(0, \frac{2}{n_{in}+n_{out}}\right)
```

---

### üß† Intuition

* Forward ‡¶ì backward signal-‡¶è‡¶∞ variance ‡¶∏‡¶Æ‡¶æ‡¶® ‡¶∞‡¶æ‡¶ñ‡ßá
* Gradient neither vanish nor explode

---

### üî¢ Example

‡¶ß‡¶∞‡¶ø:

* Input neuron = 100
* Output neuron = 50

```math
\sqrt{\frac{6}{150}} = \sqrt{0.04} = 0.2
```

Weight range:
```math
[-0.2, +0.2]
```

---

## 3Ô∏è‚É£ He Initialization

### üéØ ‡¶ï‡¶ñ‡¶® ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞?

* **ReLU**
* **Leaky ReLU**
* **ELU**

---

### üìê Formula

```math
w \sim \mathcal{N}\left(0, \frac{2}{n_{in}}\right)
```

```math
w \sim U\left(-\sqrt{\frac{6}{n_{in}}},
\sqrt{\frac{6}{n_{in}}}\right)
```

---

### üß† Intuition

ReLU ‡¶Ö‡¶∞‡ßç‡¶ß‡ßá‡¶ï neuron deactivate ‡¶ï‡¶∞‡ßá (negative ‡¶Ö‡¶Ç‡¶∂)

‚û°Ô∏è ‡¶§‡¶æ‡¶á variance ‡¶¨‡¶æ‡ßú‡¶æ‡¶®‡ßã ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞

---

### üî¢ Example

‡¶ß‡¶∞‡¶ø:

* Input neuron = 100

```math
\sqrt{\frac{6}{100}} = \sqrt{0.06} \approx 0.245
```

Weight range:
```math
[-0.245, +0.245]
```

---

## 4Ô∏è‚É£ LeCun Initialization

### üéØ ‡¶ï‡¶ñ‡¶® ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞?

* **SELU**
* **Self-Normalizing Networks**

---

### üìê Formula

```math
w \sim \mathcal{N}\left(0, \frac{1}{n_{in}}\right)
```

```math
w \sim U\left(-\sqrt{\frac{3}{n_{in}}},
\sqrt{\frac{3}{n_{in}}}\right)
```

---

## 5Ô∏è‚É£ Orthogonal Initialization

```math
W^T W = I
```

‚úîÔ∏è Gradient stable ‡¶•‡¶æ‡¶ï‡ßá
‚úîÔ∏è RNN-‡¶è ‡¶≠‡¶æ‡¶≤‡ßã ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá
‚ùå Compute expensive

---

## üìä Comparison Table

| Initialization | Best For         | Activation    | Problem Solved  |
| -------------- | ---------------- | ------------- | --------------- |
| Random         | Shallow NN       | Any           | Symmetry        |
| Xavier         | Medium / Deep    | Sigmoid, Tanh | Vanishing       |
| He             | Deep NN          | ReLU family   | Vanishing       |
| LeCun          | Self-normalizing | SELU          | Stable variance |
| Orthogonal     | RNN              | Any           | Exploding       |

---

## üß† ‡¶™‡¶∞‡ßÄ‡¶ï‡ßç‡¶∑‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø Key Points

* Sigmoid/Tanh ‚Üí **Xavier**
* ReLU family ‚Üí **He**
* SELU ‚Üí **LeCun**
* RNN ‚Üí **Orthogonal**

---

## üîó Weight Init vs Gradient Problem

| Weight      | Result             |
| ----------- | ------------------ |
| Too small   | Vanishing Gradient |
| Too large   | Exploding Gradient |
| Proper init | Stable training    |

---

## ‚úçÔ∏è ‡¶è‡¶ï ‡¶≤‡¶æ‡¶á‡¶®‡ßá‡¶∞ ‡¶∏‡¶Ç‡¶ú‡ßç‡¶û‡¶æ

> **Weight Initialization ‡¶π‡¶≤ neural network training ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶Ü‡¶ó‡ßá ‡¶è‡¶Æ‡¶®‡¶≠‡¶æ‡¶¨‡ßá weight ‡¶∏‡ßá‡¶ü ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶§‡ßá forward ‡¶ì backward signal stable ‡¶•‡¶æ‡¶ï‡ßá‡•§**

---

