![Image](https://miro.medium.com/v2/resize%3Afit%3A756/1%2AtOc--h-QU9_bHqWLPY9YLA.png)

![Image](https://ronny.rest/media/blog/2017/2017_08_16_tanh/tanh_and_gradient.jpg)

![Image](https://www.researchgate.net/publication/341958919/figure/fig2/AS%3A903383875600391%401592394908731/Sigmoid-versus-tanh-functions-on-MNIST-Dataset-using-two-classic-functions.ppm)

![Image](https://i.sstatic.net/o0JA0.png)

---

## ðŸ”¹ 1) Tanh à¦•à§€?

**Tanh (Hyperbolic Tangent)** à¦¹à¦²à§‹ à¦à¦•à¦Ÿà¦¿ **non-linear activation function** à¦¯à¦¾ à¦‡à¦¨à¦ªà§à¦Ÿà¦•à§‡ **âˆ’1 à¦¥à§‡à¦•à§‡ +1** à¦°à§‡à¦žà§à¦œà§‡ à¦®à§à¦¯à¦¾à¦ª à¦•à¦°à§‡à¥¤

à¦—à¦¾à¦£à¦¿à¦¤à¦¿à¦•à¦­à¦¾à¦¬à§‡:
[
\tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}
]

---

## ðŸ”¹ 2) Output Range

[
-1 \le \tanh(x) \le +1
]

ðŸ‘‰ Output **zero-centered** (0-à¦à¦° à¦šà¦¾à¦°à¦ªà¦¾à¦¶à§‡), à¦¯à¦¾ gradient descent-à¦•à§‡ à¦¬à§‡à¦¶à¦¿ stable à¦•à¦°à§‡à¥¤

---

## ðŸ”¹ 3) Graph à¦¦à§‡à¦–à§‡ Intuition (à¦–à§à¦¬ à¦—à§à¦°à§à¦¤à§à¦¬à¦ªà§‚à¦°à§à¦£)

### (a) Left Saturation (x â‰ª 0)

* Output â‰ˆ âˆ’1
* Derivative â‰ˆ 0
* âŒ Learning à¦§à§€à¦°/à¦¬à¦¨à§à¦§

### (b) Active / Linear Zone (x â‰ˆ 0)

* Output â‰ˆ 0
* Slope à¦¸à¦¬à¦šà§‡à§Ÿà§‡ à¦¬à§‡à¦¶à¦¿
* âœ… Learning fastest

### (c) Right Saturation (x â‰« 0)

* Output â‰ˆ +1
* Derivative â‰ˆ 0
* âŒ Learning à¦§à§€à¦°

ðŸ‘‰ à¦®à¦¾à¦à¦–à¦¾à¦¨à§‡à¦° steep à¦…à¦‚à¦¶à§‡ à¦¶à§‡à¦–à¦¾ à¦¸à¦¬à¦šà§‡à§Ÿà§‡ à¦­à¦¾à¦²à§‹ à¦¹à§Ÿà¥¤

---

## ðŸ”¹ 4) Derivative (Backpropagation-à¦à¦° à¦œà¦¨à§à¦¯)

[
\frac{d}{dx}[\tanh(x)] = 1-\tanh^2(x)
]

* (x=0) à¦ derivative à¦¸à¦°à§à¦¬à§‹à¦šà§à¦š (=1)
* |x| à¦¬à§œ à¦¹à¦²à§‡ derivative â†’ 0 (saturation)

---

## ðŸ”¹ 5) à¦•à§‡à¦¨ Tanh, Sigmoid-à¦à¦° à¦šà§‡à§Ÿà§‡ à¦­à¦¾à¦²à§‹?

* **Zero-centered output** â†’ weight update balanced
* **Stronger gradient near 0** â†’ faster convergence
* Sigmoid-à¦à¦° à¦®à¦¤à§‹ à¦¹à¦²à§‡à¦“ **learning à¦¸à¦¾à¦§à¦¾à¦°à¦£à¦¤ à¦¦à§à¦°à§à¦¤**

---

## ðŸ”¹ 6) Vanishing Gradient à¦†à¦›à§‡ à¦•à¦¿?

âš ï¸ à¦¹à§à¦¯à¦¾à¦, à¦†à¦›à§‡â€”à¦•à¦¿à¦¨à§à¦¤à§ **sigmoid-à¦à¦° à¦šà§‡à§Ÿà§‡ à¦•à¦®**à¥¤
à¦•à¦¾à¦°à¦£ saturation region-à¦ derivative à¦›à§‹à¦Ÿ à¦¹à§Ÿà§‡ à¦¯à¦¾à§Ÿà¥¤

---

## ðŸ”¹ 7) Numerical Example

à¦§à¦°à¦¿ (x=1):

[
\tanh(1)\approx 0.761
]
[
\text{Derivative}=1-(0.761)^2\approx 0.42
]

ðŸ‘‰ Sigmoid-à¦à¦° à¦¤à§à¦²à¦¨à¦¾à§Ÿ à¦à¦–à¦¾à¦¨à§‡ gradient à¦¬à§‡à¦¶à¦¿à¥¤

---

## ðŸ”¹ 8) à¦•à§‹à¦¥à¦¾à§Ÿ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à¦¾ à¦¹à§Ÿ?

âœ… **Hidden layers** (shallow/medium networks)
âŒ à¦–à§à¦¬ deep network à¦¹à¦²à§‡ ReLU-family à¦­à¦¾à¦²à§‹

---

## ðŸ”¹ 9) Tanh vs Sigmoid (Graph-based à¦¤à§à¦²à¦¨à¦¾)

| Feature            | Sigmoid | Tanh   |
| ------------------ | ------- | ------ |
| Output range       | (0,1)   | (âˆ’1,1) |
| Zero-centered      | âŒ       | âœ…      |
| Learning speed     | Slower  | Faster |
| Vanishing gradient | High    | Medium |

---

## ðŸ§  Memory Trick

> **Tanh = Sigmoid but centered at zero â†’ faster learning**

---

## âœï¸ Exam-Ready One Line

> **Tanh is a zero-centered activation function with output range (âˆ’1, +1), offering faster convergence than sigmoid but still susceptible to vanishing gradients in deep networks.**
