## **Regression Cost Function â€” à¦¬à¦¾à¦‚à¦²à¦¾à§Ÿ à¦¬à§à¦¯à¦¾à¦–à§à¦¯à¦¾ (Intuition + Math + Graph)**

![Image](https://global.discourse-cdn.com/dlai/original/3X/4/3/433e18a3eb780e1e01210d38066f4763855252c1.png)

![Image](https://miro.medium.com/0%2AfcNdB994NRWt_XZ2.gif)

![Image](https://editor.analyticsvidhya.com/uploads/46591Capture5.PNG)

![Image](https://editor.analyticsvidhya.com/uploads/661483.png)

---

## ðŸ”¹ à§§) Regression Cost Function à¦•à§€?

**Regression**â€“à¦ model continuous value predict à¦•à¦°à§‡ (à¦¯à§‡à¦®à¦¨: price, height, temperature)à¥¤
**Regression Cost Function** à¦®à¦¾à¦ªà§‡â€”

> ðŸ‘‰ model-à¦à¦° prediction à¦†à¦¸à¦² à¦®à¦¾à¦¨ à¦¥à§‡à¦•à§‡ **à¦—à§œà§‡ à¦•à¦¤à¦Ÿà¦¾ à¦¦à§‚à¦°à§‡**à¥¤

Training-à¦à¦° à¦²à¦•à§à¦·à§à¦¯:

> **Cost function minimize à¦•à¦°à¦¾**

---

## ðŸ”¹ à§¨) à¦¸à¦¬à¦šà§‡à§Ÿà§‡ à¦¬à§à¦¯à¦¬à¦¹à§ƒà¦¤ Regression Cost Function: **Mean Squared Error (MSE)**

### Mathematical Formula

à¦§à¦°à¦¿,

* (y_i) = actual value
* (\hat{y}_i) = predicted value
* (N) = à¦®à§‹à¦Ÿ data point

[
J(\theta)=\frac{1}{2N}\sum_{i=1}^{N}(y_i-\hat{y}_i)^2
]

ðŸ‘‰ (\frac{1}{2}) à¦¨à§‡à¦“à§Ÿà¦¾ à¦¹à§Ÿ derivative à¦¸à¦¹à¦œ à¦•à¦°à¦¾à¦° à¦œà¦¨à§à¦¯à¥¤

---

## ðŸ”¹ à§©) Intuition (à¦¸à¦¹à¦œ à¦­à¦¾à¦·à¦¾à§Ÿ)

* Prediction à¦¯à¦¦à¦¿ à¦à¦•à¦Ÿà§ à¦­à§à¦² â†’ cost à¦›à§‹à¦Ÿ
* Prediction à¦¯à¦¦à¦¿ à¦…à¦¨à§‡à¦• à¦­à§à¦² â†’ cost à¦…à¦¨à§‡à¦• à¦¬à§œ (à¦•à¦¾à¦°à¦£ **square**)

ðŸ‘‰ à¦¬à§œ à¦­à§à¦²à¦•à§‡ à¦¬à§‡à¦¶à¦¿ à¦¶à¦¾à¦¸à§à¦¤à¦¿ à¦¦à§‡à§Ÿ
ðŸ‘‰ à¦¤à¦¾à¦‡ regression-à¦à¦° à¦œà¦¨à§à¦¯ à¦–à§à¦¬ à¦•à¦¾à¦°à§à¦¯à¦•à¦°

---

## ðŸ”¹ à§ª) Single Data Point vs Dataset

### Loss (à¦à¦•à¦Ÿà¦¾ sample):

[
\ell_i=\frac{1}{2}(y_i-\hat{y}_i)^2
]

### Cost (à¦¸à¦¬ sample):

[
J=\frac{1}{N}\sum \ell_i
]

---

## ðŸ”¹ à§«) Numerical Example

à¦§à¦°à¦¿ 3à¦Ÿà¦¾ data point:

| Actual (y) | Predicted (\hat{y}) |
| ---------- | ------------------- |
| 3          | 2                   |
| 5          | 4                   |
| 7          | 6                   |

Error:
[
(3-2)^2=1,\quad (5-4)^2=1,\quad (7-6)^2=1
]

Cost:
[
J=\frac{1}{2\times 3}(1+1+1)=\frac{3}{6}=0.5
]

---

## ðŸ”¹ à§¬) Cost Function Graph à¦¦à¦¿à§Ÿà§‡ à¦¬à§‹à¦à¦¾

* X-axis â†’ model parameters (weight, bias)
* Y-axis â†’ cost

ðŸ‘‰ Graph à¦¸à¦¾à¦§à¦¾à¦°à¦£à¦¤ **bowl / U-shape**
ðŸ‘‰ Minimum point = **best parameters**

à¦à¦Ÿà¦¾à¦‡ Gradient Descent à¦–à§à¦à¦œà§‡ à¦¬à§‡à¦° à¦•à¦°à§‡à¥¤

---

## ðŸ”¹ à§­) à¦•à§‡à¦¨ Square Error à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à¦¾ à¦¹à§Ÿ?

âœ” Smooth & differentiable
âœ” Gradient descent à¦¸à¦¹à¦œ
âœ” Large error-à¦•à§‡ à¦¬à§‡à¦¶à¦¿ à¦—à§à¦°à§à¦¤à§à¦¬ à¦¦à§‡à§Ÿ

---

## ðŸ”¹ à§®) Regression-à¦à¦° à¦…à¦¨à§à¦¯ Cost Functions (à¦¸à¦‚à¦•à§à¦·à§‡à¦ªà§‡)

| Cost Function                 | à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦°                 |
| ----------------------------- | ----------------------- |
| **MSE**                       | Standard regression     |
| **MAE** (Mean Absolute Error) | Outlier à¦•à¦® à¦—à§à¦°à§à¦¤à§à¦¬ à¦¦à¦¿à¦¤à§‡ |
| **Huber Loss**                | MSE + MAE à¦à¦° balance    |

---

## ðŸ§  à¦®à¦¨à§‡ à¦°à¦¾à¦–à¦¾à¦° à¦•à§Œà¦¶à¦²

> **Regression = difference â†’ square â†’ average**

---

## âœï¸ Exam-Ready à¦à¦• à¦²à¦¾à¦‡à¦¨

> **Regression cost function, commonly Mean Squared Error, measures the average squared difference between actual and predicted values and is minimized to train regression models.**

