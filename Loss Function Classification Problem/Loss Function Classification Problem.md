<<<<<<< HEAD
## **Classification Problem-à¦à¦° Loss Function â€” à¦¬à¦¾à¦‚à¦²à¦¾à§Ÿ à¦¸à¦¹à¦œ à¦¬à§à¦¯à¦¾à¦–à§à¦¯à¦¾ (with intuition + graph)**

![Image](https://miro.medium.com/v2/resize%3Afit%3A1012/1%2Abe2MulnLK3cyNvR3fPPwAQ.png)

![Image](https://www.researchgate.net/publication/342520628/figure/fig2/AS%3A907606478553088%401593401655516/Graph-of-Binary-Cross-Entropy-Loss-Function-Here-Entropy-is-defined-on-Y-axis-and.ppm)

![Image](https://framerusercontent.com/images/ENUqnCzJijA3zHvNJ8Wh54Zh0hg.webp?height=856\&width=1300)

![Image](https://gombru.github.io/assets/cross_entropy_loss/sigmoid.png)

---

## ğŸ”¹ à§§) Classification Loss Function à¦•à§€?

**Classification problem**â€“à¦ model à¦•à§‹à¦¨à§‹ class predict à¦•à¦°à§‡
(à¦¯à§‡à¦®à¦¨: Yes/No, Cat/Dog/Cow)à¥¤

ğŸ‘‰ **Loss function** à¦®à¦¾à¦ªà§‡:

> model-à¦à¦° prediction à¦†à¦¸à¦² class à¦¥à§‡à¦•à§‡ **à¦•à¦¤à¦Ÿà¦¾ à¦­à§à¦²**à¥¤

Training-à¦à¦° à¦²à¦•à§à¦·à§à¦¯:

> **Loss minimize à¦•à¦°à¦¾ â†’ classification à¦ à¦¿à¦• à¦•à¦°à¦¾**

---

## ğŸ”¹ à§¨) Binary Classification Loss Function

### âœ… (a) Binary Cross-Entropy Loss (Log Loss)

à¦¸à¦¬à¦šà§‡à§Ÿà§‡ à¦¬à§‡à¦¶à¦¿ à¦¬à§à¦¯à¦¬à¦¹à§ƒà¦¤ loss (Sigmoid output-à¦à¦° à¦¸à¦¾à¦¥à§‡)

#### Formula

[
L = -\big[y\log(\hat{y}) + (1-y)\log(1-\hat{y})\big]
]

à¦¯à§‡à¦–à¦¾à¦¨à§‡

* (y \in {0,1}) = true label
* (\hat{y}) = predicted probability

#### Intuition

* Wrong confident prediction â‡’ **à¦¬à§œ à¦¶à¦¾à¦¸à§à¦¤à¦¿**
* Correct confident prediction â‡’ **à¦•à¦® loss**

ğŸ‘‰ Probability-based classification-à¦à¦° à¦œà¦¨à§à¦¯ perfect

---

### ğŸ“Œ Example

True label = 1
Prediction = 0.95 â‡’ loss à¦–à§à¦¬ à¦•à¦®
Prediction = 0.05 â‡’ loss à¦–à§à¦¬ à¦¬à§‡à¦¶à¦¿ âŒ

---

## ğŸ”¹ à§©) Multiclass Classification Loss Function

### âœ… (b) Categorical Cross-Entropy Loss

**Softmax output**â€“à¦à¦° à¦¸à¦¾à¦¥à§‡ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦¹à§Ÿà¥¤

#### Formula

[
L = -\sum_{i=1}^{K} y_i \log(p_i)
]

à¦¯à§‡à¦–à¦¾à¦¨à§‡

* (K) = number of classes
* (y_i) = one-hot encoded label
* (p_i) = softmax probability

#### Intuition

* True class-à¦à¦° probability à¦¯à¦¤ à¦¬à§‡à¦¶à¦¿ â†’ loss à¦¤à¦¤ à¦•à¦®
* à¦­à§à¦² class-à¦ confidence à¦¬à§‡à¦¶à¦¿ à¦¹à¦²à§‡ â†’ loss à¦…à¦¨à§‡à¦• à¦¬à§œ

---

### ğŸ“Œ Example

Classes: Cat, Dog, Cow
True = Dog
Prediction:

* Cat 0.1, Dog 0.8, Cow 0.1 â‡’ âœ… low loss
* Cat 0.7, Dog 0.2, Cow 0.1 â‡’ âŒ high loss

---

## ğŸ”¹ à§ª) Other Classification Loss Functions

### ğŸ”¸ (c) Hinge Loss (SVM)

[
L = \max(0,; 1 - y\hat{y})
]

* Margin-based classification
* Mostly SVM-à¦ à¦¬à§à¦¯à¦¬à¦¹à§ƒà¦¤

---

### ğŸ”¸ (d) Sparse Categorical Cross-Entropy

* One-hot à¦¦à¦°à¦•à¦¾à¦° à¦¨à§‡à¦‡
* Label à¦¸à¦°à¦¾à¦¸à¦°à¦¿ class index (0,1,2)

---

## ğŸ”¹ à§«) Which Loss for Which Classification?

| Problem Type               | Output Activation | Loss Function                 |
| -------------------------- | ----------------- | ----------------------------- |
| Binary classification      | Sigmoid           | **Binary Cross-Entropy**      |
| Multiclass (single label)  | Softmax           | **Categorical Cross-Entropy** |
| Multiclass (sparse labels) | Softmax           | Sparse Categorical CE         |
| Margin-based               | Linear            | Hinge Loss                    |

---

## ğŸ”¹ à§¬) Graph à¦¦à¦¿à§Ÿà§‡ à¦¬à§‹à¦à¦¾ (Intuition)

* Cross-Entropy loss curve **steep**
  ğŸ‘‰ wrong confident prediction-à¦ à¦¬à§œ penalty
* à¦¤à¦¾à¦‡ model à¦¦à§à¦°à§à¦¤ à¦¶à§‡à¦–à§‡
* Accuracy-à¦à¦° à¦šà§‡à§Ÿà§‡ loss à¦¬à§‡à¦¶à¦¿ informative

---

## ğŸ§  à¦®à¦¨à§‡ à¦°à¦¾à¦–à¦¾à¦° à¦•à§Œà¦¶à¦²

> **Classification = Probability â†’ Cross-Entropy**

---

## âœï¸ Exam-Ready à¦à¦• à¦²à¦¾à¦‡à¦¨

> **In classification problems, loss functions like cross-entropy measure the difference between true class labels and predicted probabilities, guiding the model to improve classification accuracy.**

---

=======
## **Classification Problem-à¦à¦° Loss Function â€” à¦¬à¦¾à¦‚à¦²à¦¾à§Ÿ à¦¸à¦¹à¦œ à¦¬à§à¦¯à¦¾à¦–à§à¦¯à¦¾ (with intuition + graph)**

![Image](https://miro.medium.com/v2/resize%3Afit%3A1012/1%2Abe2MulnLK3cyNvR3fPPwAQ.png)

![Image](https://www.researchgate.net/publication/342520628/figure/fig2/AS%3A907606478553088%401593401655516/Graph-of-Binary-Cross-Entropy-Loss-Function-Here-Entropy-is-defined-on-Y-axis-and.ppm)

![Image](https://framerusercontent.com/images/ENUqnCzJijA3zHvNJ8Wh54Zh0hg.webp?height=856\&width=1300)

![Image](https://gombru.github.io/assets/cross_entropy_loss/sigmoid.png)

---

## ğŸ”¹ à§§) Classification Loss Function à¦•à§€?

**Classification problem**â€“à¦ model à¦•à§‹à¦¨à§‹ class predict à¦•à¦°à§‡
(à¦¯à§‡à¦®à¦¨: Yes/No, Cat/Dog/Cow)à¥¤

ğŸ‘‰ **Loss function** à¦®à¦¾à¦ªà§‡:

> model-à¦à¦° prediction à¦†à¦¸à¦² class à¦¥à§‡à¦•à§‡ **à¦•à¦¤à¦Ÿà¦¾ à¦­à§à¦²**à¥¤

Training-à¦à¦° à¦²à¦•à§à¦·à§à¦¯:

> **Loss minimize à¦•à¦°à¦¾ â†’ classification à¦ à¦¿à¦• à¦•à¦°à¦¾**

---

## ğŸ”¹ à§¨) Binary Classification Loss Function

### âœ… (a) Binary Cross-Entropy Loss (Log Loss)

à¦¸à¦¬à¦šà§‡à§Ÿà§‡ à¦¬à§‡à¦¶à¦¿ à¦¬à§à¦¯à¦¬à¦¹à§ƒà¦¤ loss (Sigmoid output-à¦à¦° à¦¸à¦¾à¦¥à§‡)

#### Formula

[
L = -\big[y\log(\hat{y}) + (1-y)\log(1-\hat{y})\big]
]

à¦¯à§‡à¦–à¦¾à¦¨à§‡

* (y \in {0,1}) = true label
* (\hat{y}) = predicted probability

#### Intuition

* Wrong confident prediction â‡’ **à¦¬à§œ à¦¶à¦¾à¦¸à§à¦¤à¦¿**
* Correct confident prediction â‡’ **à¦•à¦® loss**

ğŸ‘‰ Probability-based classification-à¦à¦° à¦œà¦¨à§à¦¯ perfect

---

### ğŸ“Œ Example

True label = 1
Prediction = 0.95 â‡’ loss à¦–à§à¦¬ à¦•à¦®
Prediction = 0.05 â‡’ loss à¦–à§à¦¬ à¦¬à§‡à¦¶à¦¿ âŒ

---

## ğŸ”¹ à§©) Multiclass Classification Loss Function

### âœ… (b) Categorical Cross-Entropy Loss

**Softmax output**â€“à¦à¦° à¦¸à¦¾à¦¥à§‡ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦¹à§Ÿà¥¤

#### Formula

[
L = -\sum_{i=1}^{K} y_i \log(p_i)
]

à¦¯à§‡à¦–à¦¾à¦¨à§‡

* (K) = number of classes
* (y_i) = one-hot encoded label
* (p_i) = softmax probability

#### Intuition

* True class-à¦à¦° probability à¦¯à¦¤ à¦¬à§‡à¦¶à¦¿ â†’ loss à¦¤à¦¤ à¦•à¦®
* à¦­à§à¦² class-à¦ confidence à¦¬à§‡à¦¶à¦¿ à¦¹à¦²à§‡ â†’ loss à¦…à¦¨à§‡à¦• à¦¬à§œ

---

### ğŸ“Œ Example

Classes: Cat, Dog, Cow
True = Dog
Prediction:

* Cat 0.1, Dog 0.8, Cow 0.1 â‡’ âœ… low loss
* Cat 0.7, Dog 0.2, Cow 0.1 â‡’ âŒ high loss

---

## ğŸ”¹ à§ª) Other Classification Loss Functions

### ğŸ”¸ (c) Hinge Loss (SVM)

[
L = \max(0,; 1 - y\hat{y})
]

* Margin-based classification
* Mostly SVM-à¦ à¦¬à§à¦¯à¦¬à¦¹à§ƒà¦¤

---

### ğŸ”¸ (d) Sparse Categorical Cross-Entropy

* One-hot à¦¦à¦°à¦•à¦¾à¦° à¦¨à§‡à¦‡
* Label à¦¸à¦°à¦¾à¦¸à¦°à¦¿ class index (0,1,2)

---

## ğŸ”¹ à§«) Which Loss for Which Classification?

| Problem Type               | Output Activation | Loss Function                 |
| -------------------------- | ----------------- | ----------------------------- |
| Binary classification      | Sigmoid           | **Binary Cross-Entropy**      |
| Multiclass (single label)  | Softmax           | **Categorical Cross-Entropy** |
| Multiclass (sparse labels) | Softmax           | Sparse Categorical CE         |
| Margin-based               | Linear            | Hinge Loss                    |

---

## ğŸ”¹ à§¬) Graph à¦¦à¦¿à§Ÿà§‡ à¦¬à§‹à¦à¦¾ (Intuition)

* Cross-Entropy loss curve **steep**
  ğŸ‘‰ wrong confident prediction-à¦ à¦¬à§œ penalty
* à¦¤à¦¾à¦‡ model à¦¦à§à¦°à§à¦¤ à¦¶à§‡à¦–à§‡
* Accuracy-à¦à¦° à¦šà§‡à§Ÿà§‡ loss à¦¬à§‡à¦¶à¦¿ informative

---

## ğŸ§  à¦®à¦¨à§‡ à¦°à¦¾à¦–à¦¾à¦° à¦•à§Œà¦¶à¦²

> **Classification = Probability â†’ Cross-Entropy**

---

## âœï¸ Exam-Ready à¦à¦• à¦²à¦¾à¦‡à¦¨

> **In classification problems, loss functions like cross-entropy measure the difference between true class labels and predicted probabilities, guiding the model to improve classification accuracy.**

---

>>>>>>> f45ebbad1686e699afe9932c4175eeff501d254b
