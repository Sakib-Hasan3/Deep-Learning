<<<<<<< HEAD
---

## 1ï¸âƒ£ ANN (Artificial Neural Network) à¦•à§€?

### ğŸ§± ANN Architecture

ANN à¦¸à¦¾à¦§à¦¾à¦°à¦£à¦¤ **Feedforward Network**:

```
Input â†’ Hidden Layer â†’ Output
```

ğŸ“Œ à¦¬à§ˆà¦¶à¦¿à¦·à§à¦Ÿà§à¦¯

* à¦à¦•à¦¬à¦¾à¦°à§‡ à¦ªà§à¦°à§‹ input à¦¨à§‡à§Ÿ
* à¦•à§‹à¦¨à§‹ **memory à¦¨à§‡à¦‡**
* à¦†à¦—à§‡à¦° input à¦®à¦¨à§‡ à¦°à¦¾à¦–à§‡ à¦¨à¦¾

### ğŸ” ANN à¦•à§€à¦­à¦¾à¦¬à§‡ à¦•à¦¾à¦œ à¦•à¦°à§‡?

à¦§à¦°à§‹ à¦‡à¦¨à¦ªà§à¦Ÿ:

```
[5, 7, 3]
```

ANN à¦à¦•à¦¸à¦¾à¦¥à§‡ à¦¸à¦¬ à¦‡à¦¨à¦ªà§à¦Ÿ à¦ªà§à¦°à¦¸à§‡à¦¸ à¦•à¦°à§‡
ğŸ‘‰ Output à¦¦à§‡à§Ÿ
ğŸ‘‰ à¦à¦°à¦ªà¦° à¦¸à¦¬ à¦­à§à¦²à§‡ à¦¯à¦¾à§Ÿ ğŸ˜„

---

## ANN à¦•à§‹à¦¥à¦¾à§Ÿ à¦­à¦¾à¦²à§‹?

âœ” Image classification
âœ” Tabular data
âœ” Fixed-size input

âŒ Sequence data (text, speech)

---

## 2ï¸âƒ£ RNN (Recurrent Neural Network) à¦•à§€?

### ğŸ”„ RNN Architecture

RNN à¦à¦° à¦­à¦¿à¦¤à¦°à§‡ à¦à¦•à¦Ÿà¦¾ **loop** à¦¥à¦¾à¦•à§‡ ğŸ”

```
xâ‚ â†’ [RNN] â†’ hâ‚
xâ‚‚ â†’ [RNN] â†’ hâ‚‚
xâ‚ƒ â†’ [RNN] â†’ hâ‚ƒ
```

ğŸ“Œ à¦à¦–à¦¾à¦¨à§‡

* hâ‚œ = hidden state (memory)
* hâ‚œ à¦†à¦—à§‡à¦° hâ‚œâ‚‹â‚ à¦®à¦¨à§‡ à¦°à¦¾à¦–à§‡

---

## RNN Architecture (Inside View)

à¦à¦•à¦Ÿà¦¾ time step à¦:

```
      h(t-1)
        â†‘
x(t) â†’ [RNN Cell] â†’ h(t)
```

ğŸ‘‰ à¦¬à¦°à§à¦¤à¦®à¦¾à¦¨ input + à¦†à¦—à§‡à¦° memory
ğŸ‘‰ à¦¨à¦¤à§à¦¨ memory à¦¤à§ˆà¦°à¦¿

---

## RNN à¦à¦° Core Equation

[
h_t = \tanh(W_x x_t + W_h h_{t-1} + b)
]

ğŸ‘‰ à¦à¦Ÿà¦¾à¦•à§‡à¦‡ RNN â€œmemoryâ€ à¦¬à¦²à¦¾ à¦¹à§Ÿ

---

## Real-life Example ğŸ§ 

### Sentence:

```
"à¦†à¦œ à¦†à¦•à¦¾à¦¶ à¦–à§à¦¬ à¦¸à§à¦¨à§à¦¦à¦°"
```

### ANN à¦¹à¦²à§‡:

* "à¦†à¦œ", "à¦†à¦•à¦¾à¦¶", "à¦–à§à¦¬", "à¦¸à§à¦¨à§à¦¦à¦°" à¦à¦•à¦¸à¦¾à¦¥à§‡ à¦¦à§‡à¦–à§‡
* word order à¦¬à§à¦à¦¤à§‡ à¦ªà¦¾à¦°à§‡ à¦¨à¦¾

### RNN à¦¹à¦²à§‡:

* à¦†à¦œ â†’ à¦†à¦•à¦¾à¦¶ â†’ à¦–à§à¦¬ â†’ à¦¸à§à¦¨à§à¦¦à¦°
* sequence à¦§à¦°à§‡ à¦§à¦°à§‡ à¦¬à§à¦à§‡
* "à¦–à§à¦¬ à¦¸à§à¦¨à§à¦¦à¦°" context à¦§à¦°à§‡ à¦°à¦¾à¦–à§‡

---

## 3ï¸âƒ£ RNN vs ANN (Side-by-Side)

| Feature       | ANN        | RNN           |
| ------------- | ---------- | ------------- |
| Input type    | Fixed size | Sequence      |
| Memory        | âŒ à¦¨à§‡à¦‡      | âœ… à¦†à¦›à§‡         |
| Order matters | âŒ à¦¨à¦¾       | âœ… à¦¹à§à¦¯à¦¾à¦       |
| Same weights  | âŒ          | âœ… (time-wise) |
| Text / Speech | âŒ à¦¦à§à¦°à§à¦¬à¦²   | âœ… à¦­à¦¾à¦²à§‹        |

---

## Architecture Difference (Conceptually)

### ANN

```
x1   x2   x3
 |    |    |
 v    v    v
[  Hidden Layer ]
        |
      Output
```

### RNN (Unrolled)

```
x1 â†’ h1 â†’ y1
x2 â†’ h2 â†’ y2
x3 â†’ h3 â†’ y3
```

ğŸ‘‰ à¦à¦–à¦¾à¦¨à§‡ **same RNN cell à¦¬à¦¾à¦°à¦¬à¦¾à¦° reuse à¦¹à§Ÿ**

---

## Key Concept: Weight Sharing

RNNâ€“à¦:

* à¦à¦•à¦‡ weight à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾ time step à¦ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦¹à§Ÿ
* ANNâ€“à¦ à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾ layer à¦†à¦²à¦¾à¦¦à¦¾ weight

---

## à¦•à§‡à¦¨ ANN sequence à¦ fail à¦•à¦°à§‡?

Sentence:

```
"à¦¸à§‡ à¦†à¦®à¦¾à¦•à§‡ à¦®à¦¾à¦°à§‡à¦¨à¦¿"
```

ANN à¦¶à§à¦§à§ à¦¶à¦¬à§à¦¦ à¦¦à§‡à¦–à§‡
âŒ context + order miss à¦•à¦°à§‡

RNN à¦œà¦¾à¦¨à§‡:

* "à¦®à¦¾à¦°à§‡à¦¨à¦¿" à¦¶à§‡à¦·à§‡ à¦à¦¸à§‡à¦›à§‡
* negation à¦¬à§à¦à¦¤à§‡ à¦ªà¦¾à¦°à§‡

---

## RNN à¦à¦° Limitation

* Long sentence â†’ memory fade
* Vanishing gradient

ğŸ‘‰ Solution:

* **LSTM**
* **GRU**
* **Transformer**

---

## Quick Intuition Summary ğŸ§©

* ANN = â€œà¦à¦•à¦¬à¦¾à¦°à§‡ à¦¦à§‡à¦–à§‡ à¦¬à¦¿à¦šà¦¾à¦°â€
* RNN = â€œà¦à¦•à¦Ÿà¦¾à¦° à¦ªà¦° à¦à¦•à¦Ÿà¦¾ à¦¦à§‡à¦–à§‡ à¦®à¦¨à§‡ à¦°à§‡à¦–à§‡ à¦¬à¦¿à¦šà¦¾à¦°â€

---


=======
---

## 1ï¸âƒ£ ANN (Artificial Neural Network) à¦•à§€?

### ğŸ§± ANN Architecture

ANN à¦¸à¦¾à¦§à¦¾à¦°à¦£à¦¤ **Feedforward Network**:

```
Input â†’ Hidden Layer â†’ Output
```

ğŸ“Œ à¦¬à§ˆà¦¶à¦¿à¦·à§à¦Ÿà§à¦¯

* à¦à¦•à¦¬à¦¾à¦°à§‡ à¦ªà§à¦°à§‹ input à¦¨à§‡à§Ÿ
* à¦•à§‹à¦¨à§‹ **memory à¦¨à§‡à¦‡**
* à¦†à¦—à§‡à¦° input à¦®à¦¨à§‡ à¦°à¦¾à¦–à§‡ à¦¨à¦¾

### ğŸ” ANN à¦•à§€à¦­à¦¾à¦¬à§‡ à¦•à¦¾à¦œ à¦•à¦°à§‡?

à¦§à¦°à§‹ à¦‡à¦¨à¦ªà§à¦Ÿ:

```
[5, 7, 3]
```

ANN à¦à¦•à¦¸à¦¾à¦¥à§‡ à¦¸à¦¬ à¦‡à¦¨à¦ªà§à¦Ÿ à¦ªà§à¦°à¦¸à§‡à¦¸ à¦•à¦°à§‡
ğŸ‘‰ Output à¦¦à§‡à§Ÿ
ğŸ‘‰ à¦à¦°à¦ªà¦° à¦¸à¦¬ à¦­à§à¦²à§‡ à¦¯à¦¾à§Ÿ ğŸ˜„

---

## ANN à¦•à§‹à¦¥à¦¾à§Ÿ à¦­à¦¾à¦²à§‹?

âœ” Image classification
âœ” Tabular data
âœ” Fixed-size input

âŒ Sequence data (text, speech)

---

## 2ï¸âƒ£ RNN (Recurrent Neural Network) à¦•à§€?

### ğŸ”„ RNN Architecture

RNN à¦à¦° à¦­à¦¿à¦¤à¦°à§‡ à¦à¦•à¦Ÿà¦¾ **loop** à¦¥à¦¾à¦•à§‡ ğŸ”

```
xâ‚ â†’ [RNN] â†’ hâ‚
xâ‚‚ â†’ [RNN] â†’ hâ‚‚
xâ‚ƒ â†’ [RNN] â†’ hâ‚ƒ
```

ğŸ“Œ à¦à¦–à¦¾à¦¨à§‡

* hâ‚œ = hidden state (memory)
* hâ‚œ à¦†à¦—à§‡à¦° hâ‚œâ‚‹â‚ à¦®à¦¨à§‡ à¦°à¦¾à¦–à§‡

---

## RNN Architecture (Inside View)

à¦à¦•à¦Ÿà¦¾ time step à¦:

```
      h(t-1)
        â†‘
x(t) â†’ [RNN Cell] â†’ h(t)
```

ğŸ‘‰ à¦¬à¦°à§à¦¤à¦®à¦¾à¦¨ input + à¦†à¦—à§‡à¦° memory
ğŸ‘‰ à¦¨à¦¤à§à¦¨ memory à¦¤à§ˆà¦°à¦¿

---

## RNN à¦à¦° Core Equation

[
h_t = \tanh(W_x x_t + W_h h_{t-1} + b)
]

ğŸ‘‰ à¦à¦Ÿà¦¾à¦•à§‡à¦‡ RNN â€œmemoryâ€ à¦¬à¦²à¦¾ à¦¹à§Ÿ

---

## Real-life Example ğŸ§ 

### Sentence:

```
"à¦†à¦œ à¦†à¦•à¦¾à¦¶ à¦–à§à¦¬ à¦¸à§à¦¨à§à¦¦à¦°"
```

### ANN à¦¹à¦²à§‡:

* "à¦†à¦œ", "à¦†à¦•à¦¾à¦¶", "à¦–à§à¦¬", "à¦¸à§à¦¨à§à¦¦à¦°" à¦à¦•à¦¸à¦¾à¦¥à§‡ à¦¦à§‡à¦–à§‡
* word order à¦¬à§à¦à¦¤à§‡ à¦ªà¦¾à¦°à§‡ à¦¨à¦¾

### RNN à¦¹à¦²à§‡:

* à¦†à¦œ â†’ à¦†à¦•à¦¾à¦¶ â†’ à¦–à§à¦¬ â†’ à¦¸à§à¦¨à§à¦¦à¦°
* sequence à¦§à¦°à§‡ à¦§à¦°à§‡ à¦¬à§à¦à§‡
* "à¦–à§à¦¬ à¦¸à§à¦¨à§à¦¦à¦°" context à¦§à¦°à§‡ à¦°à¦¾à¦–à§‡

---

## 3ï¸âƒ£ RNN vs ANN (Side-by-Side)

| Feature       | ANN        | RNN           |
| ------------- | ---------- | ------------- |
| Input type    | Fixed size | Sequence      |
| Memory        | âŒ à¦¨à§‡à¦‡      | âœ… à¦†à¦›à§‡         |
| Order matters | âŒ à¦¨à¦¾       | âœ… à¦¹à§à¦¯à¦¾à¦       |
| Same weights  | âŒ          | âœ… (time-wise) |
| Text / Speech | âŒ à¦¦à§à¦°à§à¦¬à¦²   | âœ… à¦­à¦¾à¦²à§‹        |

---

## Architecture Difference (Conceptually)

### ANN

```
x1   x2   x3
 |    |    |
 v    v    v
[  Hidden Layer ]
        |
      Output
```

### RNN (Unrolled)

```
x1 â†’ h1 â†’ y1
x2 â†’ h2 â†’ y2
x3 â†’ h3 â†’ y3
```

ğŸ‘‰ à¦à¦–à¦¾à¦¨à§‡ **same RNN cell à¦¬à¦¾à¦°à¦¬à¦¾à¦° reuse à¦¹à§Ÿ**

---

## Key Concept: Weight Sharing

RNNâ€“à¦:

* à¦à¦•à¦‡ weight à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾ time step à¦ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦¹à§Ÿ
* ANNâ€“à¦ à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾ layer à¦†à¦²à¦¾à¦¦à¦¾ weight

---

## à¦•à§‡à¦¨ ANN sequence à¦ fail à¦•à¦°à§‡?

Sentence:

```
"à¦¸à§‡ à¦†à¦®à¦¾à¦•à§‡ à¦®à¦¾à¦°à§‡à¦¨à¦¿"
```

ANN à¦¶à§à¦§à§ à¦¶à¦¬à§à¦¦ à¦¦à§‡à¦–à§‡
âŒ context + order miss à¦•à¦°à§‡

RNN à¦œà¦¾à¦¨à§‡:

* "à¦®à¦¾à¦°à§‡à¦¨à¦¿" à¦¶à§‡à¦·à§‡ à¦à¦¸à§‡à¦›à§‡
* negation à¦¬à§à¦à¦¤à§‡ à¦ªà¦¾à¦°à§‡

---

## RNN à¦à¦° Limitation

* Long sentence â†’ memory fade
* Vanishing gradient

ğŸ‘‰ Solution:

* **LSTM**
* **GRU**
* **Transformer**

---

## Quick Intuition Summary ğŸ§©

* ANN = â€œà¦à¦•à¦¬à¦¾à¦°à§‡ à¦¦à§‡à¦–à§‡ à¦¬à¦¿à¦šà¦¾à¦°â€
* RNN = â€œà¦à¦•à¦Ÿà¦¾à¦° à¦ªà¦° à¦à¦•à¦Ÿà¦¾ à¦¦à§‡à¦–à§‡ à¦®à¦¨à§‡ à¦°à§‡à¦–à§‡ à¦¬à¦¿à¦šà¦¾à¦°â€

---


>>>>>>> f45ebbad1686e699afe9932c4175eeff501d254b
