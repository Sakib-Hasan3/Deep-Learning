<<<<<<< HEAD
---

## ЁЯФ┤ RNN-ржПрж░ ржкрзНрж░ржзрж╛ржи рж╕ржорж╕рзНржпрж╛ржЧрзБрж▓рзЛ (Easy but Detailed)

![Image](https://cdn.prod.website-files.com/5ef788f07804fb7d78a4127a/6245a9aca7defe61cea5ea7d_Engati-vanishing-point-problem.jpg)

![Image](https://miro.medium.com/1%2AAOwXWfBegd-qlr2RV_YmDg.png)

![Image](https://ai-master.gitbooks.io/recurrent-neural-network/content/assets/RNN_connection.jpg)

![Image](https://www.researchgate.net/publication/355764482/figure/fig2/AS%3A1161618813657097%401653962915912/The-long-term-dependency-problem-a-severe-problem-of-RNN-like-models-in-dealing-with.png)

---

## 1я╕ПтГг Vanishing Gradient Problem (рж╕ржмржЪрзЗрзЯрзЗ ржмрзЬ рж╕ржорж╕рзНржпрж╛)

### ЁЯФН ржХрзА рж╕ржорж╕рзНржпрж╛?

Training-ржПрж░ рж╕ржорзЯ **gradient ржзрзАрж░рзЗ ржзрзАрж░рзЗ ржПржд ржЫрзЛржЯ рж╣рзЯрзЗ ржпрж╛рзЯ ржпрзЗ ржкрзНрж░рж╛рзЯ рж╢рзВржирзНржп рж╣рзЯрзЗ ржпрж╛рзЯ**ред

ЁЯСЙ ржлрж▓рзЗ:

* ржЖржЧрзЗрж░ (old) time step-ржПрж░ weight ржЖржкржбрзЗржЯ рж╣рзЯ ржирж╛
* RNN **ржжрзВрж░рзЗрж░ past рждржерзНржп рж╢рзЗржЦрзЗ ржирж╛**

---

### ЁЯза ржХрзЗржи рж╣рзЯ?

BPTT-рждрзЗ gradient ржмрж╛рж░ржмрж╛рж░ ржЧрзБржг рж╣рзЯ:


$$
	ext{gradient} \propto (W_{hh})^t
$$

ржпржжрж┐

* $W_{hh} < 1$
  рждрж╛рж╣рж▓рзЗ ржмрж╛рж░ржмрж╛рж░ ржЧрзБржг ржХрж░рж▓рзЗ тЖТ **gradient тЖТ 0**

---

### ЁЯУМ рж╕рж╣ржЬ ржЙржжрж╛рж╣рж░ржг

ржмрж╛ржХрзНржп:

> *тАЬI grew up in France тАж I speak fluent ___тАЭ*

RNN-ржПрж░ ржЙржЪрж┐ржд **France тЖТ French** ржоржирзЗ рж░рж╛ржЦрж╛
ржХрж┐ржирзНрждрзБ vanishing gradient ржПрж░ ржХрж╛рж░ржгрзЗ RNN ржнрзБрж▓рзЗ ржпрж╛рзЯред

---

### тЭМ ржлрж▓рж╛ржлрж▓

* Long-term dependency рж╢рзЗржЦрзЗ ржирж╛
* Language model, translation, long text-ржП ржЦрж╛рж░рж╛ржк performance

---

## 2я╕ПтГг Exploding Gradient Problem

### ЁЯФН ржХрзА рж╕ржорж╕рзНржпрж╛?

Gradient рж╣ржарж╛рзО **ржЕрж╕рзНржмрж╛ржнрж╛ржмрж┐ржХ ржмрзЬ рж╣рзЯрзЗ ржпрж╛рзЯ**ред

---

### ЁЯза ржХрзЗржи рж╣рзЯ?

ржпржжрж┐

* $W_{hh} > 1$
  рждрж╛рж╣рж▓рзЗ ржмрж╛рж░ржмрж╛рж░ ржЧрзБржг тЖТ **gradient тЖТ тИЮ**

---

### тЭМ ржлрж▓рж╛ржлрж▓

* Weight ржЦрзБржм ржмрзЬ рж╣рзЯрзЗ ржпрж╛рзЯ
* Loss тЖТ NaN / тИЮ
* Training unstable, model ржнрзЗржЩрзЗ ржкрзЬрзЗ

---

### ЁЯФз Common Fix

* **Gradient clipping**
  (gradient ржХрзЗ ржПржХржЯрж┐ max limit-ржП ржЖржЯржХрзЗ ржжрзЗржУрзЯрж╛)

---

## 3я╕ПтГг Long-Term Dependency Problem

### ЁЯФН ржХрзА рж╕ржорж╕рзНржпрж╛?

RNN **ржЕржирзЗржХ ржЖржЧрзЗрж░ рждржерзНржп ржоржирзЗ рж░рж╛ржЦрждрзЗ ржкрж╛рж░рзЗ ржирж╛**ред

---

### ЁЯза ржХрзЗржи рж╣рзЯ?

* Hidden state ржПржХржЯрж╛ржЗ memory
* ржкрзНрж░рждрж┐ржмрж╛рж░ ржирждрзБржи input ржПрж▓рзЗ ржкрзБрж░рзЛржирзЛ рждржерзНржп overwrite рж╣рзЯ
* Vanishing gradient ржПржЯрж╛ржХрзЗ ржЖрж░ржУ ржЦрж╛рж░рж╛ржк ржХрж░рзЗ

---

### ЁЯУМ ржЙржжрж╛рж╣рж░ржг

Time series:

> Stock price today depends on event 30 days ago

Simple RNN:

* last 3тАУ5 step ржоржирзЗ рж░рж╛ржЦрзЗ
* 30 step ржЖржЧрзЗрж░ рждржерзНржп рж╣рж╛рж░рж╛рзЯ

---

## 4я╕ПтГг Sequential Computation (Slow Training)

### ЁЯФН ржХрзА рж╕ржорж╕рзНржпрж╛?

RNN **parallel-ржП train ржХрж░рж╛ ржпрж╛рзЯ ржирж╛**ред

---

### ЁЯза ржХрзЗржи?


$$
h_t \text{ depends on } h_{t-1}
$$

ржорж╛ржирзЗ:

* ржЖржЧрзЗ $h_1$
* рждрж╛рж░ржкрж░ $h_2$
* рждрж╛рж░ржкрж░ $h_3$

ЁЯСЙ ржПржХржЯрж╛рж░ ржкрж░ ржПржХржЯрж╛ (serial)

---

### тЭМ ржлрж▓рж╛ржлрж▓

* Training ржЦрзБржм slow
* GPU-рж░ ржкрзБрж░рзЛ ржХрзНрж╖ржорждрж╛ ржмрзНржпржмрж╣рж╛рж░ рж╣рзЯ ржирж╛
* Long sequence тЖТ ржЦрзБржм рж╕ржорзЯ рж▓рж╛ржЧрзЗ

---

## 5я╕ПтГг Memory Bottleneck (Single Hidden State)

### ЁЯФН ржХрзА рж╕ржорж╕рзНржпрж╛?

RNN-ржПрж░ ржкрзБрж░рзЛ memory ржерж╛ржХрзЗ **ржПржХржЯрж╛ hidden state-ржП**ред

---

### ЁЯза ржХрзЗржи рж╕ржорж╕рзНржпрж╛?

* Complex sequence-ржПрж░ ржЬржирзНржп memory ржпржерзЗрж╖рзНржЯ ржирж╛
* Important + unimportant рж╕ржм рждржерзНржп ржПржХ ржЬрж╛рзЯржЧрж╛рзЯ ржЬржорзЗ

---

### ЁЯУМ рждрзБрж▓ржирж╛

* RNN = ржЫрзЛржЯ ржирзЛржЯржмрзБржХ
* LSTM = organized diary (gate system)

---

## 6я╕ПтГг Bias Toward Recent Inputs

### ЁЯФН ржХрзА рж╕ржорж╕рзНржпрж╛?

RNN **recent input-ржХрзЗ ржмрзЗрж╢рж┐ ржЧрзБрж░рзБрждрзНржм ржжрзЗрзЯ**ред

---

### ЁЯза ржХрзЗржи?

* Recent gradient рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзА
* Old gradient ржжрзБрж░рзНржмрж▓ (vanishing)

---

### тЭМ ржлрж▓рж╛ржлрж▓

* Context skewed
* Early information ignored

---

## 7я╕ПтГг Difficult to Tune & Unstable Training

### ЁЯФН ржХрзА рж╕ржорж╕рзНржпрж╛?

* Learning rate sensitive
* Initialization ржнрзБрж▓ рж╣рж▓рзЗ training fail
* Activation function choice critical

---

### тЭМ ржлрж▓рж╛ржлрж▓

* Beginners-ржПрж░ ржЬржирзНржп ржХржарж┐ржи
* Production-ржП risky

---

## ЁЯФЪ рж╕ржм рж╕ржорж╕рзНржпрж╛рж░ ржПржХ ржиржЬрж░рзЗ рж╕рж╛рж░рж╛ржВрж╢

| Problem              | ржХрзА рж╣рзЯ               | ржХрзЗржи ржЦрж╛рж░рж╛ржк                |
| -------------------- | ------------------- | ------------------------ |
| Vanishing Gradient   | Gradient тЖТ 0        | Past рж╢рзЗржЦрзЗ ржирж╛             |
| Exploding Gradient   | Gradient тЖТ тИЮ        | Training unstable        |
| Long-term dependency | ржжрзВрж░рзЗрж░ рждржерзНржп ржнрзБрж▓рзЗ ржпрж╛рзЯ | Context loss             |
| Sequential training  | Slow                | Large data-ржП impractical |
| Single memory        | Capacity ржХржо         | Complex task fail        |
| Recent bias          | Old info ignore     | Wrong prediction         |

---

## тЬЕ ржПрж╕ржм рж╕ржорж╕рзНржпрж╛рж░ рж╕ржорж╛ржзрж╛ржи ржХрзА?

ржПржЗ рж╕ржорж╕рзНржпрж╛ржЧрзБрж▓рзЛрж░ ржЬржирзНржпржЗ ржПрж╕рзЗржЫрзЗ:

* **LSTM (Long Short-Term Memory)**
* **GRU**
* ржкрж░рзЗ тЖТ **Transformer**

---

## ЁЯза ржПржХ рж▓рж╛ржЗржирзЗрж░ takeaway

**Simple RNN conceptually рж╕рж╣ржЬ, ржХрж┐ржирзНрждрзБ long sequence рж╢рзЗржЦрж╛рж░ ржЬржирзНржп ржжрзБрж░рзНржмрж▓тАФgradient, memory ржУ speed-ржПрж░ рж╕рзАржорж╛ржмржжрзНржзрждрж╛рж░ ржХрж╛рж░ржгрзЗред**

---

=======
---

## ЁЯФ┤ RNN-ржПрж░ ржкрзНрж░ржзрж╛ржи рж╕ржорж╕рзНржпрж╛ржЧрзБрж▓рзЛ (Easy but Detailed)

![Image](https://cdn.prod.website-files.com/5ef788f07804fb7d78a4127a/6245a9aca7defe61cea5ea7d_Engati-vanishing-point-problem.jpg)

![Image](https://miro.medium.com/1%2AAOwXWfBegd-qlr2RV_YmDg.png)

![Image](https://ai-master.gitbooks.io/recurrent-neural-network/content/assets/RNN_connection.jpg)

![Image](https://www.researchgate.net/publication/355764482/figure/fig2/AS%3A1161618813657097%401653962915912/The-long-term-dependency-problem-a-severe-problem-of-RNN-like-models-in-dealing-with.png)

---

## 1я╕ПтГг Vanishing Gradient Problem (рж╕ржмржЪрзЗрзЯрзЗ ржмрзЬ рж╕ржорж╕рзНржпрж╛)

### ЁЯФН ржХрзА рж╕ржорж╕рзНржпрж╛?

Training-ржПрж░ рж╕ржорзЯ **gradient ржзрзАрж░рзЗ ржзрзАрж░рзЗ ржПржд ржЫрзЛржЯ рж╣рзЯрзЗ ржпрж╛рзЯ ржпрзЗ ржкрзНрж░рж╛рзЯ рж╢рзВржирзНржп рж╣рзЯрзЗ ржпрж╛рзЯ**ред

ЁЯСЙ ржлрж▓рзЗ:

* ржЖржЧрзЗрж░ (old) time step-ржПрж░ weight ржЖржкржбрзЗржЯ рж╣рзЯ ржирж╛
* RNN **ржжрзВрж░рзЗрж░ past рждржерзНржп рж╢рзЗржЦрзЗ ржирж╛**

---

### ЁЯза ржХрзЗржи рж╣рзЯ?

BPTT-рждрзЗ gradient ржмрж╛рж░ржмрж╛рж░ ржЧрзБржг рж╣рзЯ:


$$
	ext{gradient} \propto (W_{hh})^t
$$

ржпржжрж┐

* $W_{hh} < 1$
  рждрж╛рж╣рж▓рзЗ ржмрж╛рж░ржмрж╛рж░ ржЧрзБржг ржХрж░рж▓рзЗ тЖТ **gradient тЖТ 0**

---

### ЁЯУМ рж╕рж╣ржЬ ржЙржжрж╛рж╣рж░ржг

ржмрж╛ржХрзНржп:

> *тАЬI grew up in France тАж I speak fluent ___тАЭ*

RNN-ржПрж░ ржЙржЪрж┐ржд **France тЖТ French** ржоржирзЗ рж░рж╛ржЦрж╛
ржХрж┐ржирзНрждрзБ vanishing gradient ржПрж░ ржХрж╛рж░ржгрзЗ RNN ржнрзБрж▓рзЗ ржпрж╛рзЯред

---

### тЭМ ржлрж▓рж╛ржлрж▓

* Long-term dependency рж╢рзЗржЦрзЗ ржирж╛
* Language model, translation, long text-ржП ржЦрж╛рж░рж╛ржк performance

---

## 2я╕ПтГг Exploding Gradient Problem

### ЁЯФН ржХрзА рж╕ржорж╕рзНржпрж╛?

Gradient рж╣ржарж╛рзО **ржЕрж╕рзНржмрж╛ржнрж╛ржмрж┐ржХ ржмрзЬ рж╣рзЯрзЗ ржпрж╛рзЯ**ред

---

### ЁЯза ржХрзЗржи рж╣рзЯ?

ржпржжрж┐

* $W_{hh} > 1$
  рждрж╛рж╣рж▓рзЗ ржмрж╛рж░ржмрж╛рж░ ржЧрзБржг тЖТ **gradient тЖТ тИЮ**

---

### тЭМ ржлрж▓рж╛ржлрж▓

* Weight ржЦрзБржм ржмрзЬ рж╣рзЯрзЗ ржпрж╛рзЯ
* Loss тЖТ NaN / тИЮ
* Training unstable, model ржнрзЗржЩрзЗ ржкрзЬрзЗ

---

### ЁЯФз Common Fix

* **Gradient clipping**
  (gradient ржХрзЗ ржПржХржЯрж┐ max limit-ржП ржЖржЯржХрзЗ ржжрзЗржУрзЯрж╛)

---

## 3я╕ПтГг Long-Term Dependency Problem

### ЁЯФН ржХрзА рж╕ржорж╕рзНржпрж╛?

RNN **ржЕржирзЗржХ ржЖржЧрзЗрж░ рждржерзНржп ржоржирзЗ рж░рж╛ржЦрждрзЗ ржкрж╛рж░рзЗ ржирж╛**ред

---

### ЁЯза ржХрзЗржи рж╣рзЯ?

* Hidden state ржПржХржЯрж╛ржЗ memory
* ржкрзНрж░рждрж┐ржмрж╛рж░ ржирждрзБржи input ржПрж▓рзЗ ржкрзБрж░рзЛржирзЛ рждржерзНржп overwrite рж╣рзЯ
* Vanishing gradient ржПржЯрж╛ржХрзЗ ржЖрж░ржУ ржЦрж╛рж░рж╛ржк ржХрж░рзЗ

---

### ЁЯУМ ржЙржжрж╛рж╣рж░ржг

Time series:

> Stock price today depends on event 30 days ago

Simple RNN:

* last 3тАУ5 step ржоржирзЗ рж░рж╛ржЦрзЗ
* 30 step ржЖржЧрзЗрж░ рждржерзНржп рж╣рж╛рж░рж╛рзЯ

---

## 4я╕ПтГг Sequential Computation (Slow Training)

### ЁЯФН ржХрзА рж╕ржорж╕рзНржпрж╛?

RNN **parallel-ржП train ржХрж░рж╛ ржпрж╛рзЯ ржирж╛**ред

---

### ЁЯза ржХрзЗржи?


$$
h_t \text{ depends on } h_{t-1}
$$

ржорж╛ржирзЗ:

* ржЖржЧрзЗ $h_1$
* рждрж╛рж░ржкрж░ $h_2$
* рждрж╛рж░ржкрж░ $h_3$

ЁЯСЙ ржПржХржЯрж╛рж░ ржкрж░ ржПржХржЯрж╛ (serial)

---

### тЭМ ржлрж▓рж╛ржлрж▓

* Training ржЦрзБржм slow
* GPU-рж░ ржкрзБрж░рзЛ ржХрзНрж╖ржорждрж╛ ржмрзНржпржмрж╣рж╛рж░ рж╣рзЯ ржирж╛
* Long sequence тЖТ ржЦрзБржм рж╕ржорзЯ рж▓рж╛ржЧрзЗ

---

## 5я╕ПтГг Memory Bottleneck (Single Hidden State)

### ЁЯФН ржХрзА рж╕ржорж╕рзНржпрж╛?

RNN-ржПрж░ ржкрзБрж░рзЛ memory ржерж╛ржХрзЗ **ржПржХржЯрж╛ hidden state-ржП**ред

---

### ЁЯза ржХрзЗржи рж╕ржорж╕рзНржпрж╛?

* Complex sequence-ржПрж░ ржЬржирзНржп memory ржпржерзЗрж╖рзНржЯ ржирж╛
* Important + unimportant рж╕ржм рждржерзНржп ржПржХ ржЬрж╛рзЯржЧрж╛рзЯ ржЬржорзЗ

---

### ЁЯУМ рждрзБрж▓ржирж╛

* RNN = ржЫрзЛржЯ ржирзЛржЯржмрзБржХ
* LSTM = organized diary (gate system)

---

## 6я╕ПтГг Bias Toward Recent Inputs

### ЁЯФН ржХрзА рж╕ржорж╕рзНржпрж╛?

RNN **recent input-ржХрзЗ ржмрзЗрж╢рж┐ ржЧрзБрж░рзБрждрзНржм ржжрзЗрзЯ**ред

---

### ЁЯза ржХрзЗржи?

* Recent gradient рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзА
* Old gradient ржжрзБрж░рзНржмрж▓ (vanishing)

---

### тЭМ ржлрж▓рж╛ржлрж▓

* Context skewed
* Early information ignored

---

## 7я╕ПтГг Difficult to Tune & Unstable Training

### ЁЯФН ржХрзА рж╕ржорж╕рзНржпрж╛?

* Learning rate sensitive
* Initialization ржнрзБрж▓ рж╣рж▓рзЗ training fail
* Activation function choice critical

---

### тЭМ ржлрж▓рж╛ржлрж▓

* Beginners-ржПрж░ ржЬржирзНржп ржХржарж┐ржи
* Production-ржП risky

---

## ЁЯФЪ рж╕ржм рж╕ржорж╕рзНржпрж╛рж░ ржПржХ ржиржЬрж░рзЗ рж╕рж╛рж░рж╛ржВрж╢

| Problem              | ржХрзА рж╣рзЯ               | ржХрзЗржи ржЦрж╛рж░рж╛ржк                |
| -------------------- | ------------------- | ------------------------ |
| Vanishing Gradient   | Gradient тЖТ 0        | Past рж╢рзЗржЦрзЗ ржирж╛             |
| Exploding Gradient   | Gradient тЖТ тИЮ        | Training unstable        |
| Long-term dependency | ржжрзВрж░рзЗрж░ рждржерзНржп ржнрзБрж▓рзЗ ржпрж╛рзЯ | Context loss             |
| Sequential training  | Slow                | Large data-ржП impractical |
| Single memory        | Capacity ржХржо         | Complex task fail        |
| Recent bias          | Old info ignore     | Wrong prediction         |

---

## тЬЕ ржПрж╕ржм рж╕ржорж╕рзНржпрж╛рж░ рж╕ржорж╛ржзрж╛ржи ржХрзА?

ржПржЗ рж╕ржорж╕рзНржпрж╛ржЧрзБрж▓рзЛрж░ ржЬржирзНржпржЗ ржПрж╕рзЗржЫрзЗ:

* **LSTM (Long Short-Term Memory)**
* **GRU**
* ржкрж░рзЗ тЖТ **Transformer**

---

## ЁЯза ржПржХ рж▓рж╛ржЗржирзЗрж░ takeaway

**Simple RNN conceptually рж╕рж╣ржЬ, ржХрж┐ржирзНрждрзБ long sequence рж╢рзЗржЦрж╛рж░ ржЬржирзНржп ржжрзБрж░рзНржмрж▓тАФgradient, memory ржУ speed-ржПрж░ рж╕рзАржорж╛ржмржжрзНржзрждрж╛рж░ ржХрж╛рж░ржгрзЗред**

---

>>>>>>> f45ebbad1686e699afe9932c4175eeff501d254b
