## ЁЯФБ RNN Training-ржП Backward Propagation With Time (BPTT) тАФ ржмрж╛ржВрж▓рж╛рзЯ

![Image](https://miro.medium.com/0%2ASWHzEFzYRDSnc3w2)

![Image](https://www.researchgate.net/publication/341956650/figure/fig1/AS%3A11431281078694336%401660200861643/An-RNN-unrolled-through-the-time-The-same-structure-is-repeated-at-adjacent-time-steps.ppm)

![Image](https://i.sstatic.net/S4C1U.png)

---

### 1я╕ПтГг ржХрзЗржи тАЬWith TimeтАЭ ржжрж░ржХрж╛рж░?

RNN-ржП ржкрзНрж░рждрж┐ржЯрж┐ time step-ржПрж░ output рж╢рзБржзрзБ ржмрж░рзНрждржорж╛ржи input-ржПрж░ ржЙржкрж░ ржирзЯ, **ржЖржЧрзЗрж░ hidden state**-ржПрж░ ржЙржкрж░ржУ ржирж┐рж░рзНржнрж░ ржХрж░рзЗред
рждрж╛ржЗ loss-ржПрж░ ржкрзНрж░ржнрж╛ржм (error) рж╢рзБржзрзБ ржПржХ ржзрж╛ржкрзЗ рж╕рзАржорж╛ржмржжрзНржз ржирж╛тАФ**ржкрзБрж░рзЛ sequence ржЬрзБрзЬрзЗ ржЫрзЬрж┐рзЯрзЗ ржерж╛ржХрзЗ**ред
ржПржЗ ржХрж╛рж░ржгрзЗржЗ backward pass-ржП time axis ржзрж░рзЗ ржкрж┐ржЫржирзЗ ржпрзЗрждрзЗ рж╣рзЯред

---

### 2я╕ПтГг BPTT ржХрзА ржХрж░рзЗ (High-level Flow)

1. **Forward pass**: (x_1 \rightarrow x_T) тАФ рж╕ржм (h_t, y_t) ржмрзЗрж░ рж╣рзЯ
2. **Loss ржЧржгржирж╛**: рж╕рж╛ржзрж╛рж░ржгржд рж╕ржм time-ржПрж░ loss ржпрзЛржЧ/ржЧрзЬ
3. **Backward pass (BPTT)**:

   * (t=T) ржерзЗржХрзЗ (t=1) ржкрж░рзНржпржирзНржд
   * error propagate рж╣рзЯ hidden states ржУ weights-ржП
4. **Weights update**: рж╕ржм time-ржПрж░ gradient ржпрзЛржЧ ржХрж░рзЗ ржПржХржмрж╛рж░рзЗ ржЖржкржбрзЗржЯ

---

### 3я╕ПтГг Forward рж╕ржорзАржХрж░ржг (рж░рзЗржлрж╛рж░рзЗржирзНрж╕)


$$
h_t = f(W_{xh}x_t + W_{hh}h_{t-1} + b_h)
$$
$$
y_t = g(W_{hy}h_t + b_y)
$$

---

### 4я╕ПтГг Backward-ржПрж░ ржорзВрж▓ ржзрж╛рж░ржгрж╛ (Gradient Flow)

#### ЁЯФ╣ Output loss ржерзЗржХрзЗ hidden-ржП error


$$
\delta^y_t = \frac{\partial L}{\partial y_t}
$$

#### ЁЯФ╣ Hidden state-ржП ржорзЛржЯ error


$$
\delta^h_t = \underbrace{W_{hy}^\top \delta^y_t}_{\text{current output ржерзЗржХрзЗ}} + \underbrace{W_{hh}^\top \delta^h_{t+1}}_{\text{future time ржерзЗржХрзЗ}} \odot f'(a_t)
$$

ЁЯСЙ ржПржЦрж╛ржирзЗ ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг ржмрж┐рж╖рзЯ:

* **ржПржХржЗ hidden state ржнржмрж┐рж╖рзНржпрзО step-ржПржУ ржмрзНржпржмрж╣рзГржд**, рждрж╛ржЗ ( \delta^h_{t+1} ) ржлрж┐рж░рзЗ ржЖрж╕рзЗ
* ржПржЯрж╛ржЗ тАЬ**Through Time**тАЭ


$$
\frac{\partial L}{\partial W_{hh}} = \sum_{t=1}^{T} \delta^h_t \, h_{t-1}^\top
$$
$$
\frac{\partial L}{\partial W_{xh}} = \sum_{t=1}^{T} \delta^h_t \, x_t^\top
$$
$$
\frac{\partial L}{\partial W_{hy}} = \sum_{t=1}^{T} \delta^y_t \, h_t^\top
$$

[
\frac{\partial L}{\partial W_{xh}} = \sum_{t=1}^{T} \delta^h_t , x_t^\top
]

[
\frac{\partial L}{\partial W_{hy}} = \sum_{t=1}^{T} \delta^y_t , h_t^\top
]

ЁЯСЙ **Weights shared**, рждрж╛ржЗ gradient-ржУ shared ржПржмржВ ржпрзЛржЧ рж╣рзЯред

---

### 6я╕ПтГг Step-by-Step BPTT (рж╕рж╣ржЬ ржнрж╛рж╖рж╛рзЯ)

ржзрж░рж╛ ржпрж╛ржХ sequence length = 3

1. **t=3 (рж╢рзЗрж╖ step)**

   * Output error рж╣рж┐рж╕рж╛ржм
   * (h_3)-ржПрж░ gradient ржмрзЗрж░
2. **t=2**

   * (h_3) ржерзЗржХрзЗ ржЖрж╕рж╛ error + (y_2)-ржПрж░ error
   * (h_2)-ржПрж░ gradient
3. **t=1**

   * ржнржмрж┐рж╖рзНржпрзО рж╕ржм error ржЬржорзЗ (h_1)-ржП ржЖрж╕рзЗ
4. рж╕ржм gradient ржпрзЛржЧ тЖТ **ржПржХржмрж╛рж░ weight update**

---

### 7я╕ПтГг Vanishing & Exploding Gradient (BPTT-ржПрж░ ржмрзЬ рж╕ржорж╕рзНржпрж╛)

Time ржмрзЗрж╢рж┐ рж╣рж▓рзЗ:

* ЁЯФ╗ **Vanishing Gradient**

  * Gradient ржЫрзЛржЯ рж╣рждрзЗ рж╣рждрзЗ рж╢рзВржирзНржпрзЗрж░ ржХрж╛ржЫрж╛ржХрж╛ржЫрж┐
  * ржжрзВрж░рзЗрж░ past рж╢рзЗржЦрж╛ ржпрж╛рзЯ ржирж╛
* ЁЯФ║ **Exploding Gradient**

  * Gradient ржЦрзБржм ржмрзЬ
  * Training unstable

ЁЯФз рж╕ржорж╛ржзрж╛ржи:

* Gradient clipping
* Better activation (`tanh`)
* **LSTM / GRU** ржмрзНржпржмрж╣рж╛рж░

---

### 8я╕ПтГг Truncated BPTT (Practical ржХрзМрж╢рж▓)

ржкрзБрж░рзЛ sequence ржирж╛ ржирж┐рзЯрзЗ:

* рж╢рзБржзрзБ рж╢рзЗрж╖ **k time step** ржкрж░рзНржпржирзНржд backprop ржХрж░рж╛ рж╣рзЯ
* Memory ржУ computation ржХржорзЗ
* Long sequence-ржП ржЦрзБржм ржжрж░ржХрж╛рж░рж┐

---

### 9я╕ПтГг ржПржХ рж▓рж╛ржЗржирзЗ рж╕рж╛рж░рж╛ржВрж╢

**BPTT рж╣рж▓рзЛ RNN-ржПрж░ backward pass ржпрзЗржЦрж╛ржирзЗ loss-ржПрж░ error time-ржПрж░ ржЙрж▓рзНржЯрзЛ ржжрж┐ржХрзЗ propagate рж╣рзЯрзЗ shared weights ржЖржкржбрзЗржЯ ржХрж░рзЗред**

---
