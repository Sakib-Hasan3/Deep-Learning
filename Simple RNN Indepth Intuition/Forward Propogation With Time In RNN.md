<<<<<<< HEAD
## ğŸ” RNN Training-à¦ Forward Propagation with Time (à¦¬à¦¾à¦‚à¦²à¦¾à§Ÿ à¦¬à§à¦¯à¦¾à¦–à§à¦¯à¦¾)

![Image](https://www.researchgate.net/publication/373992726/figure/fig5/AS%3A11431281199245610%401697561256982/RNN-forward-and-backward-propagation-A-Forward-propagation-B-Backward-propagation.tif)

![Image](https://www.researchgate.net/publication/341956650/figure/fig1/AS%3A11431281078694336%401660200861643/An-RNN-unrolled-through-the-time-The-same-structure-is-repeated-at-adjacent-time-steps.ppm)

![Image](https://d2l.ai/_images/rnn.svg)

### 1ï¸âƒ£ RNN à¦•à§€à¦­à¦¾à¦¬à§‡ à¦•à¦¾à¦œ à¦•à¦°à§‡ (à¦¸à¦‚à¦•à§à¦·à§‡à¦ªà§‡)

Recurrent Neural Network (RNN) à¦®à§‚à¦²à¦¤ **sequence data** (à¦¯à§‡à¦®à¦¨: text, speech, time series) à¦¨à¦¿à§Ÿà§‡ à¦•à¦¾à¦œ à¦•à¦°à§‡à¥¤
à¦à¦–à¦¾à¦¨à§‡ à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ à¦‡à¦¨à¦ªà§à¦Ÿ à¦à¦•à¦¾ à¦¨à§Ÿâ€”**à¦†à¦—à§‡à¦° à¦¸à¦®à§Ÿà§‡à¦° à¦¤à¦¥à§à¦¯ (memory)**-à¦“ à¦¬à¦¿à¦¬à§‡à¦šà¦¨à¦¾à§Ÿ à¦¨à§‡à§Ÿà¥¤

à¦à¦‡ à¦¸à§à¦®à§ƒà¦¤à¦¿à¦Ÿà¦¾à¦‡ à¦¥à¦¾à¦•à§‡ **Hidden State (h)**-à¦à¦° à¦®à¦§à§à¦¯à§‡à¥¤

---

### 2ï¸âƒ£ â€œWith Timeâ€ à¦¬à¦²à¦¤à§‡ à¦•à§€ à¦¬à§‹à¦à¦¾à§Ÿ?

RNN-à¦ à¦‡à¦¨à¦ªà§à¦Ÿ à¦†à¦¸à§‡ à¦§à¦¾à¦°à¦¾à¦¬à¦¾à¦¹à¦¿à¦•à¦­à¦¾à¦¬à§‡:


$$
x_1, x_2, x_3, \dots, x_T
$$

à¦à¦–à¦¾à¦¨à§‡ à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ (x_t) à¦¹à¦²à§‹ à¦à¦•à¦Ÿà¦¿ **time step**à¥¤

**Forward propagation with time** à¦®à¦¾à¦¨à§‡:

* à¦†à¦®à¦°à¦¾ RNN-à¦•à§‡ **time axis à¦¬à¦°à¦¾à¦¬à¦° à¦–à§à¦²à§‡ (unroll)** à¦¦à§‡à¦–à¦¿
* à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ à¦¸à¦®à§Ÿ à¦§à¦¾à¦ªà§‡:

  * à¦†à¦—à§‡à¦° hidden state
  * à¦¬à¦°à§à¦¤à¦®à¦¾à¦¨ input
    â†’ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§‡ à¦¨à¦¤à§à¦¨ hidden state à¦“ output à¦¹à¦¿à¦¸à¦¾à¦¬ à¦•à¦°à¦¿

---

### 3ï¸âƒ£ Forward Propagation-à¦à¦° à¦®à§‚à¦² à¦¸à¦®à§€à¦•à¦°à¦£

#### ğŸ”¹ Hidden State Update


$$
h_t = f(W_{xh} x_t + W_{hh} h_{t-1} + b_h)
$$

#### ğŸ”¹ Output


$$
y_t = g(W_{hy} h_t + b_y)
$$

à¦¯à§‡à¦–à¦¾à¦¨à§‡:

* $x_t$ = à¦¬à¦°à§à¦¤à¦®à¦¾à¦¨ time-à¦à¦° input
* $h_{t-1}$ = à¦†à¦—à§‡à¦° time-à¦à¦° hidden state
* $h_t$ = à¦¬à¦°à§à¦¤à¦®à¦¾à¦¨ hidden state
* $f$ = activation function (à¦¸à¦¾à¦§à¦¾à¦°à¦£à¦¤ `tanh` à¦¬à¦¾ `ReLU`)
* $g$ = output activation (à¦¯à§‡à¦®à¦¨ `softmax`)
* **à¦¸à¦¬ time step-à¦ weights à¦à¦•à¦‡ à¦¥à¦¾à¦•à§‡**

---

### 4ï¸âƒ£ Step-by-Step Forward Propagation (à¦¸à¦¹à¦œ à¦­à¦¾à¦·à¦¾à§Ÿ)

à¦§à¦°à¦¾ à¦¯à¦¾à¦• à¦à¦•à¦Ÿà¦¿ à¦¬à¦¾à¦•à§à¦¯:

> **â€œI love AIâ€**

| Time (t) | Input (x_t) | Previous State | New Hidden State | Output |
| -------- | ----------- | -------------- | ---------------- | ------ |
| t=1      | "I"         | (h_0 = 0)      | (h_1)            | (y_1)  |
| t=2      | "love"      | (h_1)          | (h_2)            | (y_2)  |
| t=3      | "AI"        | (h_2)          | (h_3)            | (y_3)  |

ğŸ‘‰ à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ à¦§à¦¾à¦ªà§‡ à¦†à¦—à§‡à¦° à¦¤à¦¥à§à¦¯ **carry forward** à¦¹à¦šà§à¦›à§‡à¥¤

---

### 5ï¸âƒ£ Training-à¦ Forward Propagation-à¦à¦° à¦­à§‚à¦®à¦¿à¦•à¦¾

Training à¦šà¦²à¦¾à¦•à¦¾à¦²à§‡:

1. **Forward propagation with time**
   â†’ à¦¸à¦¬ time step-à¦à¦° output à¦¬à§‡à¦° à¦•à¦°à¦¾ à¦¹à§Ÿ
2. Output à¦¥à§‡à¦•à§‡ **loss** à¦—à¦£à¦¨à¦¾ à¦•à¦°à¦¾ à¦¹à§Ÿ
3. à¦¤à¦¾à¦°à¦ªà¦° **Backpropagation Through Time (BPTT)** à¦¦à¦¿à§Ÿà§‡ weight update à¦¹à§Ÿ

âš ï¸ à¦®à¦¨à§‡ à¦°à¦¾à¦–à¦¬à§‡:

* Forward propagation à¦¶à§à¦§à§ **value à¦¹à¦¿à¦¸à¦¾à¦¬ à¦•à¦°à§‡**
* Gradient update à¦¹à§Ÿ **backward phase-à¦**

---

### 6ï¸âƒ£ à¦•à§‡à¦¨ Forward Propagation with Time à¦—à§à¦°à§à¦¤à§à¦¬à¦ªà§‚à¦°à§à¦£?

* Sequence-à¦à¦° **context à¦¬à§‹à¦à¦¾ à¦¯à¦¾à§Ÿ**
* à¦†à¦—à§‡à¦° à¦¶à¦¬à§à¦¦/à¦¡à§‡à¦Ÿà¦¾ à¦­à¦¬à¦¿à¦·à§à¦¯à§ à¦¸à¦¿à¦¦à§à¦§à¦¾à¦¨à§à¦¤à§‡ à¦ªà§à¦°à¦­à¦¾à¦¬ à¦«à§‡à¦²à§‡
* Language model, speech recognition, time-series forecasting à¦¸à¦®à§à¦­à¦¬ à¦¹à§Ÿ

---

### 7ï¸âƒ£ à¦à¦• à¦²à¦¾à¦‡à¦¨à§‡ à¦¸à¦¾à¦°à¦¾à¦‚à¦¶

**RNN-à¦ Forward Propagation with Time à¦¹à¦²à§‹â€”à¦à¦•à¦‡ network à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§‡ à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ à¦¸à¦®à§Ÿ à¦§à¦¾à¦ªà§‡ input à¦“ à¦†à¦—à§‡à¦° memory à¦¥à§‡à¦•à§‡ à¦¨à¦¤à§à¦¨ memory à¦“ output à¦¤à§ˆà¦°à¦¿ à¦•à¦°à¦¾à¦° à¦§à¦¾à¦°à¦¾à¦¬à¦¾à¦¹à¦¿à¦• à¦ªà§à¦°à¦•à§à¦°à¦¿à§Ÿà¦¾à¥¤**

---
=======
## ğŸ” RNN Training-à¦ Forward Propagation with Time (à¦¬à¦¾à¦‚à¦²à¦¾à§Ÿ à¦¬à§à¦¯à¦¾à¦–à§à¦¯à¦¾)

![Image](https://www.researchgate.net/publication/373992726/figure/fig5/AS%3A11431281199245610%401697561256982/RNN-forward-and-backward-propagation-A-Forward-propagation-B-Backward-propagation.tif)

![Image](https://www.researchgate.net/publication/341956650/figure/fig1/AS%3A11431281078694336%401660200861643/An-RNN-unrolled-through-the-time-The-same-structure-is-repeated-at-adjacent-time-steps.ppm)

![Image](https://d2l.ai/_images/rnn.svg)

### 1ï¸âƒ£ RNN à¦•à§€à¦­à¦¾à¦¬à§‡ à¦•à¦¾à¦œ à¦•à¦°à§‡ (à¦¸à¦‚à¦•à§à¦·à§‡à¦ªà§‡)

Recurrent Neural Network (RNN) à¦®à§‚à¦²à¦¤ **sequence data** (à¦¯à§‡à¦®à¦¨: text, speech, time series) à¦¨à¦¿à§Ÿà§‡ à¦•à¦¾à¦œ à¦•à¦°à§‡à¥¤
à¦à¦–à¦¾à¦¨à§‡ à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ à¦‡à¦¨à¦ªà§à¦Ÿ à¦à¦•à¦¾ à¦¨à§Ÿâ€”**à¦†à¦—à§‡à¦° à¦¸à¦®à§Ÿà§‡à¦° à¦¤à¦¥à§à¦¯ (memory)**-à¦“ à¦¬à¦¿à¦¬à§‡à¦šà¦¨à¦¾à§Ÿ à¦¨à§‡à§Ÿà¥¤

à¦à¦‡ à¦¸à§à¦®à§ƒà¦¤à¦¿à¦Ÿà¦¾à¦‡ à¦¥à¦¾à¦•à§‡ **Hidden State (h)**-à¦à¦° à¦®à¦§à§à¦¯à§‡à¥¤

---

### 2ï¸âƒ£ â€œWith Timeâ€ à¦¬à¦²à¦¤à§‡ à¦•à§€ à¦¬à§‹à¦à¦¾à§Ÿ?

RNN-à¦ à¦‡à¦¨à¦ªà§à¦Ÿ à¦†à¦¸à§‡ à¦§à¦¾à¦°à¦¾à¦¬à¦¾à¦¹à¦¿à¦•à¦­à¦¾à¦¬à§‡:


$$
x_1, x_2, x_3, \dots, x_T
$$

à¦à¦–à¦¾à¦¨à§‡ à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ (x_t) à¦¹à¦²à§‹ à¦à¦•à¦Ÿà¦¿ **time step**à¥¤

**Forward propagation with time** à¦®à¦¾à¦¨à§‡:

* à¦†à¦®à¦°à¦¾ RNN-à¦•à§‡ **time axis à¦¬à¦°à¦¾à¦¬à¦° à¦–à§à¦²à§‡ (unroll)** à¦¦à§‡à¦–à¦¿
* à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ à¦¸à¦®à§Ÿ à¦§à¦¾à¦ªà§‡:

  * à¦†à¦—à§‡à¦° hidden state
  * à¦¬à¦°à§à¦¤à¦®à¦¾à¦¨ input
    â†’ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§‡ à¦¨à¦¤à§à¦¨ hidden state à¦“ output à¦¹à¦¿à¦¸à¦¾à¦¬ à¦•à¦°à¦¿

---

### 3ï¸âƒ£ Forward Propagation-à¦à¦° à¦®à§‚à¦² à¦¸à¦®à§€à¦•à¦°à¦£

#### ğŸ”¹ Hidden State Update


$$
h_t = f(W_{xh} x_t + W_{hh} h_{t-1} + b_h)
$$

#### ğŸ”¹ Output


$$
y_t = g(W_{hy} h_t + b_y)
$$

à¦¯à§‡à¦–à¦¾à¦¨à§‡:

* $x_t$ = à¦¬à¦°à§à¦¤à¦®à¦¾à¦¨ time-à¦à¦° input
* $h_{t-1}$ = à¦†à¦—à§‡à¦° time-à¦à¦° hidden state
* $h_t$ = à¦¬à¦°à§à¦¤à¦®à¦¾à¦¨ hidden state
* $f$ = activation function (à¦¸à¦¾à¦§à¦¾à¦°à¦£à¦¤ `tanh` à¦¬à¦¾ `ReLU`)
* $g$ = output activation (à¦¯à§‡à¦®à¦¨ `softmax`)
* **à¦¸à¦¬ time step-à¦ weights à¦à¦•à¦‡ à¦¥à¦¾à¦•à§‡**

---

### 4ï¸âƒ£ Step-by-Step Forward Propagation (à¦¸à¦¹à¦œ à¦­à¦¾à¦·à¦¾à§Ÿ)

à¦§à¦°à¦¾ à¦¯à¦¾à¦• à¦à¦•à¦Ÿà¦¿ à¦¬à¦¾à¦•à§à¦¯:

> **â€œI love AIâ€**

| Time (t) | Input (x_t) | Previous State | New Hidden State | Output |
| -------- | ----------- | -------------- | ---------------- | ------ |
| t=1      | "I"         | (h_0 = 0)      | (h_1)            | (y_1)  |
| t=2      | "love"      | (h_1)          | (h_2)            | (y_2)  |
| t=3      | "AI"        | (h_2)          | (h_3)            | (y_3)  |

ğŸ‘‰ à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ à¦§à¦¾à¦ªà§‡ à¦†à¦—à§‡à¦° à¦¤à¦¥à§à¦¯ **carry forward** à¦¹à¦šà§à¦›à§‡à¥¤

---

### 5ï¸âƒ£ Training-à¦ Forward Propagation-à¦à¦° à¦­à§‚à¦®à¦¿à¦•à¦¾

Training à¦šà¦²à¦¾à¦•à¦¾à¦²à§‡:

1. **Forward propagation with time**
   â†’ à¦¸à¦¬ time step-à¦à¦° output à¦¬à§‡à¦° à¦•à¦°à¦¾ à¦¹à§Ÿ
2. Output à¦¥à§‡à¦•à§‡ **loss** à¦—à¦£à¦¨à¦¾ à¦•à¦°à¦¾ à¦¹à§Ÿ
3. à¦¤à¦¾à¦°à¦ªà¦° **Backpropagation Through Time (BPTT)** à¦¦à¦¿à§Ÿà§‡ weight update à¦¹à§Ÿ

âš ï¸ à¦®à¦¨à§‡ à¦°à¦¾à¦–à¦¬à§‡:

* Forward propagation à¦¶à§à¦§à§ **value à¦¹à¦¿à¦¸à¦¾à¦¬ à¦•à¦°à§‡**
* Gradient update à¦¹à§Ÿ **backward phase-à¦**

---

### 6ï¸âƒ£ à¦•à§‡à¦¨ Forward Propagation with Time à¦—à§à¦°à§à¦¤à§à¦¬à¦ªà§‚à¦°à§à¦£?

* Sequence-à¦à¦° **context à¦¬à§‹à¦à¦¾ à¦¯à¦¾à§Ÿ**
* à¦†à¦—à§‡à¦° à¦¶à¦¬à§à¦¦/à¦¡à§‡à¦Ÿà¦¾ à¦­à¦¬à¦¿à¦·à§à¦¯à§ à¦¸à¦¿à¦¦à§à¦§à¦¾à¦¨à§à¦¤à§‡ à¦ªà§à¦°à¦­à¦¾à¦¬ à¦«à§‡à¦²à§‡
* Language model, speech recognition, time-series forecasting à¦¸à¦®à§à¦­à¦¬ à¦¹à§Ÿ

---

### 7ï¸âƒ£ à¦à¦• à¦²à¦¾à¦‡à¦¨à§‡ à¦¸à¦¾à¦°à¦¾à¦‚à¦¶

**RNN-à¦ Forward Propagation with Time à¦¹à¦²à§‹â€”à¦à¦•à¦‡ network à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§‡ à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ à¦¸à¦®à§Ÿ à¦§à¦¾à¦ªà§‡ input à¦“ à¦†à¦—à§‡à¦° memory à¦¥à§‡à¦•à§‡ à¦¨à¦¤à§à¦¨ memory à¦“ output à¦¤à§ˆà¦°à¦¿ à¦•à¦°à¦¾à¦° à¦§à¦¾à¦°à¦¾à¦¬à¦¾à¦¹à¦¿à¦• à¦ªà§à¦°à¦•à§à¦°à¦¿à§Ÿà¦¾à¥¤**

---
>>>>>>> f45ebbad1686e699afe9932c4175eeff501d254b
