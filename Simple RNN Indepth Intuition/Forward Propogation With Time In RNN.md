## ЁЯФБ RNN Training-ржП Forward Propagation with Time (ржмрж╛ржВрж▓рж╛рзЯ ржмрзНржпрж╛ржЦрзНржпрж╛)

![Image](https://www.researchgate.net/publication/373992726/figure/fig5/AS%3A11431281199245610%401697561256982/RNN-forward-and-backward-propagation-A-Forward-propagation-B-Backward-propagation.tif)

![Image](https://www.researchgate.net/publication/341956650/figure/fig1/AS%3A11431281078694336%401660200861643/An-RNN-unrolled-through-the-time-The-same-structure-is-repeated-at-adjacent-time-steps.ppm)

![Image](https://d2l.ai/_images/rnn.svg)

### 1я╕ПтГг RNN ржХрзАржнрж╛ржмрзЗ ржХрж╛ржЬ ржХрж░рзЗ (рж╕ржВржХрзНрж╖рзЗржкрзЗ)

Recurrent Neural Network (RNN) ржорзВрж▓ржд **sequence data** (ржпрзЗржоржи: text, speech, time series) ржирж┐рзЯрзЗ ржХрж╛ржЬ ржХрж░рзЗред
ржПржЦрж╛ржирзЗ ржкрзНрж░рждрж┐ржЯрж┐ ржЗржиржкрзБржЯ ржПржХрж╛ ржирзЯтАФ**ржЖржЧрзЗрж░ рж╕ржорзЯрзЗрж░ рждржерзНржп (memory)**-ржУ ржмрж┐ржмрзЗржЪржирж╛рзЯ ржирзЗрзЯред

ржПржЗ рж╕рзНржорзГрждрж┐ржЯрж╛ржЗ ржерж╛ржХрзЗ **Hidden State (h)**-ржПрж░ ржоржзрзНржпрзЗред

---

### 2я╕ПтГг тАЬWith TimeтАЭ ржмрж▓рждрзЗ ржХрзА ржмрзЛржЭрж╛рзЯ?

RNN-ржП ржЗржиржкрзБржЯ ржЖрж╕рзЗ ржзрж╛рж░рж╛ржмрж╛рж╣рж┐ржХржнрж╛ржмрзЗ:


$$
x_1, x_2, x_3, \dots, x_T
$$

ржПржЦрж╛ржирзЗ ржкрзНрж░рждрж┐ржЯрж┐ (x_t) рж╣рж▓рзЛ ржПржХржЯрж┐ **time step**ред

**Forward propagation with time** ржорж╛ржирзЗ:

* ржЖржорж░рж╛ RNN-ржХрзЗ **time axis ржмрж░рж╛ржмрж░ ржЦрзБрж▓рзЗ (unroll)** ржжрзЗржЦрж┐
* ржкрзНрж░рждрж┐ржЯрж┐ рж╕ржорзЯ ржзрж╛ржкрзЗ:

  * ржЖржЧрзЗрж░ hidden state
  * ржмрж░рзНрждржорж╛ржи input
    тЖТ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржирждрзБржи hidden state ржУ output рж╣рж┐рж╕рж╛ржм ржХрж░рж┐

---

### 3я╕ПтГг Forward Propagation-ржПрж░ ржорзВрж▓ рж╕ржорзАржХрж░ржг

#### ЁЯФ╣ Hidden State Update


$$
h_t = f(W_{xh} x_t + W_{hh} h_{t-1} + b_h)
$$

#### ЁЯФ╣ Output


$$
y_t = g(W_{hy} h_t + b_y)
$$

ржпрзЗржЦрж╛ржирзЗ:

* $x_t$ = ржмрж░рзНрждржорж╛ржи time-ржПрж░ input
* $h_{t-1}$ = ржЖржЧрзЗрж░ time-ржПрж░ hidden state
* $h_t$ = ржмрж░рзНрждржорж╛ржи hidden state
* $f$ = activation function (рж╕рж╛ржзрж╛рж░ржгржд `tanh` ржмрж╛ `ReLU`)
* $g$ = output activation (ржпрзЗржоржи `softmax`)
* **рж╕ржм time step-ржП weights ржПржХржЗ ржерж╛ржХрзЗ**

---

### 4я╕ПтГг Step-by-Step Forward Propagation (рж╕рж╣ржЬ ржнрж╛рж╖рж╛рзЯ)

ржзрж░рж╛ ржпрж╛ржХ ржПржХржЯрж┐ ржмрж╛ржХрзНржп:

> **тАЬI love AIтАЭ**

| Time (t) | Input (x_t) | Previous State | New Hidden State | Output |
| -------- | ----------- | -------------- | ---------------- | ------ |
| t=1      | "I"         | (h_0 = 0)      | (h_1)            | (y_1)  |
| t=2      | "love"      | (h_1)          | (h_2)            | (y_2)  |
| t=3      | "AI"        | (h_2)          | (h_3)            | (y_3)  |

ЁЯСЙ ржкрзНрж░рждрж┐ржЯрж┐ ржзрж╛ржкрзЗ ржЖржЧрзЗрж░ рждржерзНржп **carry forward** рж╣ржЪрзНржЫрзЗред

---

### 5я╕ПтГг Training-ржП Forward Propagation-ржПрж░ ржнрзВржорж┐ржХрж╛

Training ржЪрж▓рж╛ржХрж╛рж▓рзЗ:

1. **Forward propagation with time**
   тЖТ рж╕ржм time step-ржПрж░ output ржмрзЗрж░ ржХрж░рж╛ рж╣рзЯ
2. Output ржерзЗржХрзЗ **loss** ржЧржгржирж╛ ржХрж░рж╛ рж╣рзЯ
3. рждрж╛рж░ржкрж░ **Backpropagation Through Time (BPTT)** ржжрж┐рзЯрзЗ weight update рж╣рзЯ

тЪая╕П ржоржирзЗ рж░рж╛ржЦржмрзЗ:

* Forward propagation рж╢рзБржзрзБ **value рж╣рж┐рж╕рж╛ржм ржХрж░рзЗ**
* Gradient update рж╣рзЯ **backward phase-ржП**

---

### 6я╕ПтГг ржХрзЗржи Forward Propagation with Time ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг?

* Sequence-ржПрж░ **context ржмрзЛржЭрж╛ ржпрж╛рзЯ**
* ржЖржЧрзЗрж░ рж╢ржмрзНржж/ржбрзЗржЯрж╛ ржнржмрж┐рж╖рзНржпрзО рж╕рж┐ржжрзНржзрж╛ржирзНрждрзЗ ржкрзНрж░ржнрж╛ржм ржлрзЗрж▓рзЗ
* Language model, speech recognition, time-series forecasting рж╕ржорзНржнржм рж╣рзЯ

---

### 7я╕ПтГг ржПржХ рж▓рж╛ржЗржирзЗ рж╕рж╛рж░рж╛ржВрж╢

**RNN-ржП Forward Propagation with Time рж╣рж▓рзЛтАФржПржХржЗ network ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржкрзНрж░рждрж┐ржЯрж┐ рж╕ржорзЯ ржзрж╛ржкрзЗ input ржУ ржЖржЧрзЗрж░ memory ржерзЗржХрзЗ ржирждрзБржи memory ржУ output рждрзИрж░рж┐ ржХрж░рж╛рж░ ржзрж╛рж░рж╛ржмрж╛рж╣рж┐ржХ ржкрзНрж░ржХрзНрж░рж┐рзЯрж╛ред**

---
