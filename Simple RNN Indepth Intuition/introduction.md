<<<<<<< HEAD
---

## RNN à¦•à§€?

**RNN** à¦à¦®à¦¨ à¦à¦• à¦§à¦°à¦¨à§‡à¦° Neural Network à¦¯à¦¾ **sequence data** à¦¨à¦¿à§Ÿà§‡ à¦•à¦¾à¦œ à¦•à¦°à§‡à¥¤

ğŸ“Œ Sequence à¦®à¦¾à¦¨à§‡

* à¦¬à¦¾à¦•à§à¦¯ (à¦¶à¦¬à§à¦¦à§‡à¦° à¦¸à¦¿à¦°à¦¿à¦œ)
* à¦Ÿà¦¾à¦‡à¦® à¦¸à¦¿à¦°à¦¿à¦œ
* Speech
* Text

ğŸ‘‰ RNN à¦†à¦—à§‡à¦° à¦¤à¦¥à§à¦¯ **à¦®à¦¨à§‡ à¦°à§‡à¦–à§‡** à¦ªà¦°à§‡à¦° à¦‡à¦¨à¦ªà§à¦Ÿ à¦¬à§à¦à¦¤à§‡ à¦ªà¦¾à¦°à§‡à¥¤

---

## Simple Neural Network vs RNN

### ğŸ”¹ Normal Neural Network

* à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾ input à¦†à¦²à¦¾à¦¦à¦¾
* à¦•à§‹à¦¨à§‹ memory à¦¨à§‡à¦‡

### ğŸ”¹ RNN

* à¦†à¦—à§‡à¦° output / state à¦®à¦¨à§‡ à¦°à¦¾à¦–à§‡
* sequence à¦¬à§à¦à¦¤à§‡ à¦ªà¦¾à¦°à§‡

---

## RNN à¦•à§€à¦­à¦¾à¦¬à§‡ à¦•à¦¾à¦œ à¦•à¦°à§‡?

à¦à¦•à¦Ÿà¦¾ sentence à¦§à¦°à§‹:

```
à¦†à¦®à¦¿ à¦†à¦œ à¦¸à§à¦•à§à¦²à§‡ à¦¯à¦¾à¦‡
```

RNN à¦à¦•à¦¬à¦¾à¦°à§‡ à¦ªà§à¦°à§‹ à¦¬à¦¾à¦•à§à¦¯ à¦¨à§‡à§Ÿ à¦¨à¦¾ âŒ
ğŸ‘‰ à¦¶à¦¬à§à¦¦ à¦§à¦°à§‡ à¦§à¦°à§‡ à¦¨à§‡à§Ÿ:

```
à¦†à¦®à¦¿ â†’ à¦†à¦œ â†’ à¦¸à§à¦•à§à¦²à§‡ â†’ à¦¯à¦¾à¦‡
```

à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾ à¦§à¦¾à¦ªà§‡ RNN à¦¦à§à¦‡à¦Ÿà¦¾ à¦œà¦¿à¦¨à¦¿à¦¸ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§‡:

* à¦¬à¦°à§à¦¤à¦®à¦¾à¦¨ input (xâ‚œ)
* à¦†à¦—à§‡à¦° hidden state (hâ‚œâ‚‹â‚)

---

## Simple RNN à¦à¦° Formula (Basic)

à¦ªà§à¦°à¦¤à¦¿ time step à¦:

[
h_t = \tanh(W_h h_{t-1} + W_x x_t + b)
]

* (x_t) = à¦¬à¦°à§à¦¤à¦®à¦¾à¦¨ à¦¶à¦¬à§à¦¦
* (h_{t-1}) = à¦†à¦—à§‡à¦° memory
* (h_t) = à¦¨à¦¤à§à¦¨ memory
* (W) = weight
* tanh = activation function

---

## Visual Idea ğŸ§ 

```
x1 â†’ [RNN] â†’ h1
x2 â†’ [RNN] â†’ h2
x3 â†’ [RNN] â†’ h3
```

ğŸ‘‰ h2 à¦¤à§ˆà¦°à¦¿ à¦¹à§Ÿ h1 à¦®à¦¨à§‡ à¦°à§‡à¦–à§‡
ğŸ‘‰ h3 à¦¤à§ˆà¦°à¦¿ à¦¹à§Ÿ h2 à¦®à¦¨à§‡ à¦°à§‡à¦–à§‡

---

## Simple Example (Sentiment)

Sentence:

```
"à¦à¦‡ à¦¸à¦¿à¦¨à§‡à¦®à¦¾à¦Ÿà¦¾ à¦–à§à¦¬ à¦­à¦¾à¦²à§‹"
```

* "à¦­à¦¾à¦²à§‹" à¦†à¦¸à¦¾à¦° à¦†à¦—à§‡ RNN à¦œà¦¾à¦¨à§‡
* à¦†à¦—à§‡ "à¦–à§à¦¬" à¦à¦¸à§‡à¦›à§‡
  â¡ï¸ à¦¤à¦¾à¦‡ sentiment positive à¦¬à§à¦à¦¤à§‡ à¦ªà¦¾à¦°à§‡

---

## Simple RNN Code (Keras / TensorFlow)

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense

model = Sequential()
model.add(SimpleRNN(32, input_shape=(10, 1)))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.summary()
```

ğŸ“Œ à¦à¦–à¦¾à¦¨à§‡

* `32` = hidden units
* `10` = sequence length
* `1` = feature per step

---

## RNN à¦•à§‹à¦¥à¦¾à§Ÿ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦¹à§Ÿ?

* Text classification
* Language modeling
* Simple chatbot
* Speech recognition

---

## RNN à¦à¦° à¦¸à¦®à¦¸à§à¦¯à¦¾ âŒ

* Long sentence à¦ à¦†à¦—à§‡à¦° à¦¤à¦¥à§à¦¯ à¦­à§à¦²à§‡ à¦¯à¦¾à§Ÿ
* Vanishing gradient problem

ğŸ‘‰ à¦à¦œà¦¨à§à¦¯à¦‡ à¦ªà¦°à§‡ à¦à¦¸à§‡à¦›à§‡
âœ… **LSTM**
âœ… **GRU**

---

## à¦•à¦–à¦¨ Simple RNN à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à¦¬à§‡?

âœ” à¦›à§‹à¦Ÿ sequence
âœ” learning purpose
âœ” basic NLP understanding

âŒ à¦¬à§œ paragraph / long context â†’ LSTM / Transformer à¦­à¦¾à¦²à§‹

---

=======
---

## RNN à¦•à§€?

**RNN** à¦à¦®à¦¨ à¦à¦• à¦§à¦°à¦¨à§‡à¦° Neural Network à¦¯à¦¾ **sequence data** à¦¨à¦¿à§Ÿà§‡ à¦•à¦¾à¦œ à¦•à¦°à§‡à¥¤

ğŸ“Œ Sequence à¦®à¦¾à¦¨à§‡

* à¦¬à¦¾à¦•à§à¦¯ (à¦¶à¦¬à§à¦¦à§‡à¦° à¦¸à¦¿à¦°à¦¿à¦œ)
* à¦Ÿà¦¾à¦‡à¦® à¦¸à¦¿à¦°à¦¿à¦œ
* Speech
* Text

ğŸ‘‰ RNN à¦†à¦—à§‡à¦° à¦¤à¦¥à§à¦¯ **à¦®à¦¨à§‡ à¦°à§‡à¦–à§‡** à¦ªà¦°à§‡à¦° à¦‡à¦¨à¦ªà§à¦Ÿ à¦¬à§à¦à¦¤à§‡ à¦ªà¦¾à¦°à§‡à¥¤

---

## Simple Neural Network vs RNN

### ğŸ”¹ Normal Neural Network

* à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾ input à¦†à¦²à¦¾à¦¦à¦¾
* à¦•à§‹à¦¨à§‹ memory à¦¨à§‡à¦‡

### ğŸ”¹ RNN

* à¦†à¦—à§‡à¦° output / state à¦®à¦¨à§‡ à¦°à¦¾à¦–à§‡
* sequence à¦¬à§à¦à¦¤à§‡ à¦ªà¦¾à¦°à§‡

---

## RNN à¦•à§€à¦­à¦¾à¦¬à§‡ à¦•à¦¾à¦œ à¦•à¦°à§‡?

à¦à¦•à¦Ÿà¦¾ sentence à¦§à¦°à§‹:

```
à¦†à¦®à¦¿ à¦†à¦œ à¦¸à§à¦•à§à¦²à§‡ à¦¯à¦¾à¦‡
```

RNN à¦à¦•à¦¬à¦¾à¦°à§‡ à¦ªà§à¦°à§‹ à¦¬à¦¾à¦•à§à¦¯ à¦¨à§‡à§Ÿ à¦¨à¦¾ âŒ
ğŸ‘‰ à¦¶à¦¬à§à¦¦ à¦§à¦°à§‡ à¦§à¦°à§‡ à¦¨à§‡à§Ÿ:

```
à¦†à¦®à¦¿ â†’ à¦†à¦œ â†’ à¦¸à§à¦•à§à¦²à§‡ â†’ à¦¯à¦¾à¦‡
```

à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¾ à¦§à¦¾à¦ªà§‡ RNN à¦¦à§à¦‡à¦Ÿà¦¾ à¦œà¦¿à¦¨à¦¿à¦¸ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§‡:

* à¦¬à¦°à§à¦¤à¦®à¦¾à¦¨ input (xâ‚œ)
* à¦†à¦—à§‡à¦° hidden state (hâ‚œâ‚‹â‚)

---

## Simple RNN à¦à¦° Formula (Basic)

à¦ªà§à¦°à¦¤à¦¿ time step à¦:

[
h_t = \tanh(W_h h_{t-1} + W_x x_t + b)
]

* (x_t) = à¦¬à¦°à§à¦¤à¦®à¦¾à¦¨ à¦¶à¦¬à§à¦¦
* (h_{t-1}) = à¦†à¦—à§‡à¦° memory
* (h_t) = à¦¨à¦¤à§à¦¨ memory
* (W) = weight
* tanh = activation function

---

## Visual Idea ğŸ§ 

```
x1 â†’ [RNN] â†’ h1
x2 â†’ [RNN] â†’ h2
x3 â†’ [RNN] â†’ h3
```

ğŸ‘‰ h2 à¦¤à§ˆà¦°à¦¿ à¦¹à§Ÿ h1 à¦®à¦¨à§‡ à¦°à§‡à¦–à§‡
ğŸ‘‰ h3 à¦¤à§ˆà¦°à¦¿ à¦¹à§Ÿ h2 à¦®à¦¨à§‡ à¦°à§‡à¦–à§‡

---

## Simple Example (Sentiment)

Sentence:

```
"à¦à¦‡ à¦¸à¦¿à¦¨à§‡à¦®à¦¾à¦Ÿà¦¾ à¦–à§à¦¬ à¦­à¦¾à¦²à§‹"
```

* "à¦­à¦¾à¦²à§‹" à¦†à¦¸à¦¾à¦° à¦†à¦—à§‡ RNN à¦œà¦¾à¦¨à§‡
* à¦†à¦—à§‡ "à¦–à§à¦¬" à¦à¦¸à§‡à¦›à§‡
  â¡ï¸ à¦¤à¦¾à¦‡ sentiment positive à¦¬à§à¦à¦¤à§‡ à¦ªà¦¾à¦°à§‡

---

## Simple RNN Code (Keras / TensorFlow)

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense

model = Sequential()
model.add(SimpleRNN(32, input_shape=(10, 1)))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.summary()
```

ğŸ“Œ à¦à¦–à¦¾à¦¨à§‡

* `32` = hidden units
* `10` = sequence length
* `1` = feature per step

---

## RNN à¦•à§‹à¦¥à¦¾à§Ÿ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦¹à§Ÿ?

* Text classification
* Language modeling
* Simple chatbot
* Speech recognition

---

## RNN à¦à¦° à¦¸à¦®à¦¸à§à¦¯à¦¾ âŒ

* Long sentence à¦ à¦†à¦—à§‡à¦° à¦¤à¦¥à§à¦¯ à¦­à§à¦²à§‡ à¦¯à¦¾à§Ÿ
* Vanishing gradient problem

ğŸ‘‰ à¦à¦œà¦¨à§à¦¯à¦‡ à¦ªà¦°à§‡ à¦à¦¸à§‡à¦›à§‡
âœ… **LSTM**
âœ… **GRU**

---

## à¦•à¦–à¦¨ Simple RNN à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à¦¬à§‡?

âœ” à¦›à§‹à¦Ÿ sequence
âœ” learning purpose
âœ” basic NLP understanding

âŒ à¦¬à§œ paragraph / long context â†’ LSTM / Transformer à¦­à¦¾à¦²à§‹

---

>>>>>>> f45ebbad1686e699afe9932c4175eeff501d254b
