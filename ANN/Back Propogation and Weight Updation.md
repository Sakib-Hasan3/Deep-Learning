<<<<<<< HEAD
### **Backpropagation and Weight Updation (with Mathematical Example)**

à¦à¦Ÿà¦¾ ANN à¦¶à§‡à¦–à¦¾à¦° **à¦¸à¦¬à¦šà§‡à§Ÿà§‡ à¦—à§à¦°à§à¦¤à§à¦¬à¦ªà§‚à¦°à§à¦£ à¦…à¦‚à¦¶**à¥¤ à¦¸à¦¹à¦œ à¦­à¦¾à¦·à¦¾à§Ÿ à¦¬à¦²à¦²à§‡â€”

> **Backpropagation = à¦­à§à¦² à¦¬à§‡à¦° à¦•à¦°à¦¾ + à¦¸à§‡à¦‡ à¦­à§à¦² à¦…à¦¨à§à¦¯à¦¾à§Ÿà§€ weight à¦ à¦¿à¦• à¦•à¦°à¦¾**

---

## 1ï¸âƒ£ Basic Idea (à¦¸à¦¹à¦œ à¦•à¦°à§‡)

* ANN à¦à¦•à¦Ÿà¦¿ output à¦¦à§‡à§Ÿ
* output à¦¯à¦¦à¦¿ à¦­à§à¦² à¦¹à§Ÿ â†’ **error** à¦¬à§‡à¦° à¦•à¦°à¦¿
* error à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§‡ **weight update** à¦•à¦°à¦¿
* à¦¬à¦¾à¦°à¦¬à¦¾à¦° à¦•à¦°à¦²à§‡ error à¦•à¦®à§‡ â†’ ANN à¦¶à§‡à¦–à§‡

---

## 2ï¸âƒ£ Simple ANN Model (Single Neuron)

à¦§à¦°à¦¿ à¦à¦•à¦Ÿà¦¿ neuron à¦†à¦›à§‡:

[
y = f(z)
]
à¦¯à§‡à¦–à¦¾à¦¨à§‡,
[
z = w_1x_1 + w_2x_2 + b
]

Activation function (Sigmoid):

[
f(z) = \frac{1}{1+e^{-z}}
]

---

## 3ï¸âƒ£ Given Example Data

à¦§à¦°à¦¿:

* Input:
  [
  x_1=1,; x_2=0
  ]
* Target output:
  [
  t=1
  ]

Initial values:
[
w_1=0.4,; w_2=0.6,; b=0.1
]
Learning rate:
[
\eta = 0.5
]

---

## 4ï¸âƒ£ Forward Propagation (Output à¦¬à§‡à¦° à¦•à¦°à¦¾)

### Step 1: Net input

[
z = (1)(0.4) + (0)(0.6) + 0.1 = 0.5
]

### Step 2: Output

[
y = \frac{1}{1+e^{-0.5}} \approx 0.62
]

---

## 5ï¸âƒ£ Error Calculation

Mean Squared Error (MSE):

[
E = \frac{1}{2}(t-y)^2
]

[
E = \frac{1}{2}(1-0.62)^2 = 0.072
]

---

## 6ï¸âƒ£ Backpropagation (Gradient à¦¬à§‡à¦° à¦•à¦°à¦¾)

à¦†à¦®à¦°à¦¾ à¦¬à§‡à¦° à¦•à¦°à¦¬:

[
\frac{\partial E}{\partial w}
]

### Step 1: Error derivative

[
\frac{\partial E}{\partial y} = (y-t)
]

[
= (0.62-1) = -0.38
]

---

### Step 2: Sigmoid derivative

[
\frac{dy}{dz} = y(1-y)
]

[
= 0.62(1-0.62) = 0.2356
]

---

### Step 3: Chain Rule (Backpropagation)

[
\frac{\partial E}{\partial w_1}
= (y-t)\cdot y(1-y)\cdot x_1
]

[
= (-0.38)(0.2356)(1)
= -0.0895
]

---

## 7ï¸âƒ£ Weight Updation Formula

[
w_{new} = w_{old} - \eta \frac{\partial E}{\partial w}
]

---

### Update (w_1)

[
w_1 = 0.4 - 0.5(-0.0895)
]

[
w_1 = 0.4447
]

---

### Update (w_2)

[
\frac{\partial E}{\partial w_2}
= (y-t)y(1-y)x_2
= (-0.38)(0.2356)(0) = 0
]

[
w_2 = 0.6
]

---

### Update Bias

[
\frac{\partial E}{\partial b}
= (y-t)y(1-y)
= -0.0895
]

[
b = 0.1 - 0.5(-0.0895) = 0.1447
]

---

## 8ï¸âƒ£ Updated Parameters (After Learning)

[
w_1=0.4447,; w_2=0.6,; b=0.1447
]

ğŸ‘‰ Output à¦à¦–à¦¨ target-à¦à¦° à¦†à¦°à¦“ à¦•à¦¾à¦›à¦¾à¦•à¦¾à¦›à¦¿ à¦¹à¦¬à§‡
ğŸ‘‰ **Error à¦•à¦®à§‡à¦›à§‡**
ğŸ‘‰ ANN à¦¶à¦¿à¦–à§‡à¦›à§‡

---

## ğŸ” Backpropagation à¦®à¦¾à¦¨à§‡ à¦•à§€?

* Output à¦¥à§‡à¦•à§‡ error à¦ªà§‡à¦›à¦¨à§‡à¦° à¦¦à¦¿à¦•à§‡ à¦ªà¦¾à¦ à¦¾à¦¨à§‹
* Chain rule à¦¦à¦¿à§Ÿà§‡ à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ weight-à¦à¦° contribution à¦¬à§‡à¦° à¦•à¦°à¦¾
* Gradient descent à¦¦à¦¿à§Ÿà§‡ weight à¦ à¦¿à¦• à¦•à¦°à¦¾

---

## ğŸ”‘ One-Line Summary (Exam Ready)

> **Backpropagation computes the gradient of error using chain rule, and weight updation minimizes error using gradient descent.**

---

=======
### **Backpropagation and Weight Updation (with Mathematical Example)**

à¦à¦Ÿà¦¾ ANN à¦¶à§‡à¦–à¦¾à¦° **à¦¸à¦¬à¦šà§‡à§Ÿà§‡ à¦—à§à¦°à§à¦¤à§à¦¬à¦ªà§‚à¦°à§à¦£ à¦…à¦‚à¦¶**à¥¤ à¦¸à¦¹à¦œ à¦­à¦¾à¦·à¦¾à§Ÿ à¦¬à¦²à¦²à§‡â€”

> **Backpropagation = à¦­à§à¦² à¦¬à§‡à¦° à¦•à¦°à¦¾ + à¦¸à§‡à¦‡ à¦­à§à¦² à¦…à¦¨à§à¦¯à¦¾à§Ÿà§€ weight à¦ à¦¿à¦• à¦•à¦°à¦¾**

---

## 1ï¸âƒ£ Basic Idea (à¦¸à¦¹à¦œ à¦•à¦°à§‡)

* ANN à¦à¦•à¦Ÿà¦¿ output à¦¦à§‡à§Ÿ
* output à¦¯à¦¦à¦¿ à¦­à§à¦² à¦¹à§Ÿ â†’ **error** à¦¬à§‡à¦° à¦•à¦°à¦¿
* error à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§‡ **weight update** à¦•à¦°à¦¿
* à¦¬à¦¾à¦°à¦¬à¦¾à¦° à¦•à¦°à¦²à§‡ error à¦•à¦®à§‡ â†’ ANN à¦¶à§‡à¦–à§‡

---

## 2ï¸âƒ£ Simple ANN Model (Single Neuron)

à¦§à¦°à¦¿ à¦à¦•à¦Ÿà¦¿ neuron à¦†à¦›à§‡:

[
y = f(z)
]
à¦¯à§‡à¦–à¦¾à¦¨à§‡,
[
z = w_1x_1 + w_2x_2 + b
]

Activation function (Sigmoid):

[
f(z) = \frac{1}{1+e^{-z}}
]

---

## 3ï¸âƒ£ Given Example Data

à¦§à¦°à¦¿:

* Input:
  [
  x_1=1,; x_2=0
  ]
* Target output:
  [
  t=1
  ]

Initial values:
[
w_1=0.4,; w_2=0.6,; b=0.1
]
Learning rate:
[
\eta = 0.5
]

---

## 4ï¸âƒ£ Forward Propagation (Output à¦¬à§‡à¦° à¦•à¦°à¦¾)

### Step 1: Net input

[
z = (1)(0.4) + (0)(0.6) + 0.1 = 0.5
]

### Step 2: Output

[
y = \frac{1}{1+e^{-0.5}} \approx 0.62
]

---

## 5ï¸âƒ£ Error Calculation

Mean Squared Error (MSE):

[
E = \frac{1}{2}(t-y)^2
]

[
E = \frac{1}{2}(1-0.62)^2 = 0.072
]

---

## 6ï¸âƒ£ Backpropagation (Gradient à¦¬à§‡à¦° à¦•à¦°à¦¾)

à¦†à¦®à¦°à¦¾ à¦¬à§‡à¦° à¦•à¦°à¦¬:

[
\frac{\partial E}{\partial w}
]

### Step 1: Error derivative

[
\frac{\partial E}{\partial y} = (y-t)
]

[
= (0.62-1) = -0.38
]

---

### Step 2: Sigmoid derivative

[
\frac{dy}{dz} = y(1-y)
]

[
= 0.62(1-0.62) = 0.2356
]

---

### Step 3: Chain Rule (Backpropagation)

[
\frac{\partial E}{\partial w_1}
= (y-t)\cdot y(1-y)\cdot x_1
]

[
= (-0.38)(0.2356)(1)
= -0.0895
]

---

## 7ï¸âƒ£ Weight Updation Formula

[
w_{new} = w_{old} - \eta \frac{\partial E}{\partial w}
]

---

### Update (w_1)

[
w_1 = 0.4 - 0.5(-0.0895)
]

[
w_1 = 0.4447
]

---

### Update (w_2)

[
\frac{\partial E}{\partial w_2}
= (y-t)y(1-y)x_2
= (-0.38)(0.2356)(0) = 0
]

[
w_2 = 0.6
]

---

### Update Bias

[
\frac{\partial E}{\partial b}
= (y-t)y(1-y)
= -0.0895
]

[
b = 0.1 - 0.5(-0.0895) = 0.1447
]

---

## 8ï¸âƒ£ Updated Parameters (After Learning)

[
w_1=0.4447,; w_2=0.6,; b=0.1447
]

ğŸ‘‰ Output à¦à¦–à¦¨ target-à¦à¦° à¦†à¦°à¦“ à¦•à¦¾à¦›à¦¾à¦•à¦¾à¦›à¦¿ à¦¹à¦¬à§‡
ğŸ‘‰ **Error à¦•à¦®à§‡à¦›à§‡**
ğŸ‘‰ ANN à¦¶à¦¿à¦–à§‡à¦›à§‡

---

## ğŸ” Backpropagation à¦®à¦¾à¦¨à§‡ à¦•à§€?

* Output à¦¥à§‡à¦•à§‡ error à¦ªà§‡à¦›à¦¨à§‡à¦° à¦¦à¦¿à¦•à§‡ à¦ªà¦¾à¦ à¦¾à¦¨à§‹
* Chain rule à¦¦à¦¿à§Ÿà§‡ à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ weight-à¦à¦° contribution à¦¬à§‡à¦° à¦•à¦°à¦¾
* Gradient descent à¦¦à¦¿à§Ÿà§‡ weight à¦ à¦¿à¦• à¦•à¦°à¦¾

---

## ğŸ”‘ One-Line Summary (Exam Ready)

> **Backpropagation computes the gradient of error using chain rule, and weight updation minimizes error using gradient descent.**

---

>>>>>>> f45ebbad1686e699afe9932c4175eeff501d254b
