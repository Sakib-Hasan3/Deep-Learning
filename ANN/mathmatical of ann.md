<<<<<<< HEAD
---

## 1) Problem (à¦à¦•à¦Ÿà¦¾ à¦•à¦¾à¦œ à¦¶à¦¿à¦–à¦¬à§‡)

à¦§à¦°à¦¿ ANN à¦¶à¦¿à¦–à¦¬à§‡ à¦à¦‡ rule:

> **OR gate**
> (à¦¯à§‡à¦•à§‹à¦¨à§‹ à¦à¦•à¦Ÿà¦¾ 1 à¦¹à¦²à§‡à¦‡ output 1)

Training data:

| x1 | x2 | Target (t) |
| -- | -- | ---------- |
| 0  | 0  | 0          |
| 0  | 1  | 1          |
| 1  | 0  | 1          |
| 1  | 1  | 1          |

---

## 2) Model (Neuron equation)

Neuron output:

[
y = \begin{cases}
1, & \text{if } (w_1x_1 + w_2x_2 + b)\ge 0\
0, & \text{otherwise}
\end{cases}
]

Learning rule (Perceptron update):

[
w_i \leftarrow w_i + \eta (t-y)x_i
]
[
b \leftarrow b + \eta (t-y)
]

à¦§à¦°à¦¿ learning rate (\eta = 1)

---

## 3) Initialize (à¦¶à§à¦°à§à¦¤à§‡ à¦§à¦°à¦²à¦¾à¦®)

[
w_1=0,; w_2=0,; b=0
]

---

## 4) Training step-by-step (à¦à¦•à¦Ÿà¦¾ epoch à¦¦à§‡à¦–à¦¾à¦‡)

### âœ… Sample 1: (x1=0, x2=0, t=0)

Net:
[
0\cdot0 + 0\cdot0 + 0 = 0
\Rightarrow y=1 ; (\text{à¦•à¦¾à¦°à¦£ } \ge 0)
]
Error term:
[
(t-y)=0-1=-1
]
Update:
[
w_1 = 0 + (-1)\cdot0 = 0
]
[
w_2 = 0 + (-1)\cdot0 = 0
]
[
b = 0 + (-1) = -1
]

---

### âœ… Sample 2: (0,1, t=1)

Net:
[
0\cdot0 + 0\cdot1 - 1 = -1 \Rightarrow y=0
]
Error:
[
(t-y)=1-0=1
]
Update:
[
w_1 = 0 + 1\cdot0 = 0
]
[
w_2 = 0 + 1\cdot1 = 1
]
[
b = -1 + 1 = 0
]

---

### âœ… Sample 3: (1,0, t=1)

Net:
[
0\cdot1 + 1\cdot0 + 0 = 0 \Rightarrow y=1
]
Error:
[
(t-y)=1-1=0
]
Update: à¦•à¦¿à¦›à§à¦‡ à¦¬à¦¦à¦²à¦¾à¦¬à§‡ à¦¨à¦¾

---

### âœ… Sample 4: (1,1, t=1)

Net:
[
0\cdot1 + 1\cdot1 + 0 = 1 \Rightarrow y=1
]
Error:
[
(t-y)=0
]
Update: à¦•à¦¿à¦›à§à¦‡ à¦¬à¦¦à¦²à¦¾à¦¬à§‡ à¦¨à¦¾

---

## 5) Final learned parameters (à¦à¦–à¦¾à¦¨à§‡ à¦¶à§‡à¦–à¦¾ à¦¶à§‡à¦·à§‡à¦° à¦®à¦¤à§‹)

[
w_1=0,; w_2=1,; b=0
]

---

## 6) ANN Intuition (à¦à¦–à¦¾à¦¨à§‡ â€œintuitionâ€ à¦•à§€?)

à¦à¦–à¦¨ ANN-à¦à¦° **intuition** à¦¹à¦²à§‹ à¦à¦‡ learned rule:

[
\text{If } x_2=1,; \text{output à¦¹à¦¬à§‡ 1}
]

à¦®à¦¾à¦¨à§‡ ANN **à¦¡à§‡à¦Ÿà¦¾ à¦¦à§‡à¦–à§‡ à¦¬à§à¦à§‡ à¦¨à¦¿à§Ÿà§‡à¦›à§‡** à¦•à§‹à¦¨ à¦‡à¦¨à¦ªà§à¦Ÿà¦Ÿà¦¾ à¦¬à§‡à¦¶à¦¿ à¦—à§à¦°à§à¦¤à§à¦¬à¦ªà§‚à¦°à§à¦£ (à¦à¦–à¦¾à¦¨à§‡ (x_2))â€”à¦à¦Ÿà¦¾à¦‡ à¦¤à¦¾à¦° learned intuition/pattern understandingà¥¤

---

### âœ… Quick test

* (0,0): net = 0 â‡’ y=1 (à¦à¦Ÿà¦¾ à¦­à§à¦² à¦¹à¦¤à§‡ à¦ªà¦¾à¦°à§‡ threshold rule à¦à¦° à¦•à¦¾à¦°à¦£à§‡)
  ğŸ‘‰ à¦¬à¦¾à¦¸à§à¦¤à¦¬à§‡ à¦†à¦®à¦°à¦¾ bias/threshold à¦à¦•à¦Ÿà§ à¦Ÿà¦¿à¦‰à¦¨ à¦•à¦°à¦¿ à¦¬à¦¾ activation rule à¦ªà¦°à¦¿à¦¬à¦°à§à¦¤à¦¨ à¦•à¦°à¦¿ (>(0) à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à¦¿) à¦¯à¦¾à¦¤à§‡ (0,0) à¦ à¦¿à¦•à¦­à¦¾à¦¬à§‡ 0 à¦¹à§Ÿà¥¤

à¦‰à¦¦à¦¾à¦¹à¦°à¦£ à¦¹à¦¿à¦¸à§‡à¦¬à§‡ à¦¯à¦¦à¦¿ rule à¦¨à¦¿à¦‡:

[
y=1 \text{ if net} > 0
]
à¦¤à¦¾à¦¹à¦²à§‡ (0,0) net=0 â‡’ y=0 (à¦ à¦¿à¦•)

---

=======
---

## 1) Problem (à¦à¦•à¦Ÿà¦¾ à¦•à¦¾à¦œ à¦¶à¦¿à¦–à¦¬à§‡)

à¦§à¦°à¦¿ ANN à¦¶à¦¿à¦–à¦¬à§‡ à¦à¦‡ rule:

> **OR gate**
> (à¦¯à§‡à¦•à§‹à¦¨à§‹ à¦à¦•à¦Ÿà¦¾ 1 à¦¹à¦²à§‡à¦‡ output 1)

Training data:

| x1 | x2 | Target (t) |
| -- | -- | ---------- |
| 0  | 0  | 0          |
| 0  | 1  | 1          |
| 1  | 0  | 1          |
| 1  | 1  | 1          |

---

## 2) Model (Neuron equation)

Neuron output:

[
y = \begin{cases}
1, & \text{if } (w_1x_1 + w_2x_2 + b)\ge 0\
0, & \text{otherwise}
\end{cases}
]

Learning rule (Perceptron update):

[
w_i \leftarrow w_i + \eta (t-y)x_i
]
[
b \leftarrow b + \eta (t-y)
]

à¦§à¦°à¦¿ learning rate (\eta = 1)

---

## 3) Initialize (à¦¶à§à¦°à§à¦¤à§‡ à¦§à¦°à¦²à¦¾à¦®)

[
w_1=0,; w_2=0,; b=0
]

---

## 4) Training step-by-step (à¦à¦•à¦Ÿà¦¾ epoch à¦¦à§‡à¦–à¦¾à¦‡)

### âœ… Sample 1: (x1=0, x2=0, t=0)

Net:
[
0\cdot0 + 0\cdot0 + 0 = 0
\Rightarrow y=1 ; (\text{à¦•à¦¾à¦°à¦£ } \ge 0)
]
Error term:
[
(t-y)=0-1=-1
]
Update:
[
w_1 = 0 + (-1)\cdot0 = 0
]
[
w_2 = 0 + (-1)\cdot0 = 0
]
[
b = 0 + (-1) = -1
]

---

### âœ… Sample 2: (0,1, t=1)

Net:
[
0\cdot0 + 0\cdot1 - 1 = -1 \Rightarrow y=0
]
Error:
[
(t-y)=1-0=1
]
Update:
[
w_1 = 0 + 1\cdot0 = 0
]
[
w_2 = 0 + 1\cdot1 = 1
]
[
b = -1 + 1 = 0
]

---

### âœ… Sample 3: (1,0, t=1)

Net:
[
0\cdot1 + 1\cdot0 + 0 = 0 \Rightarrow y=1
]
Error:
[
(t-y)=1-1=0
]
Update: à¦•à¦¿à¦›à§à¦‡ à¦¬à¦¦à¦²à¦¾à¦¬à§‡ à¦¨à¦¾

---

### âœ… Sample 4: (1,1, t=1)

Net:
[
0\cdot1 + 1\cdot1 + 0 = 1 \Rightarrow y=1
]
Error:
[
(t-y)=0
]
Update: à¦•à¦¿à¦›à§à¦‡ à¦¬à¦¦à¦²à¦¾à¦¬à§‡ à¦¨à¦¾

---

## 5) Final learned parameters (à¦à¦–à¦¾à¦¨à§‡ à¦¶à§‡à¦–à¦¾ à¦¶à§‡à¦·à§‡à¦° à¦®à¦¤à§‹)

[
w_1=0,; w_2=1,; b=0
]

---

## 6) ANN Intuition (à¦à¦–à¦¾à¦¨à§‡ â€œintuitionâ€ à¦•à§€?)

à¦à¦–à¦¨ ANN-à¦à¦° **intuition** à¦¹à¦²à§‹ à¦à¦‡ learned rule:

[
\text{If } x_2=1,; \text{output à¦¹à¦¬à§‡ 1}
]

à¦®à¦¾à¦¨à§‡ ANN **à¦¡à§‡à¦Ÿà¦¾ à¦¦à§‡à¦–à§‡ à¦¬à§à¦à§‡ à¦¨à¦¿à§Ÿà§‡à¦›à§‡** à¦•à§‹à¦¨ à¦‡à¦¨à¦ªà§à¦Ÿà¦Ÿà¦¾ à¦¬à§‡à¦¶à¦¿ à¦—à§à¦°à§à¦¤à§à¦¬à¦ªà§‚à¦°à§à¦£ (à¦à¦–à¦¾à¦¨à§‡ (x_2))â€”à¦à¦Ÿà¦¾à¦‡ à¦¤à¦¾à¦° learned intuition/pattern understandingà¥¤

---

### âœ… Quick test

* (0,0): net = 0 â‡’ y=1 (à¦à¦Ÿà¦¾ à¦­à§à¦² à¦¹à¦¤à§‡ à¦ªà¦¾à¦°à§‡ threshold rule à¦à¦° à¦•à¦¾à¦°à¦£à§‡)
  ğŸ‘‰ à¦¬à¦¾à¦¸à§à¦¤à¦¬à§‡ à¦†à¦®à¦°à¦¾ bias/threshold à¦à¦•à¦Ÿà§ à¦Ÿà¦¿à¦‰à¦¨ à¦•à¦°à¦¿ à¦¬à¦¾ activation rule à¦ªà¦°à¦¿à¦¬à¦°à§à¦¤à¦¨ à¦•à¦°à¦¿ (>(0) à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à¦¿) à¦¯à¦¾à¦¤à§‡ (0,0) à¦ à¦¿à¦•à¦­à¦¾à¦¬à§‡ 0 à¦¹à§Ÿà¥¤

à¦‰à¦¦à¦¾à¦¹à¦°à¦£ à¦¹à¦¿à¦¸à§‡à¦¬à§‡ à¦¯à¦¦à¦¿ rule à¦¨à¦¿à¦‡:

[
y=1 \text{ if net} > 0
]
à¦¤à¦¾à¦¹à¦²à§‡ (0,0) net=0 â‡’ y=0 (à¦ à¦¿à¦•)

---

>>>>>>> f45ebbad1686e699afe9932c4175eeff501d254b
