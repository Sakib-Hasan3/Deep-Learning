### **ReLU (Rectified Linear Unit): Advantages vs Disadvantages**

---

## ‚úÖ **Advantages of ReLU**

1. **Avoids Vanishing Gradient (Mostly)**
   (x>0) ‡¶π‡¶≤‡ßá derivative = 1 ‚áí gradient ‡¶∂‡¶ï‡ßç‡¶§ ‡¶•‡¶æ‡¶ï‡ßá ‚áí deep network ‡¶≠‡¶æ‡¶≤‡ßã ‡¶∂‡ßá‡¶ñ‡ßá‡•§

2. **Very Fast Computation**
   ‡¶∂‡ßÅ‡¶ß‡ßÅ `max(0, x)` ‚áí exponential ‡¶®‡ßá‡¶á ‚áí training ‡¶¶‡ßç‡¶∞‡ßÅ‡¶§‡•§

3. **Simple and Efficient**
   Implement ‡¶ï‡¶∞‡¶æ ‡¶∏‡¶π‡¶ú, memory ‡¶ì computation ‡¶ï‡¶Æ ‡¶≤‡¶æ‡¶ó‡ßá‡•§

4. **Sparse Activation**
   ‡¶Ö‡¶®‡ßá‡¶ï neuron output = 0 ‡¶π‡ßü ‚áí network efficient ‡¶ì less overfitting tendency‡•§

5. **Best Choice for Hidden Layers**
   Modern deep learning‚Äì‡¶è hidden layer-‡¶è‡¶∞ default activation‡•§

---

## ‚ùå **Disadvantages of ReLU**

1. **Dead Neuron (Dead ReLU) Problem**
   (x<0) ‡¶π‡¶≤‡ßá gradient = 0 ‚áí neuron ‡¶Ü‡¶∞ ‡¶∂‡ßá‡¶ñ‡ßá ‡¶®‡¶æ‡•§

2. **Not Zero-Centered Output**
   Output range [0, ‚àû) ‚áí gradient descent-‡¶è zig-zag updates ‡¶π‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡•§

3. **Unbounded Output**
   Output ‡¶Ö‡¶®‡ßá‡¶ï ‡¶¨‡ßú ‡¶π‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá ‚áí exploding activation risk‡•§

4. **Non-Differentiable at x = 0**
   Mathematical issue (practically ignore ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü)‡•§

5. **Sensitive to Learning Rate & Initialization**
   ‡¶¨‡ßá‡¶∂‡¶ø learning rate ‚áí neuron dead ‡¶π‡ßü‡ßá ‡¶Ø‡ßá‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡•§

---

## üìä **Advantages vs Disadvantages Table**

| Aspect             | ReLU           |
| ------------------ | -------------- |
| Gradient flow      | ‚úÖ Strong (x>0) |
| Training speed     | ‚úÖ Very fast    |
| Deep networks      | ‚úÖ Excellent    |
| Vanishing gradient | ‚úÖ Avoided      |
| Dead neuron risk   | ‚ùå Yes          |
| Zero-centered      | ‚ùå No           |
| Output bounded     | ‚ùå No           |

---

## üß† Memory Trick

> **ReLU learns fast, but negative side can kill neurons.**

---

