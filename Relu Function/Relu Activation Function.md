## **ReLU (Rectified Linear Unit) Activation Function тАФ Graph рж╕рж╣ ржмрзНржпрж╛ржЦрзНржпрж╛**

![Image](https://www.researchgate.net/publication/343675998/figure/fig3/AS%3A924834280267785%401597509083229/Graph-of-ReLu-activation-function.ppm)

![Image](https://www.researchgate.net/publication/359884439/figure/fig3/AS%3A1147051651932161%401650489833478/ReLU-activation-function-and-its-derivative.png)

![Image](https://www.researchgate.net/publication/354971308/figure/fig1/AS%3A1080246367457377%401634562212739/Curves-of-the-Sigmoid-Tanh-and-ReLu-activation-functions.jpg)

![Image](https://www.researchgate.net/publication/327435257/figure/fig4/AS%3A742898131812354%401554132125449/Activation-Functions-ReLU-Tanh-Sigmoid.ppm)

---

## ЁЯФ╣ 1) ReLU ржХрзА?

**ReLU (Rectified Linear Unit)** рж╣рж▓рзЛ рж╕ржмржЪрзЗрзЯрзЗ ржмрзЗрж╢рж┐ ржмрзНржпржмрж╣рзГржд **activation function** (ржмрж┐рж╢рзЗрж╖ ржХрж░рзЗ Deep Neural Network-ржП)ред

ржЧрж╛ржгрж┐рждрж┐ржХржнрж╛ржмрзЗ,

[
\text{ReLU}(x) = \max(0, x)
]

---

## ЁЯФ╣ 2) Output Range

[
\text{Output} \in [0, \infty)
]

* Negative input тЖТ **0**
* Positive input тЖТ **same value**

---

## ЁЯФ╣ 3) Graph ржжрзЗржЦрзЗ Intuition (ржЦрзБржм ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг)

### (a) x < 0

* Output = 0
* Neuron **inactive**
* Gradient = 0

### (b) x > 0

* Output = x
* Straight line (slope = 1)
* Gradient strong тЖТ **fast learning**

ЁЯСЙ GraphржЯрж╛ ржжрзЗржЦрждрзЗ **broken straight line** ржПрж░ ржорждрзЛред

---

## ЁЯФ╣ 4) Derivative (Backpropagation-ржПрж░ ржЬржирзНржп)

[
\text{ReLU}'(x)=
\begin{cases}
0, & x<0\
1, & x>0
\end{cases}
]

(x = 0 ржП derivative undefined, ржмрж╛рж╕рзНрждржмрзЗ 0 ржмрж╛ 1 ржзрж░рж╛ рж╣рзЯ)

---

## ЁЯФ╣ 5) ржХрзЗржи ReLU ржПржд ржЬржиржкрзНрж░рж┐рзЯ? (Key Intuition)

тЬФ **Vanishing Gradient ржирзЗржЗ** (x>0 рж╣рж▓рзЗ)
тЬФ Very fast computation
тЬФ Sparse activation (рж╕ржм neuron ржПржХрж╕рж╛ржерзЗ active рж╣рзЯ ржирж╛)
тЬФ Deep network-ржП ржЦрзБржм ржнрж╛рж▓рзЛ ржХрж╛ржЬ ржХрж░рзЗ

ЁЯСЙ рждрж╛ржЗ ржЖржЬржХрж╛рж▓ **hidden layers-ржПрж░ default choice = ReLU**

---

## ЁЯФ╣ 6) ReLU vs Sigmoid vs Tanh (Graph-based рждрзБрж▓ржирж╛)

| Feature            | Sigmoid | Tanh      | ReLU      |
| ------------------ | ------- | --------- | --------- |
| Output range       | (0,1)   | (тИТ1,1)    | [0,тИЮ)     |
| Zero-centered      | тЭМ       | тЬЕ         | тЭМ         |
| Vanishing gradient | тЭМ High  | тЪая╕П Medium | тЬЕ Low     |
| Computation        | Slow    | Medium    | Very fast |
| Deep networks      | тЭМ       | тЪая╕П        | тЬЕ Best    |

---

## ЁЯФ╣ 7) Dead Neuron Problem (ReLU-ржПрж░ drawback)

ржпржжрж┐:

* Weight update ржПржоржи рж╣рзЯ ржпрзЗ
* neuron рж╕ржмрж╕ржорзЯ negative input ржкрж╛рзЯ

рждрж╛рж╣рж▓рзЗ:
[
x<0 \Rightarrow \text{ReLU}(x)=0
\Rightarrow \text{Gradient}=0
]

ЁЯСЙ Neuron ржЖрж░ рж╢рзЗржЦрзЗ ржирж╛
ЁЯСЙ ржПржХрзЗ ржмрж▓рзЗ **Dead ReLU**

---

## ЁЯФ╣ 8) Dead ReLU рж╕ржорж╛ржзрж╛ржи

тЬФ **Leaky ReLU**
[
f(x)=
\begin{cases}
0.01x,& x<0\
x,& x>0
\end{cases}
]

тЬФ **Parametric ReLU (PReLU)**
тЬФ Proper weight initialization (He initialization)

---

## ЁЯФ╣ 9) Small Numerical Example

ржзрж░рж┐:
[
x = -2 \Rightarrow \text{ReLU}(-2)=0
]

[
x = 3 \Rightarrow \text{ReLU}(3)=3
]

Derivative:

* at (x=-2): 0
* at (x=3): 1

---

## ЁЯза Memory Trick

> **ReLU = тАЬIf negative, kill it; if positive, pass itтАЭ**

---

## тЬНя╕П Exam-Ready One Line

> **ReLU is a piecewise linear activation function defined as max(0, x) that accelerates training and avoids vanishing gradients, making it ideal for deep neural networks.**

---


